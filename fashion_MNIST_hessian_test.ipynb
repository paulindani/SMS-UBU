{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bGU6NwlsXFSt"
   },
   "outputs": [],
   "source": [
    "#@title Import Dependencies\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import itertools\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no display found. Using non-interactive Agg backend\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from typing import TypeVar, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "Tensor = TypeVar('torch.tensor')\n",
    "\n",
    "#matplotlib.use('Agg')\n",
    "\n",
    "import click\n",
    "from argparse import Namespace\n",
    "import ast\n",
    "import os\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from typing import TypeVar, Tuple\n",
    "import gdown\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_bNfVLRUYqZA"
   },
   "outputs": [],
   "source": [
    "#@title Define Hyperparameters\n",
    "\n",
    "# class_names = [\"5_o_Clock_Shadow\", \"Arched_Eyebrows\", \"Attractive\", \"Bags_Under_Eyes\", \"Bald\", \"Bangs\",\n",
    "#                 \"Big_Lips\", \"Big_Nose\", \"Black_Hair\", \"Blond_Hair\", \"Blurry\", \"Brown_Hair\", \"Bushy_Eyebrows\",\n",
    "#                 \"Chubby\", \"Double_Chin\", \"Eyeglasses\", \"Goatee\", \"Gray_Hair\", \"Heavy_Makeup\", \"High_Cheekbones\",\n",
    "#                 \"Male\", \"Mouth_Slightly_Open\", \"Mustache\", \"Narrow_Eyes\", \"No_Beard\", \"Oval_Face\", \"Pale_Skin\",\n",
    "#                 \"Pointy_Nose\", \"Receding_Hairline\", \"Rosy_Cheeks\", \"Sideburns\", \"Smiling\", \"Straight_Hair\",\n",
    "#                 \"Wavy_Hair\", \"Wearing_Earrings\", \"Wearing_Hat\", \"Wearing_Lipstick\", \"Wearing_Necklace\",\n",
    "#                 \"Wearing_Necktie\", \"Young\"]\n",
    "# num_classes = 40\n",
    "# data_shape = [3, 64, 64]\n",
    "# n_bits = 5\n",
    "# temp = 0.7\n",
    "# data_mean = [0.485, 0.456, 0.406]\n",
    "# data_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "input_size = 28*28*1 # img_size = (28,28) ---> 28*28=784 in total\n",
    "batch_size = 200 # the size of input data took for one iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lCsBCXMwbpH5"
   },
   "outputs": [],
   "source": [
    "# from torchvision.transforms import v2\n",
    "# transform_RandomErasing=transforms.Compose([v2.RandomErasing(),\n",
    "#                               transforms.ToTensor()])\n",
    "\n",
    "transform=transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_data = dsets.FashionMNIST(root = './data', train=True, transform = transform, download = True)\n",
    "test_data = dsets.FashionMNIST(root = './data', train=False, transform = transform, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rfDPBdnYgfGp"
   },
   "outputs": [],
   "source": [
    "#@title Loading the data\n",
    "\n",
    "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
    "                                             batch_size = batch_size,\n",
    "                                             shuffle = True)\n",
    "\n",
    "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
    "                                      batch_size = batch_size,\n",
    "                                      shuffle = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvmx=torch.zeros([3*64*64,3*64*64],device=device)\n",
    "images_list=[]\n",
    "labels_list=[]\n",
    "no_batches=len(train_gen)\n",
    "#images_mean=torch.zeros(3,64,64,device=device)\n",
    "for i ,(images,labels) in enumerate(train_gen):\n",
    "    images = Variable(images).cuda().detach()\n",
    "    labels=Variable(labels).cuda().detach()\n",
    "    # images_mean=images_mean+images.mean(0)\n",
    "    # im=torch.reshape(images,[images.shape[0],3*64*64])\n",
    "    # cvmx+=torch.matmul(torch.transpose(im,0,1),im)\n",
    "    if(i<(len(train_gen))):\n",
    "        images_list.append(images)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "\n",
    "\n",
    "test_images_list=[]\n",
    "test_labels_list=[]\n",
    "test_no_batches=len(test_gen)\n",
    "for i ,(images,labels) in enumerate(test_gen):\n",
    "    images = Variable(images).cuda().detach()\n",
    "    labels=Variable(labels).cuda().detach()\n",
    "    if(i<(len(test_gen))):\n",
    "        test_images_list.append(images)\n",
    "        test_labels_list.append(labels)\n",
    "\n",
    "train_data_len=len(train_data)\n",
    "test_data_len=len(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fL-YXTvghaz_"
   },
   "outputs": [],
   "source": [
    "#@title Define model class\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from typing import TypeVar, Tuple\n",
    "\n",
    "Tensor = TypeVar('torch.tensor')\n",
    "\n",
    "\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    base class for all NN classifiers\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "                torch.nn.init.constant_(m.weight, 1)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight, 0, 0.01)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "\n",
    "class CNN(NeuralNet):\n",
    "    \"\"\"\n",
    "    CNN for (binary) classification for CelebA, CheXpert\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 3,\n",
    "                 num_classes: int = 2,\n",
    "                 flattened_size: int = 16384,\n",
    "                 low_rank: int = 32,\n",
    "                 batch_norm_mom: float = 1.0):\n",
    "        \"\"\"CNN Builder.\"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "\n",
    "        self.conv_layer = nn.Sequential(\n",
    "\n",
    "            # Conv Layer block 1\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=32, kernel_size=3, padding=1),\n",
    "            nn.Softplus(beta=1.0),\n",
    "            nn.BatchNorm2d(32,momentum=batch_norm_mom),\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.Softplus(beta=1.0),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.BatchNorm2d(64,momentum=batch_norm_mom),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1),\n",
    "            nn.Softplus(beta=1.0),\n",
    "            nn.BatchNorm2d(64,momentum=batch_norm_mom),\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.Softplus(beta=1.0),\n",
    "            nn.BatchNorm2d(128,momentum=batch_norm_mom),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            #nn.BatchNorm2d(128),\n",
    "\n",
    "            # nn.BatchNorm2d(128),\n",
    "            # nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            # nn.Softplus(beta=1.0),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # nn.BatchNorm2d(128),\n",
    "\n",
    "            # # Conv Layer block 2\n",
    "            # nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1),\n",
    "            # nn.BatchNorm2d(128),\n",
    "            # nn.Softplus(beta=1.0),\n",
    "            # nn.BatchNorm2d(128),\n",
    "            # nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1),\n",
    "            # nn.Softplus(beta=1.0),\n",
    "            # nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            # nn.BatchNorm2d(128),\n",
    "\n",
    "            # Conv Layer block 3\n",
    "            nn.BatchNorm2d(128,momentum=batch_norm_mom),\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.Softplus(beta=1.0),\n",
    "            nn.BatchNorm2d(256,momentum=batch_norm_mom),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1),\n",
    "            nn.Softplus(beta=1.0),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            #nn.Dropout(p=0.1),\n",
    "            nn.BatchNorm1d(flattened_size,momentum=batch_norm_mom),\n",
    "#            nn.Linear(flattened_size, 512),\n",
    "            nn.Linear(flattened_size, low_rank),\n",
    "            nn.BatchNorm1d(low_rank,momentum=batch_norm_mom),\n",
    "            nn.Linear(low_rank,512),            \n",
    "            nn.Softplus(beta=1.0),\n",
    "            nn.BatchNorm1d(512,momentum=batch_norm_mom),\n",
    "            # nn.Linear(2048, 512),\n",
    "            # nn.Softplus(beta=1.0),\n",
    "            # nn.BatchNorm1d(512),\n",
    "            # nn.Linear(1024, 512),\n",
    "            # nn.Softplus(beta=1.0), \n",
    "            # nn.BatchNorm1d(512),\n",
    "            #nn.Dropout(p=0.1),\n",
    "        )\n",
    "\n",
    "        self.last_layer=nn.Sequential(\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "\n",
    "        # conv layers\n",
    "        x = self.conv_layer(x)\n",
    "\n",
    "        # flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # # # fc layer\n",
    "        x = self.fc_layer(x)\n",
    "\n",
    "        x=self.last_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def classify(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        net_out = self.forward(x)\n",
    "        acc = F.softmax(net_out, dim=1)\n",
    "        class_idx = torch.max(net_out, 1)[1]\n",
    "\n",
    "        return acc, acc[0, class_idx], class_idx\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CelebA_CNN(CNN):\n",
    "    \"\"\"CNN.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 3,\n",
    "                 num_classes: int = 2,\n",
    "                 #num_classes: int = 1,\n",
    "                 flattened_size: int = 16384):\n",
    "        \"\"\"CNN Builder.\"\"\"\n",
    "        super(CelebA_CNN, self).__init__(in_channels=in_channels, num_classes=num_classes,\n",
    "                                         flattened_size=flattened_size)\n",
    "\n",
    "class Fashion_MNIST_CNN(CNN):\n",
    "    \"\"\"CNN.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 1,\n",
    "                 num_classes: int = 10,\n",
    "                 #flattened_size: int = 6272,\n",
    "                 flattened_size: int = 2304,\n",
    "                 low_rank: int = 64,\n",
    "                 batch_norm_mom: float = 1.0):\n",
    "        \"\"\"CNN Builder.\"\"\"\n",
    "        super(Fashion_MNIST_CNN, self).__init__(in_channels=in_channels, num_classes=num_classes,\n",
    "                                         flattened_size=flattened_size)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CheXpert_CNN(CNN):\n",
    "    def __init__(self,\n",
    "                 in_channels: int = 1,\n",
    "                 num_classes: int = 2,\n",
    "                 flattened_size: int = 65536):\n",
    "        \"\"\"CNN Builder.\"\"\"\n",
    "        super(CheXpert_CNN, self).__init__(in_channels=in_channels, num_classes=num_classes,\n",
    "                                           flattened_size=flattened_size)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, num_classes):\n",
    "    super(Net,self).__init__()\n",
    "    self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "  def forward(self,x):\n",
    "    out = self.fc1(x)\n",
    "    out = self.relu(out)\n",
    "    out = self.fc2(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net=Fashion_MNIST_CNN()\n",
    "# n=0\n",
    "# for par in net.parameters():\n",
    "#     n+=par.numel()\n",
    "\n",
    "# n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ePLIwvAFj2zH"
   },
   "outputs": [],
   "source": [
    "#@title Define loss-function & optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def images_regulariser(): \n",
    "    li_reg_loss = 0\n",
    "    penalized     = [p for name,p in net.named_parameters() if 'bias' not in name]\n",
    "    not_penalized = [p for name,p in net.named_parameters() if 'bias' in name]\n",
    "    for p in penalized:\n",
    "        li_reg_loss += (p**2).sum()*0.5\n",
    "    #for p in net.parameters():\n",
    "#        li_reg_loss += (p**2).sum()*0.5\n",
    "    reg=li_reg_loss/(train_data_len)*l2regconst\n",
    "    return(reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addnet(net,net2):\n",
    "    for param1, param2 in zip(net.parameters(), net2.parameters()):\n",
    "     param1.data += param2.data\n",
    "\n",
    "def multiplynet(net,a):\n",
    "   for param1 in net.parameters():\n",
    "     param1.data *=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class hclass:\n",
    "    h: Tensor\n",
    "    eta: Tensor\n",
    "    etam1g: Tensor\n",
    "    c11: Tensor\n",
    "    c21: Tensor\n",
    "    c22: Tensor\n",
    "\n",
    "def hper2const(h,gam):\n",
    "    gh=gam.double()*h.double()\n",
    "    s=torch.sqrt(4*torch.expm1(-gh/2)-torch.expm1(-gh)+gh)\n",
    "    eta=(torch.exp(-gh/2)).float()\n",
    "    etam1g=((-torch.expm1(-gh/2))/gam.double()).float()\n",
    "    c11=(s/gam).float()\n",
    "    c21=(torch.exp(-gh)*(torch.expm1(gh/2.0))**2/s).float()\n",
    "    c22=(torch.sqrt(8*torch.expm1(-gh/2)-4*torch.expm1(-gh)-gh*torch.expm1(-gh))/s).float()\n",
    "    hc=hclass(h=h,eta=eta,etam1g=etam1g,c11=c11,c21=c21,c22=c22)\n",
    "    return(hc)\n",
    "\n",
    "def U(x,v,hc):\n",
    "    xi1=torch.randn(x.size(),device=device)\n",
    "    xi2=torch.randn(x.size(),device=device)\n",
    "\n",
    "    xn=x+hc.etam1g*v+hc.c11*xi1\n",
    "    vn=v*hc.eta+hc.c21*xi1+hc.c22*xi2\n",
    "    return([xn, vn])\n",
    "\n",
    "def bounce(x,v,xstar,width):\n",
    "    vsign=(((x-xstar+width)/(2*width)).floor()% 2)*(-2)+1\n",
    "    vn=v*vsign\n",
    "    xn=((x-xstar-width)% (4*width)-2*width).abs()-width+xstar\n",
    "    # num_outside=((xn-xstar)>width).sum()+((xstar-xn)>width).sum()\n",
    "    # if(num_outside>0):\n",
    "    #     print(num_outside)    \n",
    "    return([xn, vn])\n",
    "\n",
    "def bouncenet():\n",
    "    for p,p_star in zip(net.parameters(),net_star.parameters()):\n",
    "        [p.data, p.v]=bounce(p.data, p.v, p_star.data, 6/torch.sqrt(l2regconst_extra))\n",
    "\n",
    "def UBU_step(hper2c,images,labels,batch_it):   \n",
    "    with torch.no_grad():\n",
    "        for p in list(net.parameters()):\n",
    "            # maxlen=20*torch.sqrt((torch.tensor(torch.numel(p.v))).float())\n",
    "            # if(torch.norm(p.v)>maxlen):\n",
    "            #    print(\"big trouble!!!!\")    \n",
    "            [p.data,p.v]=U(p.data,p.v,hper2c)\n",
    "\n",
    "        bouncenet()\n",
    "    #print(\"outputsU\",len(outputsU))\n",
    "    #print(\"labelsU\",len(labels))\n",
    "    #print(\"imagesU\",len(images))\n",
    "    outputsU = net(images)\n",
    "    loss_likelihood = loss_function(outputsU, labels)  \n",
    "\n",
    "\n",
    "    grads_reg=[torch.zeros_like(par) for par in net.parameters()]\n",
    "    net_pars=list(net.parameters())\n",
    "    with torch.no_grad():\n",
    "        for it in range(len_params):\n",
    "            if(list_no_bias[it]):\n",
    "                grads_reg[it]=net_pars[it].data*l2regconst\n",
    "\n",
    "    net.zero_grad()\n",
    "    #loss.backward()\n",
    "    loss_likelihood.backward()\n",
    "    with torch.no_grad():\n",
    "        grads_likelihood=[par.grad*batch_size for par in net.parameters()]\n",
    "    \n",
    "        #Normal, no variance reduction\n",
    "        # for p,p_star in zip(net.parameters(),net_star.parameters()):      \n",
    "        #     p.v-=hper2c.h*(p.grad*train_data_len+l2regconst_extra*(p.data-p_star.data))\n",
    "\n",
    "        for p,grad,grad_reg,p_star,grad_star,star_sum_grad in zip(list(net.parameters()),grads_likelihood,grads_reg,list(net_star.parameters()),net_star_grad_list[batch_it],net_star_full_grad):              \n",
    "            #Using variance reduction\n",
    "            p.v-=hper2c.h*(grad_reg+star_sum_grad+(grad-grad_star)*no_batches+l2regconst_extra*(p.data-p_star.data))\n",
    "\n",
    "            # maxlen=20*torch.sqrt((torch.tensor(torch.numel(p.v))).float())\n",
    "            # if(torch.norm(p.v)>maxlen):\n",
    "            #     print(\"trouble\")\n",
    "                #p.v=(p.v/torch.norm(p.v))*maxlen\n",
    "        # for it in range(len_params):\n",
    "        #     [list(net.parameters())[it].data,list(net.parameters())[it].v]=U(list(net.parameters())[it].data,list(net.parameters())[it].v,hper2c)        \n",
    "        for p in list(net.parameters()):\n",
    "            [p.data,p.v]=U(p.data,p.v,hper2c)\n",
    "\n",
    "    #bouncenet()\n",
    "    return(loss_likelihood.data)\n",
    "\n",
    "def ind_create(batch_it):\n",
    "    modit=batch_it %(2*no_batches)\n",
    "    ind=(modit<=(no_batches-1))*modit+(modit>=no_batches)*(2*no_batches-modit-1)\n",
    "    return ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = Fashion_MNIST_CNN().cuda()\n",
    "#net2=copy.deepcopy(net)\n",
    "#addnet(net2,net)\n",
    "#multiplynet(net2,1/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath=\"output_fashion_low_rank_9n.pickle\"\n",
    "# #filepath=\"output_fashion_low_rank_long.pickle\"\n",
    "# with open(filepath,\"rb\") as file:\n",
    "#    [labels_arr,test_labels_arr,test_prob_arr]=pickle.load(file)\n",
    "# labels_arr=torch.tensor(labels_arr).detach()\n",
    "# test_labels_arr=torch.tensor(test_labels_arr).detach()\n",
    "# test_prob_arr=torch.tensor(test_prob_arr).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net_star=Fashion_MNIST_CNN().cuda()\n",
    "#net_star=net\n",
    "# net_star=copy.deepcopy(net_star_backup)\n",
    "# net_star.train()\n",
    "# with torch.no_grad():\n",
    "#     for par in net_star.parameters():\n",
    "#         par+=(torch.rand_like(par)*2-1)*6/torch.sqrt(l2regconst_extra)\n",
    "\n",
    "def hess_vec(net,vec,images,labels):   \n",
    "    out=net(images)\n",
    "    loss=loss_function(out,labels)*batch_size\n",
    "    net.zero_grad()\n",
    "    grad=list(torch.autograd.grad(loss,list(net.parameters()),create_graph=True))\n",
    "    res=torch.zeros(1).cuda()\n",
    "    for it in range(len_params):\n",
    "        res+=(grad[it]*vec[it]).sum()\n",
    "    net.zero_grad()\n",
    "    hvp=list(torch.autograd.grad(res,list(net.parameters()),create_graph=False))\n",
    "\n",
    "    return(hvp)\n",
    "\n",
    "def hess_vec_full(net,vec):\n",
    "    hvfull=copy.deepcopy(list(vec))\n",
    "    for p in hvfull:\n",
    "        p.requires_grad=False\n",
    "        p*=0\n",
    "    len_pars=len(hvfull)\n",
    "    for it in range(no_batches):\n",
    "        images=images_list[it]\n",
    "        labels=labels_list[it]\n",
    "        hv=hess_vec(net,vec,images,labels)\n",
    "        for pit in range(len_pars):\n",
    "            hvfull[pit]+=hv[pit]\n",
    "    for pit in range(len_pars):\n",
    "        hvfull[pit]+=list(vec)[pit]*l2regconst_extra\n",
    "        if(list_no_bias[pit]):\n",
    "            hvfull[pit]+=list(vec)[pit]*l2regconst\n",
    "    return(hvfull)\n",
    "\n",
    "def norm_par(vec):\n",
    "    n=torch.zeros(1).detach().cuda()\n",
    "    for p in vec:\n",
    "        n+=p.pow(2).sum()\n",
    "    return n.sqrt()\n",
    "\n",
    "def multiply_par(vec,c):\n",
    "    res_vec=copy.deepcopy(vec)\n",
    "    for it in range(len_params):\n",
    "        res_vec[it].requires_grad=False\n",
    "        res_vec[it]=res_vec[it]*c\n",
    "    return res_vec\n",
    "\n",
    "def scalar_prod_par(vec1,vec2):\n",
    "    res=torch.zeros(1).cuda()\n",
    "    for it in range(len_params):\n",
    "        res+=(vec1[it]*vec2[it]).sum()\n",
    "    return res\n",
    "\n",
    "def add_par(vec1,vec2):\n",
    "    res_vec=copy.deepcopy(vec1)\n",
    "    for it in range(len_params):\n",
    "        res_vec[it].requires_grad=False\n",
    "        res_vec[it]=vec1[it]+vec2[it]\n",
    "    return res_vec\n",
    "\n",
    "def max_eigen_hess(net,num_iters=10):\n",
    "    vec=copy.deepcopy(list(net.parameters()))\n",
    "    for p in vec:\n",
    "        p.requires_grad=False\n",
    "    norm_iter=norm_par(vec)\n",
    "    vec=multiply_par(vec,torch.Tensor(1/norm_iter))\n",
    "    for iter in range(num_iters):\n",
    "        vec=hess_vec_full(net,vec)\n",
    "        norm_iter=norm_par(vec)\n",
    "        vec=multiply_par(vec,1/norm_iter)    \n",
    "    return(norm_iter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "u75Xa5VckuTH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "par_it: 0 \n",
      "\n",
      "0 ,0, tensor(22857272.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5285\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.2963\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2481\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2105\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1873\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1750\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1612\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1446\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1283\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1100\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0886\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0702\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0536\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0279\n",
      "0 ,1, tensor(15651.2744)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0205\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0168\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0138\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0174\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0101\n",
      "Epoch [20/20], Step [300/300]\n",
      "0 ,2, tensor(2806.1624)\n",
      "0 ,3, tensor(250442.0156)\n",
      "Epoch [20], Average Loss: 0.0125\n",
      "par_it: 1 \n",
      "\n",
      "1 ,0, tensor(21183114.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5954\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3332\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2632\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2268\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.2030\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1878\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1710\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1460\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1329\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1213\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.1029\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0832\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0552\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0391\n",
      "1 ,1, tensor(26035.2500)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0280\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0202\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0166\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0186\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0158\n",
      "Epoch [20/20], Step [300/300]\n",
      "1 ,2, tensor(5880.4121)\n",
      "1 ,3, tensor(368351.3125)\n",
      "Epoch [20], Average Loss: 0.0158\n",
      "par_it: 2 \n",
      "\n",
      "2 ,0, tensor(24385326.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5421\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3000\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2485\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2177\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1966\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1795\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1594\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1414\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1312\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1112\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0957\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0810\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0506\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0342\n",
      "2 ,1, tensor(16254.5127)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0220\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0175\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0151\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0177\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0150\n",
      "Epoch [20/20], Step [300/300]\n",
      "2 ,2, tensor(3671.9365)\n",
      "2 ,3, tensor(437298.5938)\n",
      "Epoch [20], Average Loss: 0.0097\n",
      "par_it: 3 \n",
      "\n",
      "3 ,0, tensor(15913043.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5002\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.2888\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2419\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2162\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1915\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1626\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1547\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1413\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1225\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1055\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0850\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0653\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0424\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0262\n",
      "3 ,1, tensor(16811.9102)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0164\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0141\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0090\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0128\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0089\n",
      "Epoch [20/20], Step [300/300]\n",
      "3 ,2, tensor(2471.8506)\n",
      "3 ,3, tensor(280017.3438)\n",
      "Epoch [20], Average Loss: 0.0140\n",
      "par_it: 4 \n",
      "\n",
      "4 ,0, tensor(22041578.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5615\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3128\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2528\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2158\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1977\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1787\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1547\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1468\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1272\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1101\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0941\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0759\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0576\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0327\n",
      "4 ,1, tensor(19627.1895)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0241\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0186\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0171\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0132\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0137\n",
      "Epoch [20/20], Step [300/300]\n",
      "4 ,2, tensor(3545.5615)\n",
      "4 ,3, tensor(296396.9688)\n",
      "Epoch [20], Average Loss: 0.0112\n",
      "par_it: 5 \n",
      "\n",
      "5 ,0, tensor(17656568.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5392\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3040\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2454\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2096\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1943\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1743\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1573\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1456\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1241\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1106\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0927\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0657\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0457\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0299\n",
      "5 ,1, tensor(22726.5625)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0183\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0145\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0107\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0144\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0111\n",
      "Epoch [20/20], Step [300/300]\n",
      "5 ,2, tensor(3159.1367)\n",
      "5 ,3, tensor(348824.3438)\n",
      "Epoch [20], Average Loss: 0.0120\n",
      "par_it: 6 \n",
      "\n",
      "6 ,0, tensor(17851020.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5777\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3028\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2423\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2153\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1925\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1725\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1565\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1425\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1274\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1143\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0932\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0735\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0493\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0272\n",
      "6 ,1, tensor(18066.2227)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0199\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0152\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0115\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0134\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0133\n",
      "Epoch [20/20], Step [300/300]\n",
      "6 ,2, tensor(2700.6863)\n",
      "6 ,3, tensor(620409.5625)\n",
      "Epoch [20], Average Loss: 0.0102\n",
      "par_it: 7 \n",
      "\n",
      "7 ,0, tensor(22992720.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5368\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3072\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2525\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2162\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.2009\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1811\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1585\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1491\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1343\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1119\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.1014\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0749\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0534\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0399\n",
      "7 ,1, tensor(20183.6875)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0211\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0182\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0132\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0188\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0126\n",
      "Epoch [20/20], Step [300/300]\n",
      "7 ,2, tensor(3757.8774)\n",
      "7 ,3, tensor(424474.0312)\n",
      "Epoch [20], Average Loss: 0.0110\n",
      "par_it: 8 \n",
      "\n",
      "8 ,0, tensor(20981358.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5310\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3065\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2504\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2189\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1994\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1847\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1661\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1490\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1356\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1181\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0988\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0797\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0551\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0351\n",
      "8 ,1, tensor(22425.7754)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0243\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0195\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0190\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0160\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0164\n",
      "Epoch [20/20], Step [300/300]\n",
      "8 ,2, tensor(4845.5273)\n",
      "8 ,3, tensor(491407.1250)\n",
      "Epoch [20], Average Loss: 0.0074\n",
      "par_it: 9 \n",
      "\n",
      "9 ,0, tensor(20086432.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5064\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.2883\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2479\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2119\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1902\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1724\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1499\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1406\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1206\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1070\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0910\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0758\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0428\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0276\n",
      "9 ,1, tensor(15545.1025)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0191\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0153\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0124\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0135\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0094\n",
      "Epoch [20/20], Step [300/300]\n",
      "9 ,2, tensor(3106.2871)\n",
      "9 ,3, tensor(298891.1250)\n",
      "Epoch [20], Average Loss: 0.0115\n",
      "par_it: 10 \n",
      "\n",
      "10 ,0, tensor(21205478.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5367\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.2991\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2415\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2177\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1907\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1734\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1558\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1474\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1254\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1107\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0963\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0724\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0527\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0274\n",
      "10 ,1, tensor(16541.0977)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0185\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0159\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0126\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0123\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0106\n",
      "Epoch [20/20], Step [300/300]\n",
      "10 ,2, tensor(2423.8088)\n",
      "10 ,3, tensor(817002.6875)\n",
      "Epoch [20], Average Loss: 0.0136\n",
      "par_it: 11 \n",
      "\n",
      "11 ,0, tensor(24397232.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5764\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3170\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2596\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2185\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1985\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1780\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1611\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1539\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1311\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1157\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0955\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0774\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0573\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0281\n",
      "11 ,1, tensor(17676.0762)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0200\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0171\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0123\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0180\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0128\n",
      "Epoch [20/20], Step [300/300]\n",
      "11 ,2, tensor(4262.2002)\n",
      "11 ,3, tensor(319819.9688)\n",
      "Epoch [20], Average Loss: 0.0106\n",
      "par_it: 12 \n",
      "\n",
      "12 ,0, tensor(21950112.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5435\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.2970\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2449\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2198\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1912\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1731\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1563\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1440\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1283\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1120\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0929\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0752\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0498\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0297\n",
      "12 ,1, tensor(20627.2598)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0230\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0177\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0120\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0156\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0129\n",
      "Epoch [20/20], Step [300/300]\n",
      "12 ,2, tensor(3563.0212)\n",
      "12 ,3, tensor(425624.3438)\n",
      "Epoch [20], Average Loss: 0.0109\n",
      "par_it: 13 \n",
      "\n",
      "13 ,0, tensor(28742502.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5268\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3021\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2473\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2191\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1939\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1744\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1679\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1498\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1285\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1081\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0926\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0765\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0553\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0328\n",
      "13 ,1, tensor(14947.5713)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0203\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0163\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0129\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0157\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0091\n",
      "Epoch [20/20], Step [300/300]\n",
      "13 ,2, tensor(2814.3606)\n",
      "13 ,3, tensor(436539.)\n",
      "Epoch [20], Average Loss: 0.0144\n",
      "par_it: 14 \n",
      "\n",
      "14 ,0, tensor(19461064.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.6179\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3409\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2651\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2300\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.2009\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1824\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1640\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1490\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1346\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1183\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.1023\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0797\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0544\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0376\n",
      "14 ,1, tensor(21011.3828)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0272\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0201\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0173\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0137\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0158\n",
      "Epoch [20/20], Step [300/300]\n",
      "14 ,2, tensor(5067.2310)\n",
      "14 ,3, tensor(556249.7500)\n",
      "Epoch [20], Average Loss: 0.0140\n",
      "par_it: 15 \n",
      "\n",
      "15 ,0, tensor(24808850.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5628\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3036\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2491\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2093\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1923\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1729\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1569\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1400\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1260\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1127\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0890\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0751\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0551\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0288\n",
      "15 ,1, tensor(25176.2422)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0210\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0148\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0136\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0135\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0131\n",
      "Epoch [20/20], Step [300/300]\n",
      "15 ,2, tensor(2969.6936)\n",
      "15 ,3, tensor(593027.9375)\n",
      "Epoch [20], Average Loss: 0.0121\n",
      "par_it: 16 \n",
      "\n",
      "16 ,0, tensor(21744020.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5541\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3098\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2574\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2198\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1938\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1769\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1584\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1447\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1294\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1155\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0934\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0722\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0553\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0313\n",
      "16 ,1, tensor(22397.2031)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0227\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0151\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0165\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0140\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0156\n",
      "Epoch [20/20], Step [300/300]\n",
      "16 ,2, tensor(4317.2104)\n",
      "16 ,3, tensor(595631.8125)\n",
      "Epoch [20], Average Loss: 0.0138\n",
      "par_it: 17 \n",
      "\n",
      "17 ,0, tensor(21625908.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5283\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.2965\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2485\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2164\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1898\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1749\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1568\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1416\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1212\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1147\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0896\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0683\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0501\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0308\n",
      "17 ,1, tensor(16580.2734)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0192\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0168\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0125\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0125\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0111\n",
      "Epoch [20/20], Step [300/300]\n",
      "17 ,2, tensor(2832.6433)\n",
      "17 ,3, tensor(348455.9688)\n",
      "Epoch [20], Average Loss: 0.0113\n",
      "par_it: 18 \n",
      "\n",
      "18 ,0, tensor(25858812.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5509\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3171\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2592\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2261\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.2027\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1849\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1630\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1553\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1395\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1181\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0997\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0806\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0617\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0355\n",
      "18 ,1, tensor(21913.2422)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0241\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0228\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0167\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0153\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0150\n",
      "Epoch [20/20], Step [300/300]\n",
      "18 ,2, tensor(4299.4136)\n",
      "18 ,3, tensor(492473.9062)\n",
      "Epoch [20], Average Loss: 0.0128\n",
      "par_it: 19 \n",
      "\n",
      "19 ,0, tensor(19597148.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5056\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.2904\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2411\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2079\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1949\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1721\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1538\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1418\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1275\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1178\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0932\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0752\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0517\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0325\n",
      "19 ,1, tensor(19101.1699)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0209\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0160\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0154\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0133\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0146\n",
      "Epoch [20/20], Step [300/300]\n",
      "19 ,2, tensor(4296.6724)\n",
      "19 ,3, tensor(370393.4062)\n",
      "Epoch [20], Average Loss: 0.0142\n",
      "par_it: 20 \n",
      "\n",
      "20 ,0, tensor(25106308.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5364\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.2985\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2419\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2192\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1977\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1762\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1664\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1447\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1326\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1160\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0927\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0785\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0543\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0356\n",
      "20 ,1, tensor(19391.2539)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0221\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0189\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0147\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0148\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0130\n",
      "Epoch [20/20], Step [300/300]\n",
      "20 ,2, tensor(3420.3374)\n",
      "20 ,3, tensor(380890.9375)\n",
      "Epoch [20], Average Loss: 0.0134\n",
      "par_it: 21 \n",
      "\n",
      "21 ,0, tensor(25056708.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5225\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.2913\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2374\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2097\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1921\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1680\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1531\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1390\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1248\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1107\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0879\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0679\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0483\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0297\n",
      "21 ,1, tensor(29711.6680)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0199\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0127\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0143\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0127\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0137\n",
      "Epoch [20/20], Step [300/300]\n",
      "21 ,2, tensor(3782.2056)\n",
      "21 ,3, tensor(278801.8438)\n",
      "Epoch [20], Average Loss: 0.0114\n",
      "par_it: 22 \n",
      "\n",
      "22 ,0, tensor(16263690.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5595\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3032\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2529\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2168\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1913\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1779\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1640\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1485\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1321\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1118\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0976\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0743\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0525\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0323\n",
      "22 ,1, tensor(22701.3359)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0194\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0165\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0140\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0085\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0092\n",
      "Epoch [20/20], Step [300/300]\n",
      "22 ,2, tensor(3173.7834)\n",
      "22 ,3, tensor(343327.7812)\n",
      "Epoch [20], Average Loss: 0.0155\n",
      "par_it: 23 \n",
      "\n",
      "23 ,0, tensor(20568168.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5964\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3375\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2636\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2392\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.2117\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1898\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1734\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1608\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1404\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1291\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.1085\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0908\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0630\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0416\n",
      "23 ,1, tensor(24867.3906)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0256\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0205\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0206\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0180\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0133\n",
      "Epoch [20/20], Step [300/300]\n",
      "23 ,2, tensor(6140.0269)\n",
      "23 ,3, tensor(444767.1562)\n",
      "Epoch [20], Average Loss: 0.0116\n",
      "par_it: 24 \n",
      "\n",
      "24 ,0, tensor(23938450.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5019\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.2969\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2449\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2133\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1947\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1714\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1587\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1403\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1286\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1085\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0924\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0698\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0472\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0262\n",
      "24 ,1, tensor(15979.7783)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0204\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0147\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0139\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0110\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0132\n",
      "Epoch [20/20], Step [300/300]\n",
      "24 ,2, tensor(2525.6152)\n",
      "24 ,3, tensor(558305.6250)\n",
      "Epoch [20], Average Loss: 0.0147\n",
      "par_it: 25 \n",
      "\n",
      "25 ,0, tensor(22337060.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5901\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3135\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2510\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2119\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1963\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1774\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1638\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1464\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1311\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1112\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0944\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0705\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0503\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0346\n",
      "25 ,1, tensor(19279.0449)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0180\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0187\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0135\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0115\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0115\n",
      "Epoch [20/20], Step [300/300]\n",
      "25 ,2, tensor(2601.9773)\n",
      "25 ,3, tensor(507352.)\n",
      "Epoch [20], Average Loss: 0.0100\n",
      "par_it: 26 \n",
      "\n",
      "26 ,0, tensor(21120620.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.6089\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3188\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2545\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2215\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.2034\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1745\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1653\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1525\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1392\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1186\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0965\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0782\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0580\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0364\n",
      "26 ,1, tensor(21214.8184)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0293\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0170\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0170\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0158\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0130\n",
      "Epoch [20/20], Step [300/300]\n",
      "26 ,2, tensor(4610.9868)\n",
      "26 ,3, tensor(265620.3438)\n",
      "Epoch [20], Average Loss: 0.0143\n",
      "par_it: 27 \n",
      "\n",
      "27 ,0, tensor(19935238.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5145\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.2996\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2436\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2122\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1956\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1733\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1630\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1427\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1270\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1162\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0928\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0763\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0455\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0307\n",
      "27 ,1, tensor(17200.0332)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0192\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0152\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0136\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0126\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0129\n",
      "Epoch [20/20], Step [300/300]\n",
      "27 ,2, tensor(2835.8069)\n",
      "27 ,3, tensor(259081.1250)\n",
      "Epoch [20], Average Loss: 0.0098\n",
      "par_it: 28 \n",
      "\n",
      "28 ,0, tensor(28552980.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5470\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3057\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2466\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2141\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1908\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1706\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1639\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1414\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1287\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1100\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0929\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0834\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0541\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0323\n",
      "28 ,1, tensor(28954.4062)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0217\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0177\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0177\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0149\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0119\n",
      "Epoch [20/20], Step [300/300]\n",
      "28 ,2, tensor(3465.0710)\n",
      "28 ,3, tensor(387042.0312)\n",
      "Epoch [20], Average Loss: 0.0162\n",
      "par_it: 29 \n",
      "\n",
      "29 ,0, tensor(21959994.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5310\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.2979\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2469\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2146\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1958\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1801\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1660\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1500\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1335\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1187\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0945\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0736\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0546\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0347\n",
      "29 ,1, tensor(26205.6914)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0237\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0182\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0146\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0145\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0125\n",
      "Epoch [20/20], Step [300/300]\n",
      "29 ,2, tensor(3403.5115)\n",
      "29 ,3, tensor(856932.8125)\n",
      "Epoch [20], Average Loss: 0.0128\n",
      "par_it: 30 \n",
      "\n",
      "30 ,0, tensor(29249900.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5525\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3103\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2521\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2185\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1986\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1743\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1558\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1445\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1283\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1127\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0961\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0780\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0525\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0328\n",
      "30 ,1, tensor(21507.8105)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0242\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0189\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0162\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0140\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0132\n",
      "Epoch [20/20], Step [300/300]\n",
      "30 ,2, tensor(2364.1782)\n",
      "30 ,3, tensor(434828.6875)\n",
      "Epoch [20], Average Loss: 0.0123\n",
      "par_it: 31 \n",
      "\n",
      "31 ,0, tensor(15254119.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5310\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.2897\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2432\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2096\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1907\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1793\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1525\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1382\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1217\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1051\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0969\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0708\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0534\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0264\n",
      "31 ,1, tensor(17016.4668)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0178\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0153\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0103\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0151\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0105\n",
      "Epoch [20/20], Step [300/300]\n",
      "31 ,2, tensor(2796.8174)\n",
      "31 ,3, tensor(382400.7188)\n",
      "Epoch [20], Average Loss: 0.0108\n",
      "par_it: 32 \n",
      "\n",
      "32 ,0, tensor(20868308.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5446\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.2987\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2511\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2175\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1966\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1759\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1659\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1458\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1353\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1168\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0966\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0740\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0510\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0311\n",
      "32 ,1, tensor(21086.2910)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0216\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0203\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0133\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0136\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0123\n",
      "Epoch [20/20], Step [300/300]\n",
      "32 ,2, tensor(3173.7046)\n",
      "32 ,3, tensor(368207.9688)\n",
      "Epoch [20], Average Loss: 0.0110\n",
      "par_it: 33 \n",
      "\n",
      "33 ,0, tensor(26607692.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5746\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3110\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2488\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2198\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1981\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1780\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1644\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1513\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1402\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1248\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.1007\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0865\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0592\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0398\n",
      "33 ,1, tensor(24786.3672)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0256\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0203\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0133\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0169\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0161\n",
      "Epoch [20/20], Step [300/300]\n",
      "33 ,2, tensor(4924.5986)\n",
      "33 ,3, tensor(733258.8125)\n",
      "Epoch [20], Average Loss: 0.0152\n",
      "par_it: 34 \n",
      "\n",
      "34 ,0, tensor(21977682.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5223\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3054\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2471\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2157\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1991\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1762\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1548\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1413\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1226\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1120\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0825\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0726\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0493\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0268\n",
      "34 ,1, tensor(17537.3594)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0185\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0147\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0143\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0146\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0111\n",
      "Epoch [20/20], Step [300/300]\n",
      "34 ,2, tensor(3058.2290)\n",
      "34 ,3, tensor(381416.5000)\n",
      "Epoch [20], Average Loss: 0.0104\n",
      "par_it: 35 \n",
      "\n",
      "35 ,0, tensor(19120766.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5259\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.2996\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2519\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2183\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1933\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1804\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1660\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1480\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1336\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1139\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.1020\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0830\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0531\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0338\n",
      "35 ,1, tensor(17650.4043)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0236\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0166\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0176\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0164\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0155\n",
      "Epoch [20/20], Step [300/300]\n",
      "35 ,2, tensor(3852.6663)\n",
      "35 ,3, tensor(464291.0312)\n",
      "Epoch [20], Average Loss: 0.0136\n",
      "par_it: 36 \n",
      "\n",
      "36 ,0, tensor(20769590.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5120\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.2930\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2383\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2101\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1853\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1721\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1592\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1409\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1200\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1131\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0840\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0725\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0494\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0312\n",
      "36 ,1, tensor(19278.8828)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0192\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0190\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0102\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0128\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0142\n",
      "Epoch [20/20], Step [300/300]\n",
      "36 ,2, tensor(4076.7192)\n",
      "36 ,3, tensor(249226.0469)\n",
      "Epoch [20], Average Loss: 0.0154\n",
      "par_it: 37 \n",
      "\n",
      "37 ,0, tensor(25577056.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5628\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3224\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2592\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2265\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.2032\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1867\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1661\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1501\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1371\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1184\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0997\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0881\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0578\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0377\n",
      "37 ,1, tensor(21910.2305)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0246\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0200\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0151\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0174\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0117\n",
      "Epoch [20/20], Step [300/300]\n",
      "37 ,2, tensor(4052.7910)\n",
      "37 ,3, tensor(361730.1562)\n",
      "Epoch [20], Average Loss: 0.0121\n",
      "par_it: 38 \n",
      "\n",
      "38 ,0, tensor(18485056.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5733\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3129\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2616\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2260\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.2025\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1823\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1657\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1457\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1331\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1196\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0992\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0792\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0587\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0333\n",
      "38 ,1, tensor(21359.4219)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0239\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0167\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0170\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0170\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0123\n",
      "Epoch [20/20], Step [300/300]\n",
      "38 ,2, tensor(4020.9780)\n",
      "38 ,3, tensor(732261.8750)\n",
      "Epoch [20], Average Loss: 0.0135\n",
      "par_it: 39 \n",
      "\n",
      "39 ,0, tensor(15359809.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5293\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3005\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2482\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2189\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1956\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1777\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1592\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1421\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1284\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1164\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0926\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0744\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0532\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0302\n",
      "39 ,1, tensor(22108.6484)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0177\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0159\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0157\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0086\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0149\n",
      "Epoch [20/20], Step [300/300]\n",
      "39 ,2, tensor(2971.1162)\n",
      "39 ,3, tensor(514259.3438)\n",
      "Epoch [20], Average Loss: 0.0125\n",
      "par_it: 40 \n",
      "\n",
      "40 ,0, tensor(22265910.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5402\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3024\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2451\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2154\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1852\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1776\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1500\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1410\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1226\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1038\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0898\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0697\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0428\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0257\n",
      "40 ,1, tensor(13821.0889)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0167\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0174\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0096\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0105\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0144\n",
      "Epoch [20/20], Step [300/300]\n",
      "40 ,2, tensor(2354.4919)\n",
      "40 ,3, tensor(612082.3125)\n",
      "Epoch [20], Average Loss: 0.0085\n",
      "par_it: 41 \n",
      "\n",
      "41 ,0, tensor(20765668.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5694\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3014\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2551\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2123\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.2015\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1753\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1649\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1475\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1269\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1171\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0939\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0707\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0519\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0311\n",
      "41 ,1, tensor(20721.8730)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0235\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0144\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0132\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0175\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0147\n",
      "Epoch [20/20], Step [300/300]\n",
      "41 ,2, tensor(3397.9194)\n",
      "41 ,3, tensor(651670.9375)\n",
      "Epoch [20], Average Loss: 0.0102\n",
      "par_it: 42 \n",
      "\n",
      "42 ,0, tensor(25746458.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5431\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3106\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2574\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2227\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1995\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1855\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1737\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1533\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1379\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1260\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.1019\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0856\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0638\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0364\n",
      "42 ,1, tensor(20883.3867)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0305\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0203\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0172\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0155\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0163\n",
      "Epoch [20/20], Step [300/300]\n",
      "42 ,2, tensor(4910.8657)\n",
      "42 ,3, tensor(407820.9375)\n",
      "Epoch [20], Average Loss: 0.0150\n",
      "par_it: 43 \n",
      "\n",
      "43 ,0, tensor(29272738.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5176\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3060\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2583\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2143\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1985\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1790\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1597\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1571\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1342\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1180\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0952\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0746\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0577\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0337\n",
      "43 ,1, tensor(20651.7461)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0259\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0191\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0162\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0147\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0181\n",
      "Epoch [20/20], Step [300/300]\n",
      "43 ,2, tensor(4082.2437)\n",
      "43 ,3, tensor(432364.5938)\n",
      "Epoch [20], Average Loss: 0.0131\n",
      "par_it: 44 \n",
      "\n",
      "44 ,0, tensor(21706006.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5758\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3081\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2522\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2225\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1949\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1789\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1653\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1441\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1308\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1209\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0965\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0763\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0533\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0324\n",
      "44 ,1, tensor(18689.4766)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0219\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0168\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0146\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0151\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0140\n",
      "Epoch [20/20], Step [300/300]\n",
      "44 ,2, tensor(4163.9990)\n",
      "44 ,3, tensor(325738.6562)\n",
      "Epoch [20], Average Loss: 0.0120\n",
      "par_it: 45 \n",
      "\n",
      "45 ,0, tensor(14545607.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5492\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3024\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2469\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2126\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1945\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1774\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1647\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1443\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1267\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1153\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0941\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0730\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0552\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0304\n",
      "45 ,1, tensor(18471.4414)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0213\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0176\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0143\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0167\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0101\n",
      "Epoch [20/20], Step [300/300]\n",
      "45 ,2, tensor(2968.5640)\n",
      "45 ,3, tensor(625175.4375)\n",
      "Epoch [20], Average Loss: 0.0125\n",
      "par_it: 46 \n",
      "\n",
      "46 ,0, tensor(20988422.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.5738\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3144\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2502\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2162\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1995\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1808\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1666\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1428\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1434\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1187\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.1046\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0833\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0599\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0348\n",
      "46 ,1, tensor(24187.9395)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0269\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0223\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0174\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0172\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0153\n",
      "Epoch [20/20], Step [300/300]\n",
      "46 ,2, tensor(7191.9468)\n",
      "46 ,3, tensor(481159.3750)\n",
      "Epoch [20], Average Loss: 0.0144\n",
      "par_it: 47 \n",
      "\n",
      "47 ,0, tensor(20880596.)\n",
      "Epoch [1/20], Step [300/300]\n",
      "Epoch [1], Average Loss: 0.6070\n",
      "Epoch [2/20], Step [300/300]\n",
      "Epoch [2], Average Loss: 0.3207\n",
      "Epoch [3/20], Step [300/300]\n",
      "Epoch [3], Average Loss: 0.2443\n",
      "Epoch [4/20], Step [300/300]\n",
      "Epoch [4], Average Loss: 0.2177\n",
      "Epoch [5/20], Step [300/300]\n",
      "Epoch [5], Average Loss: 0.1909\n",
      "Epoch [6/20], Step [300/300]\n",
      "Epoch [6], Average Loss: 0.1767\n",
      "Epoch [7/20], Step [300/300]\n",
      "Epoch [7], Average Loss: 0.1542\n",
      "Epoch [8/20], Step [300/300]\n",
      "Epoch [8], Average Loss: 0.1441\n",
      "Epoch [9/20], Step [300/300]\n",
      "Epoch [9], Average Loss: 0.1261\n",
      "Epoch [10/20], Step [300/300]\n",
      "Epoch [10], Average Loss: 0.1090\n",
      "Epoch [11/20], Step [300/300]\n",
      "Epoch [11], Average Loss: 0.0904\n",
      "Epoch [12/20], Step [300/300]\n",
      "Epoch [12], Average Loss: 0.0698\n",
      "Epoch [13/20], Step [300/300]\n",
      "Epoch [13], Average Loss: 0.0475\n",
      "Epoch [14/20], Step [300/300]\n",
      "Epoch [14], Average Loss: 0.0295\n",
      "47 ,1, tensor(18538.4043)\n",
      "Epoch [15/20], Step [300/300]\n",
      "Epoch [15], Average Loss: 0.0213\n",
      "Epoch [16/20], Step [300/300]\n",
      "Epoch [16], Average Loss: 0.0138\n",
      "Epoch [17/20], Step [300/300]\n",
      "Epoch [17], Average Loss: 0.0155\n",
      "Epoch [18/20], Step [300/300]\n",
      "Epoch [18], Average Loss: 0.0133\n",
      "Epoch [19/20], Step [300/300]\n",
      "Epoch [19], Average Loss: 0.0125\n",
      "Epoch [20/20], Step [300/300]\n",
      "47 ,2, tensor(3487.4717)\n",
      "47 ,3, tensor(310529.4375)\n",
      "Epoch [20], Average Loss: 0.0130\n",
      "par_it: 48 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-d3ce4cda73e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m   \u001b[1;31m#net(torch.cat(images_list[0:20],dim=0).detach())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m   \u001b[1;31m#net.eval()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m   \u001b[0mmax_eig_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpar_it\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_eigen_hess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpar_it\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\",0,\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_eig_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpar_it\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m   \u001b[1;31m#net.train()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-b722d33495d1>\u001b[0m in \u001b[0;36mmax_eigen_hess\u001b[1;34m(net, num_iters)\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mvec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmultiply_par\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnorm_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mvec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhess_vec_full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[0mnorm_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm_par\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mvec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmultiply_par\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnorm_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-b722d33495d1>\u001b[0m in \u001b[0;36mhess_vec_full\u001b[1;34m(net, vec)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mimages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimages_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m         \u001b[0mhv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhess_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen_pars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[0mhvfull\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mhv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-14-b722d33495d1>\u001b[0m in \u001b[0;36mhess_vec\u001b[1;34m(net, vec, images, labels)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mgrad\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mres\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mres\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#@title Output arrays\n",
    "par_runs=64\n",
    "num_classes=10\n",
    "num_epochs=20\n",
    "switch_to_sampling_epoch=20\n",
    "switch_to_swag_epoch=15\n",
    "\n",
    "max_eig_arr=torch.zeros(par_runs,4)\n",
    "\n",
    "num_swag_epochs=switch_to_sampling_epoch-switch_to_swag_epoch\n",
    "\n",
    "training_size=no_batches*batch_size\n",
    "test_size=test_data_len\n",
    "labels_arr=torch.zeros(training_size)\n",
    "test_labels_arr=torch.zeros(test_size)\n",
    "test_prob_arr=torch.zeros([test_size,num_classes,num_epochs,par_runs])\n",
    "\n",
    "lr = 1e-2\n",
    "lr_swag=1e-3\n",
    "h=2.5e-4\n",
    "l2regconst=torch.tensor(1).detach()\n",
    "l2regconst_extra=torch.tensor(50).detach()\n",
    "gam=torch.sqrt(l2regconst_extra)\n",
    "hper2c=hper2const(torch.tensor(h/2),gam)\n",
    "\n",
    "net = Fashion_MNIST_CNN().cuda()\n",
    "len_params=len(list(net.parameters()))\n",
    "list_no_bias=torch.zeros(len_params)\n",
    "pit=0\n",
    "for name, p in net.named_parameters():\n",
    "    if 'bias' not in name:\n",
    "        list_no_bias[pit]=1.0\n",
    "    pit+=1\n",
    "\n",
    "for par_it in range(par_runs):\n",
    "  print(\"par_it:\",par_it,\"\\n\")\n",
    "  #@title Build the model\n",
    "  net=Fashion_MNIST_CNN().cuda()\n",
    "  net.train()\n",
    "  #net(torch.cat(images_list[0:20],dim=0).detach())\n",
    "  #net.eval()\n",
    "  max_eig_arr[par_it,0]=max_eigen_hess(net)\n",
    "  print(par_it,\",0,\", max_eig_arr[par_it,0])    \n",
    "  #net.train()\n",
    "  optimizer = torch.optim.Adam( net.parameters(), lr=lr)\n",
    "  \n",
    "  lr_scheduler = torch.optim.lr_scheduler.PolynomialLR(optimizer=optimizer, total_iters=switch_to_swag_epoch,power=1)\n",
    "\n",
    "\n",
    "  #@title Training the model\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    sum_loss=0\n",
    "    if(epoch==(switch_to_swag_epoch-1)):\n",
    "      net2=copy.deepcopy(net)\n",
    "      multiplynet(net2,0)\n",
    "      optimizer=torch.optim.Adam(net.parameters(),lr=lr_swag)\n",
    "      #net(torch.cat(images_list[0:20],dim=0).detach())\n",
    "      #net.eval()\n",
    "      max_eig_arr[par_it,1]=max_eigen_hess(net)\n",
    "      print(par_it,\",1,\", max_eig_arr[par_it,1])    \n",
    "      #net.train()\n",
    "\n",
    "    for i in range(no_batches): \n",
    "      b=torch.randint(high=no_batches,size=(1,1))\n",
    "      images=images_list[b]\n",
    "      labels=labels_list[b]\n",
    "      \n",
    "\n",
    "      outputs = net(images)    \n",
    "      loss_likelihood = loss_function(outputs, labels)\n",
    "      sum_loss=sum_loss+loss_likelihood    \n",
    "      reg=images_regulariser()\n",
    "      loss=loss_likelihood+reg\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      if(epoch>=(switch_to_swag_epoch)):\n",
    "        addnet(net2,net)\n",
    "\n",
    "    print('Epoch [%d/%d], Step [%d/%d]' %(epoch+1, num_epochs, i+1, no_batches))\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #for imagest,labelst in eval_gen:\n",
    "    if epoch==(num_epochs-1):\n",
    "      multiplynet(net2,1/(num_swag_epochs*no_batches))\n",
    "      multiplynet(net,0)\n",
    "      addnet(net,net2)\n",
    "      del net2\n",
    "\n",
    "      net_star=copy.deepcopy(net)\n",
    "      net_star.train()\n",
    "      #net_star(torch.cat(images_list[0:20],dim=0).detach())\n",
    "      #net_star.eval()\n",
    "      max_eig_arr[par_it,2]=max_eigen_hess(net_star)     \n",
    "      print(par_it,\",2,\", max_eig_arr[par_it,2])    \n",
    "      #net_star.train()      \n",
    "      with torch.no_grad():\n",
    "        for par in net_star.parameters():\n",
    "          par+=(torch.randn_like(par))/torch.sqrt(l2regconst_extra)\n",
    "      #net_star(torch.cat(images_list[0:20],dim=0).detach())\n",
    "      #net_star.eval()\n",
    "      net_star.train()\n",
    "      max_eig_arr[par_it,3]=max_eigen_hess(net_star)\n",
    "      print(par_it,\",3,\", max_eig_arr[par_it,3])    \n",
    "    if(epoch<=switch_to_swag_epoch):\n",
    "      lr_scheduler.step()\n",
    "    print('Epoch [%d], Average Loss: %0.4f' %(epoch+1, sum_loss/no_batches))\n",
    "  \n",
    "  \n",
    "  filepath=\"max_eigen_fashion2.pickle\"\n",
    "  with open(filepath,\"wb\") as file:\n",
    "    pickle.dump([max_eig_arr.numpy()],file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"max_eigen_fashion2.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "    pickle.dump([max_eig_arr.numpy()],file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1265258"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=0\n",
    "for par in net.parameters():\n",
    "    n+=par.numel()\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1165.5272], device='cuda:0')\n",
      "tensor([755119.0625], device='cuda:0')\n",
      "tensor([1553897.], device='cuda:0')\n",
      "tensor([1891544.7500], device='cuda:0')\n",
      "tensor([2051748.5000], device='cuda:0')\n",
      "tensor([2121906.2500], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-67bb6d07e69e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[0mvec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmultiply_par\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnorm_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[0mvec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhess_vec_full\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m     \u001b[0mnorm_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm_par\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m     \u001b[0mvec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmultiply_par\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnorm_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-67bb6d07e69e>\u001b[0m in \u001b[0;36mhess_vec_full\u001b[1;34m(vec)\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[0mimages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimages_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mlabels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m         \u001b[0mhv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhess_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mpit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen_pars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m             \u001b[0mhvfull\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mhv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mpit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-58-67bb6d07e69e>\u001b[0m in \u001b[0;36mhess_vec\u001b[1;34m(vec, images, labels)\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[0mres\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mvec\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mnet_star\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[0mhvp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mres\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet_star\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhvp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Daniel Paulin\\.conda\\envs\\torchenv39\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    410\u001b[0m         )\n\u001b[0;32m    411\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m         result = _engine_run_backward(\n\u001b[0m\u001b[0;32m    413\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m             \u001b[0mgrad_outputs_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Daniel Paulin\\.conda\\envs\\torchenv39\\lib\\site-packages\\torch\\autograd\\graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    743\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 744\u001b[1;33m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath=\"output_fashion.pickle\"\n",
    "#with open(filepath,\"rb\") as file:\n",
    "#    [labels_arr,test_labels_arr,test_prob_arr,_,_]=pickle.load(file)\n",
    "#labels_arr=torch.tensor(labels_arr).detach()\n",
    "#test_labels_arr=torch.tensor(test_labels_arr).detach()\n",
    "#test_prob_arr=torch.tensor(test_prob_arr).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome should be a binary list of the ordinal outcome. [0, 1, 0] for exmaple.\n",
    "# Probs should be a list of probabilities. [0.79, 0.09, 0.12] for example.\n",
    "# Outcome and Probs must be provided with the same order as probabilities.\n",
    "\n",
    "def rps_single(probs, true_label):\n",
    "    outcome=torch.zeros(num_classes)\n",
    "    outcome[true_label.int()]=1.0\n",
    "    cum_probs = torch.cumsum(probs,0)\n",
    "    cum_outcomes = torch.cumsum(outcome,0)\n",
    "    \n",
    "    #print(cum_outcomes)\n",
    "    #print(cum_probs)\n",
    "    sum_rps = 0\n",
    "    for i in range(len(outcome)):         \n",
    "        sum_rps+= (cum_probs[i] - cum_outcomes[i])**2\n",
    "    \n",
    "    return sum_rps/(num_classes-1)\n",
    "\n",
    "def rps_calc(test_probs, true_labels):\n",
    "    rps_vec=torch.zeros(test_data_len)\n",
    "    for it in range(test_data_len):\n",
    "        rps_vec[it]=rps_single(test_probs[it,:].reshape(num_classes),true_labels[it])\n",
    "    return rps_vec\n",
    "\n",
    "def nll_calc(test_probs, true_labels):\n",
    "    res=0\n",
    "    for it in range(test_data_len):\n",
    "        res-=torch.max(torch.tensor([torch.log(test_probs[it,true_labels[it].int()]),-100]))\n",
    "    return res/test_data_len\n",
    "\n",
    "def adaptive_calibration_error(test_probs,true_labels, num_bins=20):\n",
    "    max_probs, predicted_labels = torch.max(test_probs,1)\n",
    "    ind=torch.argsort(max_probs,stable=True)\n",
    "    sorted_max_probs=max_probs[ind]\n",
    "    sorted_predicted_labels=predicted_labels[ind]\n",
    "    sorted_true_labels=true_labels[ind]\n",
    "\n",
    "    correct = (sorted_predicted_labels == sorted_true_labels).clone().detach().float()\n",
    "    bins=(torch.tensor(range(test_data_len))/torch.tensor(test_data_len/num_bins)).floor()\n",
    "\n",
    "    o=torch.tensor(0.0)\n",
    "    for b in range(num_bins):\n",
    "        mask = (bins == b)\n",
    "        if torch.any(mask):\n",
    "            #print(b, sorted_max_probs[mask].mean(), (correct[mask] - sorted_max_probs[mask]).mean())\n",
    "            o += (correct[mask] - sorted_max_probs[mask]).mean().abs()\n",
    "\n",
    "    return o / num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chain:  0 /epoch: 0\n",
      "NLL: tensor(0.3925)\n",
      "chain:  0 /epoch: 1\n",
      "NLL: tensor(0.4747)\n",
      "chain:  0 /epoch: 2\n",
      "NLL: tensor(0.4271)\n",
      "chain:  0 /epoch: 3\n",
      "NLL: tensor(0.3360)\n",
      "chain:  0 /epoch: 4\n",
      "NLL: tensor(0.2935)\n",
      "chain:  0 /epoch: 5\n",
      "NLL: tensor(0.3111)\n",
      "chain:  0 /epoch: 6\n",
      "NLL: tensor(0.2907)\n",
      "chain:  0 /epoch: 7\n",
      "NLL: tensor(0.2972)\n",
      "chain:  0 /epoch: 8\n",
      "NLL: tensor(0.2928)\n",
      "chain:  0 /epoch: 9\n",
      "NLL: tensor(0.3098)\n",
      "chain:  0 /epoch: 10\n",
      "NLL: tensor(0.2950)\n",
      "chain:  0 /epoch: 11\n",
      "NLL: tensor(0.3044)\n",
      "chain:  0 /epoch: 12\n",
      "NLL: tensor(0.3103)\n",
      "chain:  0 /epoch: 13\n",
      "NLL: tensor(0.3670)\n",
      "chain:  0 /epoch: 14\n",
      "NLL: tensor(0.3080)\n",
      "chain:  0 /epoch: 15\n",
      "NLL: tensor(0.3193)\n",
      "chain:  0 /epoch: 16\n",
      "NLL: tensor(0.3421)\n",
      "chain:  0 /epoch: 17\n",
      "NLL: tensor(0.2957)\n",
      "chain:  0 /epoch: 18\n",
      "NLL: tensor(0.3090)\n",
      "chain:  0 /epoch: 19\n",
      "NLL: tensor(0.3250)\n",
      "chain:  0 /epoch: 20\n",
      "NLL: tensor(0.3070)\n",
      "chain:  0 /epoch: 21\n",
      "NLL: tensor(0.2956)\n",
      "chain:  0 /epoch: 22\n",
      "NLL: tensor(0.3052)\n",
      "chain:  0 /epoch: 23\n",
      "NLL: tensor(0.3219)\n",
      "chain:  0 /epoch: 24\n",
      "NLL: tensor(0.3270)\n",
      "chain:  0 /epoch: 25\n",
      "NLL: tensor(0.3110)\n",
      "chain:  0 /epoch: 26\n",
      "NLL: tensor(0.2939)\n",
      "chain:  0 /epoch: 27\n",
      "NLL: tensor(0.2907)\n",
      "chain:  0 /epoch: 28\n",
      "NLL: tensor(0.2879)\n",
      "chain:  0 /epoch: 29\n",
      "NLL: tensor(0.3057)\n",
      "chain:  0 /epoch: 30\n",
      "NLL: tensor(0.3204)\n",
      "chain:  0 /epoch: 31\n",
      "NLL: tensor(0.3001)\n",
      "chain:  0 /epoch: 32\n",
      "NLL: tensor(0.2755)\n",
      "chain:  0 /epoch: 33\n",
      "NLL: tensor(0.2904)\n",
      "chain:  0 /epoch: 34\n",
      "NLL: tensor(0.3180)\n",
      "chain:  0 /epoch: 35\n",
      "NLL: tensor(0.3047)\n",
      "chain:  0 /epoch: 36\n",
      "NLL: tensor(0.2775)\n",
      "chain:  0 /epoch: 37\n",
      "NLL: tensor(0.3241)\n",
      "chain:  0 /epoch: 38\n",
      "NLL: tensor(0.3131)\n",
      "chain:  0 /epoch: 39\n",
      "NLL: tensor(0.3243)\n",
      "chain:  1 /epoch: 0\n",
      "NLL: tensor(0.3973)\n",
      "chain:  1 /epoch: 1\n",
      "NLL: tensor(0.4840)\n",
      "chain:  1 /epoch: 2\n",
      "NLL: tensor(0.3927)\n",
      "chain:  1 /epoch: 3\n",
      "NLL: tensor(0.3250)\n",
      "chain:  1 /epoch: 4\n",
      "NLL: tensor(0.3070)\n",
      "chain:  1 /epoch: 5\n",
      "NLL: tensor(0.2899)\n",
      "chain:  1 /epoch: 6\n",
      "NLL: tensor(0.2719)\n",
      "chain:  1 /epoch: 7\n",
      "NLL: tensor(0.2889)\n",
      "chain:  1 /epoch: 8\n",
      "NLL: tensor(0.2912)\n",
      "chain:  1 /epoch: 9\n",
      "NLL: tensor(0.3269)\n",
      "chain:  1 /epoch: 10\n",
      "NLL: tensor(0.2880)\n",
      "chain:  1 /epoch: 11\n",
      "NLL: tensor(0.2835)\n",
      "chain:  1 /epoch: 12\n",
      "NLL: tensor(0.3406)\n",
      "chain:  1 /epoch: 13\n",
      "NLL: tensor(0.3049)\n",
      "chain:  1 /epoch: 14\n",
      "NLL: tensor(0.3108)\n",
      "chain:  1 /epoch: 15\n",
      "NLL: tensor(0.3079)\n",
      "chain:  1 /epoch: 16\n",
      "NLL: tensor(0.2874)\n",
      "chain:  1 /epoch: 17\n",
      "NLL: tensor(0.2836)\n",
      "chain:  1 /epoch: 18\n",
      "NLL: tensor(0.2841)\n",
      "chain:  1 /epoch: 19\n",
      "NLL: tensor(0.2761)\n",
      "chain:  1 /epoch: 20\n",
      "NLL: tensor(0.2922)\n",
      "chain:  1 /epoch: 21\n",
      "NLL: tensor(0.3422)\n",
      "chain:  1 /epoch: 22\n",
      "NLL: tensor(0.2898)\n",
      "chain:  1 /epoch: 23\n",
      "NLL: tensor(0.3360)\n",
      "chain:  1 /epoch: 24\n",
      "NLL: tensor(0.2738)\n",
      "chain:  1 /epoch: 25\n",
      "NLL: tensor(0.3107)\n",
      "chain:  1 /epoch: 26\n",
      "NLL: tensor(0.2838)\n",
      "chain:  1 /epoch: 27\n",
      "NLL: tensor(0.3114)\n",
      "chain:  1 /epoch: 28\n",
      "NLL: tensor(0.2853)\n",
      "chain:  1 /epoch: 29\n",
      "NLL: tensor(0.3351)\n",
      "chain:  1 /epoch: 30\n",
      "NLL: tensor(0.3007)\n",
      "chain:  1 /epoch: 31\n",
      "NLL: tensor(0.3034)\n",
      "chain:  1 /epoch: 32\n",
      "NLL: tensor(0.3095)\n",
      "chain:  1 /epoch: 33\n",
      "NLL: tensor(0.3517)\n",
      "chain:  1 /epoch: 34\n",
      "NLL: tensor(0.3104)\n",
      "chain:  1 /epoch: 35\n",
      "NLL: tensor(0.2936)\n",
      "chain:  1 /epoch: 36\n",
      "NLL: tensor(0.2830)\n",
      "chain:  1 /epoch: 37\n",
      "NLL: tensor(0.3254)\n",
      "chain:  1 /epoch: 38\n",
      "NLL: tensor(0.2974)\n",
      "chain:  1 /epoch: 39\n",
      "NLL: tensor(0.3165)\n",
      "chain:  2 /epoch: 0\n",
      "NLL: tensor(0.4106)\n",
      "chain:  2 /epoch: 1\n",
      "NLL: tensor(0.4778)\n",
      "chain:  2 /epoch: 2\n",
      "NLL: tensor(0.4715)\n",
      "chain:  2 /epoch: 3\n",
      "NLL: tensor(0.3729)\n",
      "chain:  2 /epoch: 4\n",
      "NLL: tensor(0.3136)\n",
      "chain:  2 /epoch: 5\n",
      "NLL: tensor(0.3075)\n",
      "chain:  2 /epoch: 6\n",
      "NLL: tensor(0.2825)\n",
      "chain:  2 /epoch: 7\n",
      "NLL: tensor(0.3087)\n",
      "chain:  2 /epoch: 8\n",
      "NLL: tensor(0.3109)\n",
      "chain:  2 /epoch: 9\n",
      "NLL: tensor(0.2933)\n",
      "chain:  2 /epoch: 10\n",
      "NLL: tensor(0.3065)\n",
      "chain:  2 /epoch: 11\n",
      "NLL: tensor(0.3094)\n",
      "chain:  2 /epoch: 12\n",
      "NLL: tensor(0.3148)\n",
      "chain:  2 /epoch: 13\n",
      "NLL: tensor(0.3220)\n",
      "chain:  2 /epoch: 14\n",
      "NLL: tensor(0.3105)\n",
      "chain:  2 /epoch: 15\n",
      "NLL: tensor(0.3091)\n",
      "chain:  2 /epoch: 16\n",
      "NLL: tensor(0.3106)\n",
      "chain:  2 /epoch: 17\n",
      "NLL: tensor(0.3072)\n",
      "chain:  2 /epoch: 18\n",
      "NLL: tensor(0.2891)\n",
      "chain:  2 /epoch: 19\n",
      "NLL: tensor(0.3079)\n",
      "chain:  2 /epoch: 20\n",
      "NLL: tensor(0.2813)\n",
      "chain:  2 /epoch: 21\n",
      "NLL: tensor(0.3171)\n",
      "chain:  2 /epoch: 22\n",
      "NLL: tensor(0.3306)\n",
      "chain:  2 /epoch: 23\n",
      "NLL: tensor(0.2943)\n",
      "chain:  2 /epoch: 24\n",
      "NLL: tensor(0.2783)\n",
      "chain:  2 /epoch: 25\n",
      "NLL: tensor(0.3132)\n",
      "chain:  2 /epoch: 26\n",
      "NLL: tensor(0.2523)\n",
      "chain:  2 /epoch: 27\n",
      "NLL: tensor(0.3163)\n",
      "chain:  2 /epoch: 28\n",
      "NLL: tensor(0.2973)\n",
      "chain:  2 /epoch: 29\n",
      "NLL: tensor(0.3094)\n",
      "chain:  2 /epoch: 30\n",
      "NLL: tensor(0.3063)\n",
      "chain:  2 /epoch: 31\n",
      "NLL: tensor(0.2916)\n",
      "chain:  2 /epoch: 32\n",
      "NLL: tensor(0.2926)\n",
      "chain:  2 /epoch: 33\n",
      "NLL: tensor(0.3222)\n",
      "chain:  2 /epoch: 34\n",
      "NLL: tensor(0.2772)\n",
      "chain:  2 /epoch: 35\n",
      "NLL: tensor(0.3255)\n",
      "chain:  2 /epoch: 36\n",
      "NLL: tensor(0.2922)\n",
      "chain:  2 /epoch: 37\n",
      "NLL: tensor(0.2813)\n",
      "chain:  2 /epoch: 38\n",
      "NLL: tensor(0.2899)\n",
      "chain:  2 /epoch: 39\n",
      "NLL: tensor(0.3027)\n",
      "chain:  3 /epoch: 0\n",
      "NLL: tensor(0.3835)\n",
      "chain:  3 /epoch: 1\n",
      "NLL: tensor(0.4546)\n",
      "chain:  3 /epoch: 2\n",
      "NLL: tensor(0.4224)\n",
      "chain:  3 /epoch: 3\n",
      "NLL: tensor(0.3178)\n",
      "chain:  3 /epoch: 4\n",
      "NLL: tensor(0.3049)\n",
      "chain:  3 /epoch: 5\n",
      "NLL: tensor(0.2976)\n",
      "chain:  3 /epoch: 6\n",
      "NLL: tensor(0.3153)\n",
      "chain:  3 /epoch: 7\n",
      "NLL: tensor(0.3228)\n",
      "chain:  3 /epoch: 8\n",
      "NLL: tensor(0.2887)\n",
      "chain:  3 /epoch: 9\n",
      "NLL: tensor(0.2946)\n",
      "chain:  3 /epoch: 10\n",
      "NLL: tensor(0.3221)\n",
      "chain:  3 /epoch: 11\n",
      "NLL: tensor(0.3399)\n",
      "chain:  3 /epoch: 12\n",
      "NLL: tensor(0.3124)\n",
      "chain:  3 /epoch: 13\n",
      "NLL: tensor(0.3286)\n",
      "chain:  3 /epoch: 14\n",
      "NLL: tensor(0.3156)\n",
      "chain:  3 /epoch: 15\n",
      "NLL: tensor(0.2945)\n",
      "chain:  3 /epoch: 16\n",
      "NLL: tensor(0.3040)\n",
      "chain:  3 /epoch: 17\n",
      "NLL: tensor(0.3215)\n",
      "chain:  3 /epoch: 18\n",
      "NLL: tensor(0.2982)\n",
      "chain:  3 /epoch: 19\n",
      "NLL: tensor(0.3249)\n",
      "chain:  3 /epoch: 20\n",
      "NLL: tensor(0.2636)\n",
      "chain:  3 /epoch: 21\n",
      "NLL: tensor(0.3388)\n",
      "chain:  3 /epoch: 22\n",
      "NLL: tensor(0.2906)\n",
      "chain:  3 /epoch: 23\n",
      "NLL: tensor(0.3284)\n",
      "chain:  3 /epoch: 24\n",
      "NLL: tensor(0.3186)\n",
      "chain:  3 /epoch: 25\n",
      "NLL: tensor(0.3245)\n",
      "chain:  3 /epoch: 26\n",
      "NLL: tensor(0.3290)\n",
      "chain:  3 /epoch: 27\n",
      "NLL: tensor(0.2993)\n",
      "chain:  3 /epoch: 28\n",
      "NLL: tensor(0.3038)\n",
      "chain:  3 /epoch: 29\n",
      "NLL: tensor(0.3101)\n",
      "chain:  3 /epoch: 30\n",
      "NLL: tensor(0.3287)\n",
      "chain:  3 /epoch: 31\n",
      "NLL: tensor(0.2987)\n",
      "chain:  3 /epoch: 32\n",
      "NLL: tensor(0.3053)\n",
      "chain:  3 /epoch: 33\n",
      "NLL: tensor(0.2929)\n",
      "chain:  3 /epoch: 34\n",
      "NLL: tensor(0.2737)\n",
      "chain:  3 /epoch: 35\n",
      "NLL: tensor(0.3095)\n",
      "chain:  3 /epoch: 36\n",
      "NLL: tensor(0.2810)\n",
      "chain:  3 /epoch: 37\n",
      "NLL: tensor(0.2922)\n",
      "chain:  3 /epoch: 38\n",
      "NLL: tensor(0.3069)\n",
      "chain:  3 /epoch: 39\n",
      "NLL: tensor(0.2972)\n",
      "tensor(0.9780)\n"
     ]
    }
   ],
   "source": [
    "def GRdiagnostics(res):\n",
    "  J=res.shape[0] #Number of chains\n",
    "  L=res.shape[1] #Number of samples after burnin\n",
    "  res_means=res.mean(dim=1)\n",
    "  res_mean=res_means.mean()\n",
    "  B=(res_means-res_mean).pow(2).sum()*L/(J-1)\n",
    "  W=(res_means.reshape([J,1])@torch.ones([1,L])-res).pow(2).sum()/(J*(L-1))\n",
    "  R=(W*(L-1)/L+B/L)/W\n",
    "  return R\n",
    "\n",
    "\n",
    "par_chains=4\n",
    "no_GR_epochs=40\n",
    "test_prob_GR_arr=torch.zeros([test_size,num_classes])\n",
    "nll_GR_arr=torch.zeros([par_chains,no_GR_epochs])\n",
    "for chain in range(par_chains):\n",
    "    net=copy.deepcopy(net_star)\n",
    "    net.eval()\n",
    "    for par in list(net.parameters()):\n",
    "      par.v = torch.randn_like(par,device=device)          \n",
    "    for epoch in range(no_GR_epochs):\n",
    "      print(\"chain: \",chain, \"/epoch:\",epoch)\n",
    "      if(epoch % 2 == 1):\n",
    "        irange=range(no_batches-1,-1,-1)\n",
    "      else:\n",
    "        irange=range(no_batches)\n",
    "      for b in irange:\n",
    "        images=images_list[b]\n",
    "        labels=labels_list[b]\n",
    "        UBU_step(hper2c,images,labels,b)\n",
    "\n",
    "      for testit in range(test_no_batches):\n",
    "        imagest=test_images_list[testit]\n",
    "        labelst=test_labels_list[testit]\n",
    "        actual_batch_size=len(imagest)\n",
    "        outputt = net(imagest).detach()\n",
    "        test_prob_GR_arr[(testit*batch_size):(testit*batch_size+actual_batch_size),:]=torch.softmax(outputt,dim=1)\n",
    "      \n",
    "      nll_GR_arr[chain,epoch]=nll_calc(test_prob_GR_arr,test_labels_arr)\n",
    "      print(\"NLL:\", nll_GR_arr[chain,epoch])\n",
    "\n",
    "print(GRdiagnostics(nll_GR_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_prob=torch.Tensor(test_prob_arr[:,:,29,0]).reshape(test_size,num_classes)\n",
    "#torch.cumsum(test_prob[1,:].reshape(num_classes),0)\n",
    "#rps_single(test_prob[1,:].reshape(num_classes),test_labels_arr[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Bayesian, ensemble size: 1\n",
      "mean accuracy: tensor(0.9320) std: tensor(nan)\n",
      "mean ace: tensor(0.0440) std: tensor(nan)\n",
      "mean nll: tensor(0.3028) std: tensor(nan)\n",
      "mean rps: tensor(0.0213) std: tensor(nan)\n",
      "Non-Bayesian, ensemble size: 2\n",
      "mean accuracy: tensor(nan) std: tensor(nan)\n",
      "mean ace: tensor(nan) std: tensor(nan)\n",
      "mean nll: tensor(nan) std: tensor(nan)\n",
      "mean rps: tensor(nan) std: tensor(nan)\n",
      "Non-Bayesian, ensemble size: 4\n",
      "mean accuracy: tensor(nan) std: tensor(nan)\n",
      "mean ace: tensor(nan) std: tensor(nan)\n",
      "mean nll: tensor(nan) std: tensor(nan)\n",
      "mean rps: tensor(nan) std: tensor(nan)\n",
      "Non-Bayesian, ensemble size: 8\n",
      "mean accuracy: tensor(nan) std: tensor(nan)\n",
      "mean ace: tensor(nan) std: tensor(nan)\n",
      "mean nll: tensor(nan) std: tensor(nan)\n",
      "mean rps: tensor(nan) std: tensor(nan)\n",
      "Non-Bayesian, ensemble size: 16\n",
      "mean accuracy: tensor(nan) std: tensor(nan)\n",
      "mean ace: tensor(nan) std: tensor(nan)\n",
      "mean nll: tensor(nan) std: tensor(nan)\n",
      "mean rps: tensor(nan) std: tensor(nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-5f500bdf8e3f>:18: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  print(\"mean accuracy:\",accuracy_arr.mean(),\"std:\",accuracy_arr.std())\n",
      "<ipython-input-18-5f500bdf8e3f>:19: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  print(\"mean ace:\",ace_arr.mean(),\"std:\",ace_arr.std())\n",
      "<ipython-input-18-5f500bdf8e3f>:20: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  print(\"mean nll:\",nll_arr.mean(),\"std:\",nll_arr.std())\n",
      "<ipython-input-18-5f500bdf8e3f>:21: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  print(\"mean rps:\",rps_arr.mean(),\"std:\",rps_arr.std())\n",
      "<ipython-input-18-5f500bdf8e3f>:22: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  return [accuracy_arr.mean(),accuracy_arr.std(),ace_arr.mean(),ace_arr.std(),rps_arr.mean(),rps_arr.std(),nll_arr.mean(),nll_arr.std()]\n"
     ]
    }
   ],
   "source": [
    "#no bayesian\n",
    "par_runs=1\n",
    "def compute_acc_ace_rps_no_bayes(es):\n",
    "    copies=int(par_runs/es)\n",
    "    ace_arr=torch.zeros(copies)\n",
    "    rps_arr=torch.zeros(copies)\n",
    "    nll_arr=torch.zeros(copies)\n",
    "    accuracy_arr=torch.zeros(copies)\n",
    "\n",
    "    for it in range(copies):\n",
    "        test_prob=torch.Tensor(test_prob_arr[:,:,14,it*es:(it+1)*es]).mean(-1).reshape(test_size,num_classes)\n",
    "        ace_arr[it]=adaptive_calibration_error(test_prob,test_labels_arr)\n",
    "        rps_arr[it]=(rps_calc(test_prob, test_labels_arr)).mean()\n",
    "        nll_arr[it]=nll_calc(test_prob, test_labels_arr)\n",
    "        _, predictedt = torch.max(test_prob,1)\n",
    "        accuracy_arr[it]= (predictedt==test_labels_arr.reshape(1,test_size)).sum()/test_size\n",
    "    print(\"Non-Bayesian, ensemble size:\", es)\n",
    "    print(\"mean accuracy:\",accuracy_arr.mean(),\"std:\",accuracy_arr.std())\n",
    "    print(\"mean ace:\",ace_arr.mean(),\"std:\",ace_arr.std())\n",
    "    print(\"mean nll:\",nll_arr.mean(),\"std:\",nll_arr.std())\n",
    "    print(\"mean rps:\",rps_arr.mean(),\"std:\",rps_arr.std())\n",
    "    return [accuracy_arr.mean(),accuracy_arr.std(),ace_arr.mean(),ace_arr.std(),rps_arr.mean(),rps_arr.std(),nll_arr.mean(),nll_arr.std()]\n",
    "\n",
    "acc=torch.zeros(5)\n",
    "acc_std=torch.zeros(5)\n",
    "ace=torch.zeros(5)\n",
    "ace_std=torch.zeros(5)\n",
    "rps=torch.zeros(5)\n",
    "rps_std=torch.zeros(5)\n",
    "nll=torch.zeros(5)\n",
    "nll_std=torch.zeros(5)\n",
    "[acc[0],acc_std[0],ace[0],ace_std[0],rps[0],rps_std[0],nll[0],nll_std[0]]=compute_acc_ace_rps_no_bayes(1)\n",
    "[acc[1],acc_std[1],ace[1],ace_std[1],rps[1],rps_std[1],nll[1],nll_std[1]]=compute_acc_ace_rps_no_bayes(2)\n",
    "[acc[2],acc_std[2],ace[2],ace_std[2],rps[2],rps_std[2],nll[2],nll_std[2]]=compute_acc_ace_rps_no_bayes(4)\n",
    "[acc[3],acc_std[3],ace[3],ace_std[3],rps[3],rps_std[3],nll[3],nll_std[3]]=compute_acc_ace_rps_no_bayes(8)\n",
    "[acc[4],acc_std[4],ace[4],ace_std[4],rps[4],rps_std[4],nll[4],nll_std[4]]=compute_acc_ace_rps_no_bayes(16)\n",
    "\n",
    "# from scipy.io import savemat\n",
    "# filepath=\"results_fashion_no_bayes_rand_test.mat\"\n",
    "# mdic={\"acc\":acc.cpu().numpy(),\"acc_std\":acc_std.cpu().numpy(),\"nll\": nll.cpu().numpy(),\"nll_std\":nll_std.cpu().numpy(),\\\n",
    "#       \"ace\": ace.cpu().numpy(),\"ace_std\":ace_std.cpu().numpy(), \"rps\":rps.cpu().numpy(),\"rps_std\":rps_std.cpu().numpy()}\n",
    "# savemat(filepath,mdic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-Bayesian, ensemble size: 1\n",
    "mean accuracy: tensor(0.9296) std: tensor(0.0020)\n",
    "mean ace: tensor(0.0555) std: tensor(0.0018)\n",
    "mean nll: tensor(0.4709) std: tensor(0.0251)\n",
    "mean rps: tensor(0.0230) std: tensor(0.0008)\n",
    "Non-Bayesian, ensemble size: 2\n",
    "mean accuracy: tensor(0.9383) std: tensor(0.0015)\n",
    "mean ace: tensor(0.0294) std: tensor(0.0016)\n",
    "mean nll: tensor(0.3109) std: tensor(0.0127)\n",
    "mean rps: tensor(0.0188) std: tensor(0.0005)\n",
    "Non-Bayesian, ensemble size: 4\n",
    "mean accuracy: tensor(0.9422) std: tensor(0.0010)\n",
    "mean ace: tensor(0.0177) std: tensor(0.0011)\n",
    "mean nll: tensor(0.2385) std: tensor(0.0066)\n",
    "mean rps: tensor(0.0168) std: tensor(0.0003)\n",
    "Non-Bayesian, ensemble size: 8\n",
    "mean accuracy: tensor(0.9443) std: tensor(0.0006)\n",
    "mean ace: tensor(0.0126) std: tensor(0.0007)\n",
    "mean nll: tensor(0.2025) std: tensor(0.0042)\n",
    "mean rps: tensor(0.0157) std: tensor(0.0002)\n",
    "Non-Bayesian, ensemble size: 16\n",
    "mean accuracy: tensor(0.9459) std: tensor(0.0009)\n",
    "mean ace: tensor(0.0098) std: tensor(0.0007)\n",
    "mean nll: tensor(0.1832) std: tensor(0.0029)\n",
    "mean rps: tensor(0.0152) std: tensor(8.2395e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWA, ensemble size: 1\n",
      "mean accuracy: tensor(0.9357) std: tensor(nan)\n",
      "mean ace: tensor(0.0458) std: tensor(nan)\n",
      "mean nll: tensor(0.3382) std: tensor(nan)\n",
      "mean rps: tensor(0.0209) std: tensor(nan)\n",
      "SWA, ensemble size: 2\n",
      "mean accuracy: tensor(nan) std: tensor(nan)\n",
      "mean ace: tensor(nan) std: tensor(nan)\n",
      "mean nll: tensor(nan) std: tensor(nan)\n",
      "mean rps: tensor(nan) std: tensor(nan)\n",
      "SWA, ensemble size: 4\n",
      "mean accuracy: tensor(nan) std: tensor(nan)\n",
      "mean ace: tensor(nan) std: tensor(nan)\n",
      "mean nll: tensor(nan) std: tensor(nan)\n",
      "mean rps: tensor(nan) std: tensor(nan)\n",
      "SWA, ensemble size: 8\n",
      "mean accuracy: tensor(nan) std: tensor(nan)\n",
      "mean ace: tensor(nan) std: tensor(nan)\n",
      "mean nll: tensor(nan) std: tensor(nan)\n",
      "mean rps: tensor(nan) std: tensor(nan)\n",
      "SWA, ensemble size: 16\n",
      "mean accuracy: tensor(nan) std: tensor(nan)\n",
      "mean ace: tensor(nan) std: tensor(nan)\n",
      "mean nll: tensor(nan) std: tensor(nan)\n",
      "mean rps: tensor(nan) std: tensor(nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-19-584ea30b4abe>:17: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  print(\"mean accuracy:\",accuracy_arr.mean(),\"std:\",accuracy_arr.std())\n",
      "<ipython-input-19-584ea30b4abe>:18: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  print(\"mean ace:\",ace_arr.mean(),\"std:\",ace_arr.std())\n",
      "<ipython-input-19-584ea30b4abe>:19: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  print(\"mean nll:\",nll_arr.mean(),\"std:\",nll_arr.std())\n",
      "<ipython-input-19-584ea30b4abe>:20: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  print(\"mean rps:\",rps_arr.mean(),\"std:\",rps_arr.std())\n",
      "<ipython-input-19-584ea30b4abe>:21: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  return [accuracy_arr.mean(),accuracy_arr.std(),ace_arr.mean(),ace_arr.std(),rps_arr.mean(),rps_arr.std(),nll_arr.mean(),nll_arr.std()]\n"
     ]
    }
   ],
   "source": [
    "#swa\n",
    "def compute_acc_ace_rps_swa(es):\n",
    "    copies=int(par_runs/es)\n",
    "    ace_arr=torch.zeros(copies)\n",
    "    rps_arr=torch.zeros(copies)\n",
    "    nll_arr=torch.zeros(copies)\n",
    "    accuracy_arr=torch.zeros(copies)\n",
    "\n",
    "    for it in range(copies):\n",
    "        test_prob=torch.Tensor(test_prob_arr[:,:,19,it*es:(it+1)*es]).mean(-1).reshape(test_size,num_classes)\n",
    "        ace_arr[it]=adaptive_calibration_error(test_prob,test_labels_arr)\n",
    "        rps_arr[it]=(rps_calc(test_prob, test_labels_arr)).mean()\n",
    "        nll_arr[it]=nll_calc(test_prob, test_labels_arr)\n",
    "        _, predictedt = torch.max(test_prob,1)\n",
    "        accuracy_arr[it]= (predictedt==test_labels_arr.reshape(1,test_size)).sum()/test_size\n",
    "    print(\"SWA, ensemble size:\", es)\n",
    "    print(\"mean accuracy:\",accuracy_arr.mean(),\"std:\",accuracy_arr.std())\n",
    "    print(\"mean ace:\",ace_arr.mean(),\"std:\",ace_arr.std())\n",
    "    print(\"mean nll:\",nll_arr.mean(),\"std:\",nll_arr.std())\n",
    "    print(\"mean rps:\",rps_arr.mean(),\"std:\",rps_arr.std())\n",
    "    return [accuracy_arr.mean(),accuracy_arr.std(),ace_arr.mean(),ace_arr.std(),rps_arr.mean(),rps_arr.std(),nll_arr.mean(),nll_arr.std()]\n",
    "\n",
    "acc=torch.zeros(5)\n",
    "acc_std=torch.zeros(5)\n",
    "ace=torch.zeros(5)\n",
    "ace_std=torch.zeros(5)\n",
    "rps=torch.zeros(5)\n",
    "rps_std=torch.zeros(5)\n",
    "nll=torch.zeros(5)\n",
    "nll_std=torch.zeros(5)\n",
    "[acc[0],acc_std[0],ace[0],ace_std[0],rps[0],rps_std[0],nll[0],nll_std[0]]=compute_acc_ace_rps_swa(1)\n",
    "[acc[1],acc_std[1],ace[1],ace_std[1],rps[1],rps_std[1],nll[1],nll_std[1]]=compute_acc_ace_rps_swa(2)\n",
    "[acc[2],acc_std[2],ace[2],ace_std[2],rps[2],rps_std[2],nll[2],nll_std[2]]=compute_acc_ace_rps_swa(4)\n",
    "[acc[3],acc_std[3],ace[3],ace_std[3],rps[3],rps_std[3],nll[3],nll_std[3]]=compute_acc_ace_rps_swa(8)\n",
    "[acc[4],acc_std[4],ace[4],ace_std[4],rps[4],rps_std[4],nll[4],nll_std[4]]=compute_acc_ace_rps_swa(16)\n",
    "\n",
    "# from scipy.io import savemat\n",
    "# filepath=\"results_fashion_swa_rand_test.mat\"\n",
    "# mdic={\"acc\":acc.cpu().numpy(),\"acc_std\":acc_std.cpu().numpy(),\"nll\": nll.cpu().numpy(),\"nll_std\":nll_std.cpu().numpy(),\\\n",
    "#       \"ace\": ace.cpu().numpy(),\"ace_std\":ace_std.cpu().numpy(), \"rps\":rps.cpu().numpy(),\"rps_std\":rps_std.cpu().numpy()}\n",
    "# savemat(filepath,mdic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SWA, ensemble size: 1\n",
    "mean accuracy: tensor(0.9337) std: tensor(0.0016)\n",
    "mean ace: tensor(0.0537) std: tensor(0.0016)\n",
    "mean nll: tensor(0.4826) std: tensor(0.0191)\n",
    "mean rps: tensor(0.0220) std: tensor(0.0006)\n",
    "SWA, ensemble size: 2\n",
    "mean accuracy: tensor(0.9403) std: tensor(0.0012)\n",
    "mean ace: tensor(0.0309) std: tensor(0.0012)\n",
    "mean nll: tensor(0.3312) std: tensor(0.0111)\n",
    "mean rps: tensor(0.0184) std: tensor(0.0004)\n",
    "SWA, ensemble size: 4\n",
    "mean accuracy: tensor(0.9435) std: tensor(0.0011)\n",
    "mean ace: tensor(0.0223) std: tensor(0.0010)\n",
    "mean nll: tensor(0.2579) std: tensor(0.0078)\n",
    "mean rps: tensor(0.0166) std: tensor(0.0003)\n",
    "SWA, ensemble size: 8\n",
    "mean accuracy: tensor(0.9453) std: tensor(0.0008)\n",
    "mean ace: tensor(0.0180) std: tensor(0.0011)\n",
    "mean nll: tensor(0.2186) std: tensor(0.0036)\n",
    "mean rps: tensor(0.0157) std: tensor(0.0002)\n",
    "SWA, ensemble size: 16\n",
    "mean accuracy: tensor(0.9462) std: tensor(0.0008)\n",
    "mean ace: tensor(0.0158) std: tensor(0.0009)\n",
    "mean nll: tensor(0.1964) std: tensor(0.0034)\n",
    "mean rps: tensor(0.0153) std: tensor(5.5335e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bayesian, ensemble size: 1\n",
      "mean accuracy: tensor(0.9373) std: tensor(nan)\n",
      "mean ace: tensor(0.0137) std: tensor(nan)\n",
      "mean nll: tensor(0.1941) std: tensor(nan)\n",
      "mean rps: tensor(0.0179) std: tensor(nan)\n",
      "Bayesian, ensemble size: 2\n",
      "mean accuracy: tensor(nan) std: tensor(nan)\n",
      "mean ace: tensor(nan) std: tensor(nan)\n",
      "mean nll: tensor(nan) std: tensor(nan)\n",
      "mean rps: tensor(nan) std: tensor(nan)\n",
      "Bayesian, ensemble size: 4\n",
      "mean accuracy: tensor(nan) std: tensor(nan)\n",
      "mean ace: tensor(nan) std: tensor(nan)\n",
      "mean nll: tensor(nan) std: tensor(nan)\n",
      "mean rps: tensor(nan) std: tensor(nan)\n",
      "Bayesian, ensemble size: 8\n",
      "mean accuracy: tensor(nan) std: tensor(nan)\n",
      "mean ace: tensor(nan) std: tensor(nan)\n",
      "mean nll: tensor(nan) std: tensor(nan)\n",
      "mean rps: tensor(nan) std: tensor(nan)\n",
      "Bayesian, ensemble size: 16\n",
      "mean accuracy: tensor(nan) std: tensor(nan)\n",
      "mean ace: tensor(nan) std: tensor(nan)\n",
      "mean nll: tensor(nan) std: tensor(nan)\n",
      "mean rps: tensor(nan) std: tensor(nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-858763f44eaa>:17: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  print(\"mean accuracy:\",accuracy_arr.mean(),\"std:\",accuracy_arr.std())\n",
      "<ipython-input-20-858763f44eaa>:18: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  print(\"mean ace:\",ace_arr.mean(),\"std:\",ace_arr.std())\n",
      "<ipython-input-20-858763f44eaa>:19: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  print(\"mean nll:\",nll_arr.mean(),\"std:\",nll_arr.std())\n",
      "<ipython-input-20-858763f44eaa>:20: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  print(\"mean rps:\",rps_arr.mean(),\"std:\",rps_arr.std())\n",
      "<ipython-input-20-858763f44eaa>:21: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\ReduceOps.cpp:1807.)\n",
      "  return [accuracy_arr.mean(),accuracy_arr.std(),ace_arr.mean(),ace_arr.std(),rps_arr.mean(),rps_arr.std(),nll_arr.mean(),nll_arr.std()]\n"
     ]
    }
   ],
   "source": [
    "#Bayesian\n",
    "def compute_acc_ace_rps_bayes(es):\n",
    "    copies=int(par_runs/es)\n",
    "    ace_arr=torch.zeros(copies)\n",
    "    rps_arr=torch.zeros(copies)\n",
    "    nll_arr=torch.zeros(copies)\n",
    "    accuracy_arr=torch.zeros(copies)\n",
    "\n",
    "    for it in range(copies):\n",
    "        test_prob=torch.Tensor(test_prob_arr[:,:,30:60,it*es:(it+1)*es]).mean(-1).mean(-1).reshape(test_size,num_classes)\n",
    "        ace_arr[it]=adaptive_calibration_error(test_prob,test_labels_arr)\n",
    "        rps_arr[it]=(rps_calc(test_prob, test_labels_arr)).mean()\n",
    "        nll_arr[it]=nll_calc(test_prob, test_labels_arr)\n",
    "        _, predictedt = torch.max(test_prob,1)\n",
    "        accuracy_arr[it]= (predictedt==test_labels_arr.reshape(1,test_size)).sum()/test_size\n",
    "    print(\"Bayesian, ensemble size:\", es)\n",
    "    print(\"mean accuracy:\",accuracy_arr.mean(),\"std:\",accuracy_arr.std())\n",
    "    print(\"mean ace:\",ace_arr.mean(),\"std:\",ace_arr.std())\n",
    "    print(\"mean nll:\",nll_arr.mean(),\"std:\",nll_arr.std())\n",
    "    print(\"mean rps:\",rps_arr.mean(),\"std:\",rps_arr.std())\n",
    "    return [accuracy_arr.mean(),accuracy_arr.std(),ace_arr.mean(),ace_arr.std(),rps_arr.mean(),rps_arr.std(),nll_arr.mean(),nll_arr.std()]\n",
    "\n",
    "\n",
    "\n",
    "acc=torch.zeros(5)\n",
    "acc_std=torch.zeros(5)\n",
    "ace=torch.zeros(5)\n",
    "ace_std=torch.zeros(5)\n",
    "rps=torch.zeros(5)\n",
    "rps_std=torch.zeros(5)\n",
    "nll=torch.zeros(5)\n",
    "nll_std=torch.zeros(5)\n",
    "[acc[0],acc_std[0],ace[0],ace_std[0],rps[0],rps_std[0],nll[0],nll_std[0]]=compute_acc_ace_rps_bayes(1)\n",
    "[acc[1],acc_std[1],ace[1],ace_std[1],rps[1],rps_std[1],nll[1],nll_std[1]]=compute_acc_ace_rps_bayes(2)\n",
    "[acc[2],acc_std[2],ace[2],ace_std[2],rps[2],rps_std[2],nll[2],nll_std[2]]=compute_acc_ace_rps_bayes(4)\n",
    "[acc[3],acc_std[3],ace[3],ace_std[3],rps[3],rps_std[3],nll[3],nll_std[3]]=compute_acc_ace_rps_bayes(8)\n",
    "[acc[4],acc_std[4],ace[4],ace_std[4],rps[4],rps_std[4],nll[4],nll_std[4]]=compute_acc_ace_rps_bayes(16)\n",
    "\n",
    "# from scipy.io import savemat\n",
    "# filepath=\"results_fashion_bayes_rand_test.mat\"\n",
    "# mdic={\"acc\":acc.cpu().numpy(),\"acc_std\":acc_std.cpu().numpy(),\"nll\": nll.cpu().numpy(),\"nll_std\":nll_std.cpu().numpy(),\\\n",
    "#       \"ace\": ace.cpu().numpy(),\"ace_std\":ace_std.cpu().numpy(), \"rps\":rps.cpu().numpy(),\"rps_std\":rps_std.cpu().numpy()}\n",
    "# savemat(filepath,mdic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian, ensemble size: 1\n",
    "mean accuracy: tensor(0.9358) std: tensor(nan)\n",
    "mean ace: tensor(0.0123) std: tensor(nan)\n",
    "mean nll: tensor(0.2039) std: tensor(nan)\n",
    "mean rps: tensor(0.0177) std: tensor(nan)\n",
    "\n",
    "Bayesian, ensemble size: 1\n",
    "mean accuracy: tensor(0.9400) std: tensor(nan)\n",
    "mean ace: tensor(0.0072) std: tensor(nan)\n",
    "mean nll: tensor(0.1876) std: tensor(nan)\n",
    "mean rps: tensor(0.0169) std: tensor(nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prob=torch.Tensor(test_prob_arr[:,7,0]).reshape(test_size,1)\n",
    "# ace=adaptive_calibration_error(test_labels_arr.reshape(test_size,1).numpy(),test_prob.numpy(),20)\n",
    "# ace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prob=torch.Tensor(test_prob_arr[:,10:16,48:64]).mean(-1).mean(-1).reshape(test_size,1)\n",
    "# ace=adaptive_calibration_error(test_labels_arr.reshape(test_size,1).numpy(),test_prob.numpy(),20)\n",
    "# ace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Could not load \"C:\\Users\\Daniel Paulin\\.conda\\envs\\torchenv39\\Library\\bin\\gvplugin_pango.dll\" - It was found, so perhaps one of its dependents was not.  Try ldd.\n",
      "Warning: no hard-coded metrics for 'Linux libertine'.  Falling back to 'Times' metrics\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 8.1.0 (0)\n",
       " -->\n",
       "<!-- Title: model Pages: 1 -->\n",
       "<svg width=\"135pt\" height=\"1555pt\"\n",
       " viewBox=\"0.00 0.00 134.52 1555.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(0.611434 0.611434) rotate(0) translate(4 2539.2)\">\n",
       "<title>model</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-2539.2 216,-2539.2 216,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_2</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"8,-730.8 8,-2493.2 204,-2493.2 204,-730.8 8,-730.8\"/>\n",
       "<text text-anchor=\"middle\" x=\"41.33\" y=\"-2478.4\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">Sequential</text>\n",
       "</g>\n",
       "<g id=\"clust3\" class=\"cluster\">\n",
       "<title>cluster_4</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"36,-8 36,-160.4 176,-160.4 176,-8 36,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"69.33\" y=\"-145.6\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">Sequential</text>\n",
       "</g>\n",
       "<g id=\"clust2\" class=\"cluster\">\n",
       "<title>cluster_3</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" stroke-dasharray=\"5,2\" points=\"20,-168.4 20,-650.8 192,-650.8 192,-168.4 20,-168.4\"/>\n",
       "<text text-anchor=\"middle\" x=\"53.33\" y=\"-636\" font-family=\"Times New Roman,serif\" font-size=\"12.00\">Sequential</text>\n",
       "</g>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"none\" points=\"165.99,-2535.2 46.01,-2535.2 46.01,-2501.2 165.99,-2501.2 165.99,-2535.2\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"46.01,-2501.2 46.01,-2535.2 104.33,-2535.2 104.33,-2501.2 46.01,-2501.2\"/>\n",
       "<text text-anchor=\"start\" x=\"51.01\" y=\"-2521.2\" font-family=\"Linux libertine\" font-size=\"10.00\">input&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"60.17\" y=\"-2509.2\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"104.33,-2501.2 104.33,-2535.2 165.99,-2535.2 165.99,-2501.2 104.33,-2501.2\"/>\n",
       "<text text-anchor=\"start\" x=\"109.33\" y=\"-2515.2\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 1, 28, 28)</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"181,-2462.8 31,-2462.8 31,-2418.8 181,-2418.8 181,-2462.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"31,-2418.8 31,-2462.8 72,-2462.8 72,-2418.8 31,-2418.8\"/>\n",
       "<text text-anchor=\"start\" x=\"35.67\" y=\"-2443.8\" font-family=\"Linux libertine\" font-size=\"10.00\">Conv2d</text>\n",
       "<text text-anchor=\"start\" x=\"36.5\" y=\"-2431.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"72,-2440.8 72,-2462.8 112,-2462.8 112,-2440.8 72,-2440.8\"/>\n",
       "<text text-anchor=\"start\" x=\"80.33\" y=\"-2448.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"112,-2440.8 112,-2462.8 181,-2462.8 181,-2440.8 112,-2440.8\"/>\n",
       "<text text-anchor=\"start\" x=\"119.42\" y=\"-2448.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 1, 28, 28) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"72,-2418.8 72,-2440.8 112,-2440.8 112,-2418.8 72,-2418.8\"/>\n",
       "<text text-anchor=\"start\" x=\"76.58\" y=\"-2426.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"112,-2418.8 112,-2440.8 181,-2440.8 181,-2418.8 112,-2418.8\"/>\n",
       "<text text-anchor=\"start\" x=\"116.92\" y=\"-2426.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 32, 28, 28) </text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-2501.21C106,-2493.13 106,-2483.04 106,-2473.47\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-2473.64 106,-2463.64 102.5,-2473.64 109.5,-2473.64\"/>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"182,-2382.8 30,-2382.8 30,-2338.8 182,-2338.8 182,-2382.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"30,-2338.8 30,-2382.8 73,-2382.8 73,-2338.8 30,-2338.8\"/>\n",
       "<text text-anchor=\"start\" x=\"34.83\" y=\"-2363.8\" font-family=\"Linux libertine\" font-size=\"10.00\">Softplus</text>\n",
       "<text text-anchor=\"start\" x=\"36.5\" y=\"-2351.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"73,-2360.8 73,-2382.8 113,-2382.8 113,-2360.8 73,-2360.8\"/>\n",
       "<text text-anchor=\"start\" x=\"81.33\" y=\"-2368.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"113,-2360.8 113,-2382.8 182,-2382.8 182,-2360.8 113,-2360.8\"/>\n",
       "<text text-anchor=\"start\" x=\"117.92\" y=\"-2368.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 32, 28, 28) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"73,-2338.8 73,-2360.8 113,-2360.8 113,-2338.8 73,-2338.8\"/>\n",
       "<text text-anchor=\"start\" x=\"77.58\" y=\"-2346.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"113,-2338.8 113,-2360.8 182,-2360.8 182,-2338.8 113,-2338.8\"/>\n",
       "<text text-anchor=\"start\" x=\"117.92\" y=\"-2346.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 32, 28, 28) </text>\n",
       "</g>\n",
       "<!-- 1&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>1&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-2418.9C106,-2411.12 106,-2402.1 106,-2393.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-2393.76 106,-2383.76 102.5,-2393.76 109.5,-2393.76\"/>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"193.5,-2302.8 18.5,-2302.8 18.5,-2258.8 193.5,-2258.8 193.5,-2302.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"18.5,-2258.8 18.5,-2302.8 84.5,-2302.8 84.5,-2258.8 18.5,-2258.8\"/>\n",
       "<text text-anchor=\"start\" x=\"23.17\" y=\"-2283.8\" font-family=\"Linux libertine\" font-size=\"10.00\">BatchNorm2d</text>\n",
       "<text text-anchor=\"start\" x=\"36.5\" y=\"-2271.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"84.5,-2280.8 84.5,-2302.8 124.5,-2302.8 124.5,-2280.8 84.5,-2280.8\"/>\n",
       "<text text-anchor=\"start\" x=\"92.83\" y=\"-2288.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"124.5,-2280.8 124.5,-2302.8 193.5,-2302.8 193.5,-2280.8 124.5,-2280.8\"/>\n",
       "<text text-anchor=\"start\" x=\"129.42\" y=\"-2288.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 32, 28, 28) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"84.5,-2258.8 84.5,-2280.8 124.5,-2280.8 124.5,-2258.8 84.5,-2258.8\"/>\n",
       "<text text-anchor=\"start\" x=\"89.08\" y=\"-2266.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"124.5,-2258.8 124.5,-2280.8 193.5,-2280.8 193.5,-2258.8 124.5,-2258.8\"/>\n",
       "<text text-anchor=\"start\" x=\"129.42\" y=\"-2266.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 32, 28, 28) </text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-2338.9C106,-2331.12 106,-2322.1 106,-2313.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-2313.76 106,-2303.76 102.5,-2313.76 109.5,-2313.76\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"181,-2222.8 31,-2222.8 31,-2178.8 181,-2178.8 181,-2222.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"31,-2178.8 31,-2222.8 72,-2222.8 72,-2178.8 31,-2178.8\"/>\n",
       "<text text-anchor=\"start\" x=\"35.67\" y=\"-2203.8\" font-family=\"Linux libertine\" font-size=\"10.00\">Conv2d</text>\n",
       "<text text-anchor=\"start\" x=\"36.5\" y=\"-2191.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"72,-2200.8 72,-2222.8 112,-2222.8 112,-2200.8 72,-2200.8\"/>\n",
       "<text text-anchor=\"start\" x=\"80.33\" y=\"-2208.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"112,-2200.8 112,-2222.8 181,-2222.8 181,-2200.8 112,-2200.8\"/>\n",
       "<text text-anchor=\"start\" x=\"116.92\" y=\"-2208.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 32, 28, 28) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"72,-2178.8 72,-2200.8 112,-2200.8 112,-2178.8 72,-2178.8\"/>\n",
       "<text text-anchor=\"start\" x=\"76.58\" y=\"-2186.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"112,-2178.8 112,-2200.8 181,-2200.8 181,-2178.8 112,-2178.8\"/>\n",
       "<text text-anchor=\"start\" x=\"116.92\" y=\"-2186.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 64, 28, 28) </text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-2258.9C106,-2251.12 106,-2242.1 106,-2233.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-2233.76 106,-2223.76 102.5,-2233.76 109.5,-2233.76\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"182,-2142.8 30,-2142.8 30,-2098.8 182,-2098.8 182,-2142.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"30,-2098.8 30,-2142.8 73,-2142.8 73,-2098.8 30,-2098.8\"/>\n",
       "<text text-anchor=\"start\" x=\"34.83\" y=\"-2123.8\" font-family=\"Linux libertine\" font-size=\"10.00\">Softplus</text>\n",
       "<text text-anchor=\"start\" x=\"36.5\" y=\"-2111.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"73,-2120.8 73,-2142.8 113,-2142.8 113,-2120.8 73,-2120.8\"/>\n",
       "<text text-anchor=\"start\" x=\"81.33\" y=\"-2128.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"113,-2120.8 113,-2142.8 182,-2142.8 182,-2120.8 113,-2120.8\"/>\n",
       "<text text-anchor=\"start\" x=\"117.92\" y=\"-2128.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 64, 28, 28) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"73,-2098.8 73,-2120.8 113,-2120.8 113,-2098.8 73,-2098.8\"/>\n",
       "<text text-anchor=\"start\" x=\"77.58\" y=\"-2106.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"113,-2098.8 113,-2120.8 182,-2120.8 182,-2098.8 113,-2098.8\"/>\n",
       "<text text-anchor=\"start\" x=\"117.92\" y=\"-2106.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 64, 28, 28) </text>\n",
       "</g>\n",
       "<!-- 4&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>4&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-2178.9C106,-2171.12 106,-2162.1 106,-2153.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-2153.76 106,-2143.76 102.5,-2153.76 109.5,-2153.76\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"188.5,-2062.8 23.5,-2062.8 23.5,-2018.8 188.5,-2018.8 188.5,-2062.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"23.5,-2018.8 23.5,-2062.8 79.5,-2062.8 79.5,-2018.8 23.5,-2018.8\"/>\n",
       "<text text-anchor=\"start\" x=\"28.17\" y=\"-2043.8\" font-family=\"Linux libertine\" font-size=\"10.00\">MaxPool2d</text>\n",
       "<text text-anchor=\"start\" x=\"36.5\" y=\"-2031.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"79.5,-2040.8 79.5,-2062.8 119.5,-2062.8 119.5,-2040.8 79.5,-2040.8\"/>\n",
       "<text text-anchor=\"start\" x=\"87.83\" y=\"-2048.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"119.5,-2040.8 119.5,-2062.8 188.5,-2062.8 188.5,-2040.8 119.5,-2040.8\"/>\n",
       "<text text-anchor=\"start\" x=\"124.42\" y=\"-2048.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 64, 28, 28) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"79.5,-2018.8 79.5,-2040.8 119.5,-2040.8 119.5,-2018.8 79.5,-2018.8\"/>\n",
       "<text text-anchor=\"start\" x=\"84.08\" y=\"-2026.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"119.5,-2018.8 119.5,-2040.8 188.5,-2040.8 188.5,-2018.8 119.5,-2018.8\"/>\n",
       "<text text-anchor=\"start\" x=\"124.42\" y=\"-2026.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 64, 14, 14) </text>\n",
       "</g>\n",
       "<!-- 5&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>5&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-2098.9C106,-2091.12 106,-2082.1 106,-2073.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-2073.76 106,-2063.76 102.5,-2073.76 109.5,-2073.76\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"193.5,-1982.8 18.5,-1982.8 18.5,-1938.8 193.5,-1938.8 193.5,-1982.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"18.5,-1938.8 18.5,-1982.8 84.5,-1982.8 84.5,-1938.8 18.5,-1938.8\"/>\n",
       "<text text-anchor=\"start\" x=\"23.17\" y=\"-1963.8\" font-family=\"Linux libertine\" font-size=\"10.00\">BatchNorm2d</text>\n",
       "<text text-anchor=\"start\" x=\"36.5\" y=\"-1951.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"84.5,-1960.8 84.5,-1982.8 124.5,-1982.8 124.5,-1960.8 84.5,-1960.8\"/>\n",
       "<text text-anchor=\"start\" x=\"92.83\" y=\"-1968.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"124.5,-1960.8 124.5,-1982.8 193.5,-1982.8 193.5,-1960.8 124.5,-1960.8\"/>\n",
       "<text text-anchor=\"start\" x=\"129.42\" y=\"-1968.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 64, 14, 14) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"84.5,-1938.8 84.5,-1960.8 124.5,-1960.8 124.5,-1938.8 84.5,-1938.8\"/>\n",
       "<text text-anchor=\"start\" x=\"89.08\" y=\"-1946.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"124.5,-1938.8 124.5,-1960.8 193.5,-1960.8 193.5,-1938.8 124.5,-1938.8\"/>\n",
       "<text text-anchor=\"start\" x=\"129.42\" y=\"-1946.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 64, 14, 14) </text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-2018.9C106,-2011.12 106,-2002.1 106,-1993.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-1993.76 106,-1983.76 102.5,-1993.76 109.5,-1993.76\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"181,-1902.8 31,-1902.8 31,-1858.8 181,-1858.8 181,-1902.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"31,-1858.8 31,-1902.8 72,-1902.8 72,-1858.8 31,-1858.8\"/>\n",
       "<text text-anchor=\"start\" x=\"35.67\" y=\"-1883.8\" font-family=\"Linux libertine\" font-size=\"10.00\">Conv2d</text>\n",
       "<text text-anchor=\"start\" x=\"36.5\" y=\"-1871.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"72,-1880.8 72,-1902.8 112,-1902.8 112,-1880.8 72,-1880.8\"/>\n",
       "<text text-anchor=\"start\" x=\"80.33\" y=\"-1888.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"112,-1880.8 112,-1902.8 181,-1902.8 181,-1880.8 112,-1880.8\"/>\n",
       "<text text-anchor=\"start\" x=\"116.92\" y=\"-1888.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 64, 14, 14) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"72,-1858.8 72,-1880.8 112,-1880.8 112,-1858.8 72,-1858.8\"/>\n",
       "<text text-anchor=\"start\" x=\"76.58\" y=\"-1866.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"112,-1858.8 112,-1880.8 181,-1880.8 181,-1858.8 112,-1858.8\"/>\n",
       "<text text-anchor=\"start\" x=\"116.92\" y=\"-1866.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 64, 14, 14) </text>\n",
       "</g>\n",
       "<!-- 7&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>7&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-1938.9C106,-1931.12 106,-1922.1 106,-1913.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-1913.76 106,-1903.76 102.5,-1913.76 109.5,-1913.76\"/>\n",
       "</g>\n",
       "<!-- 9 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>9</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"182,-1822.8 30,-1822.8 30,-1778.8 182,-1778.8 182,-1822.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"30,-1778.8 30,-1822.8 73,-1822.8 73,-1778.8 30,-1778.8\"/>\n",
       "<text text-anchor=\"start\" x=\"34.83\" y=\"-1803.8\" font-family=\"Linux libertine\" font-size=\"10.00\">Softplus</text>\n",
       "<text text-anchor=\"start\" x=\"36.5\" y=\"-1791.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"73,-1800.8 73,-1822.8 113,-1822.8 113,-1800.8 73,-1800.8\"/>\n",
       "<text text-anchor=\"start\" x=\"81.33\" y=\"-1808.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"113,-1800.8 113,-1822.8 182,-1822.8 182,-1800.8 113,-1800.8\"/>\n",
       "<text text-anchor=\"start\" x=\"117.92\" y=\"-1808.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 64, 14, 14) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"73,-1778.8 73,-1800.8 113,-1800.8 113,-1778.8 73,-1778.8\"/>\n",
       "<text text-anchor=\"start\" x=\"77.58\" y=\"-1786.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"113,-1778.8 113,-1800.8 182,-1800.8 182,-1778.8 113,-1778.8\"/>\n",
       "<text text-anchor=\"start\" x=\"117.92\" y=\"-1786.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 64, 14, 14) </text>\n",
       "</g>\n",
       "<!-- 8&#45;&gt;9 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>8&#45;&gt;9</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-1858.9C106,-1851.12 106,-1842.1 106,-1833.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-1833.76 106,-1823.76 102.5,-1833.76 109.5,-1833.76\"/>\n",
       "</g>\n",
       "<!-- 10 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>10</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"193.5,-1742.8 18.5,-1742.8 18.5,-1698.8 193.5,-1698.8 193.5,-1742.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"18.5,-1698.8 18.5,-1742.8 84.5,-1742.8 84.5,-1698.8 18.5,-1698.8\"/>\n",
       "<text text-anchor=\"start\" x=\"23.17\" y=\"-1723.8\" font-family=\"Linux libertine\" font-size=\"10.00\">BatchNorm2d</text>\n",
       "<text text-anchor=\"start\" x=\"36.5\" y=\"-1711.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"84.5,-1720.8 84.5,-1742.8 124.5,-1742.8 124.5,-1720.8 84.5,-1720.8\"/>\n",
       "<text text-anchor=\"start\" x=\"92.83\" y=\"-1728.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"124.5,-1720.8 124.5,-1742.8 193.5,-1742.8 193.5,-1720.8 124.5,-1720.8\"/>\n",
       "<text text-anchor=\"start\" x=\"129.42\" y=\"-1728.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 64, 14, 14) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"84.5,-1698.8 84.5,-1720.8 124.5,-1720.8 124.5,-1698.8 84.5,-1698.8\"/>\n",
       "<text text-anchor=\"start\" x=\"89.08\" y=\"-1706.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"124.5,-1698.8 124.5,-1720.8 193.5,-1720.8 193.5,-1698.8 124.5,-1698.8\"/>\n",
       "<text text-anchor=\"start\" x=\"129.42\" y=\"-1706.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 64, 14, 14) </text>\n",
       "</g>\n",
       "<!-- 9&#45;&gt;10 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>9&#45;&gt;10</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-1778.9C106,-1771.12 106,-1762.1 106,-1753.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-1753.76 106,-1743.76 102.5,-1753.76 109.5,-1753.76\"/>\n",
       "</g>\n",
       "<!-- 11 -->\n",
       "<g id=\"node12\" class=\"node\">\n",
       "<title>11</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"183.5,-1662.8 28.5,-1662.8 28.5,-1618.8 183.5,-1618.8 183.5,-1662.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"28.5,-1618.8 28.5,-1662.8 69.5,-1662.8 69.5,-1618.8 28.5,-1618.8\"/>\n",
       "<text text-anchor=\"start\" x=\"33.17\" y=\"-1643.8\" font-family=\"Linux libertine\" font-size=\"10.00\">Conv2d</text>\n",
       "<text text-anchor=\"start\" x=\"34\" y=\"-1631.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"69.5,-1640.8 69.5,-1662.8 109.5,-1662.8 109.5,-1640.8 69.5,-1640.8\"/>\n",
       "<text text-anchor=\"start\" x=\"77.83\" y=\"-1648.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"109.5,-1640.8 109.5,-1662.8 183.5,-1662.8 183.5,-1640.8 109.5,-1640.8\"/>\n",
       "<text text-anchor=\"start\" x=\"116.92\" y=\"-1648.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 64, 14, 14) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"69.5,-1618.8 69.5,-1640.8 109.5,-1640.8 109.5,-1618.8 69.5,-1618.8\"/>\n",
       "<text text-anchor=\"start\" x=\"74.08\" y=\"-1626.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"109.5,-1618.8 109.5,-1640.8 183.5,-1640.8 183.5,-1618.8 109.5,-1618.8\"/>\n",
       "<text text-anchor=\"start\" x=\"114.42\" y=\"-1626.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 128, 14, 14) </text>\n",
       "</g>\n",
       "<!-- 10&#45;&gt;11 -->\n",
       "<g id=\"edge11\" class=\"edge\">\n",
       "<title>10&#45;&gt;11</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-1698.9C106,-1691.12 106,-1682.1 106,-1673.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-1673.76 106,-1663.76 102.5,-1673.76 109.5,-1673.76\"/>\n",
       "</g>\n",
       "<!-- 12 -->\n",
       "<g id=\"node13\" class=\"node\">\n",
       "<title>12</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"184.5,-1582.8 27.5,-1582.8 27.5,-1538.8 184.5,-1538.8 184.5,-1582.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"27.5,-1538.8 27.5,-1582.8 70.5,-1582.8 70.5,-1538.8 27.5,-1538.8\"/>\n",
       "<text text-anchor=\"start\" x=\"32.33\" y=\"-1563.8\" font-family=\"Linux libertine\" font-size=\"10.00\">Softplus</text>\n",
       "<text text-anchor=\"start\" x=\"34\" y=\"-1551.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"70.5,-1560.8 70.5,-1582.8 110.5,-1582.8 110.5,-1560.8 70.5,-1560.8\"/>\n",
       "<text text-anchor=\"start\" x=\"78.83\" y=\"-1568.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"110.5,-1560.8 110.5,-1582.8 184.5,-1582.8 184.5,-1560.8 110.5,-1560.8\"/>\n",
       "<text text-anchor=\"start\" x=\"115.42\" y=\"-1568.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 128, 14, 14) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"70.5,-1538.8 70.5,-1560.8 110.5,-1560.8 110.5,-1538.8 70.5,-1538.8\"/>\n",
       "<text text-anchor=\"start\" x=\"75.08\" y=\"-1546.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"110.5,-1538.8 110.5,-1560.8 184.5,-1560.8 184.5,-1538.8 110.5,-1538.8\"/>\n",
       "<text text-anchor=\"start\" x=\"115.42\" y=\"-1546.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 128, 14, 14) </text>\n",
       "</g>\n",
       "<!-- 11&#45;&gt;12 -->\n",
       "<g id=\"edge12\" class=\"edge\">\n",
       "<title>11&#45;&gt;12</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-1618.9C106,-1611.12 106,-1602.1 106,-1593.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-1593.76 106,-1583.76 102.5,-1593.76 109.5,-1593.76\"/>\n",
       "</g>\n",
       "<!-- 13 -->\n",
       "<g id=\"node14\" class=\"node\">\n",
       "<title>13</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"196,-1502.8 16,-1502.8 16,-1458.8 196,-1458.8 196,-1502.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"16,-1458.8 16,-1502.8 82,-1502.8 82,-1458.8 16,-1458.8\"/>\n",
       "<text text-anchor=\"start\" x=\"20.67\" y=\"-1483.8\" font-family=\"Linux libertine\" font-size=\"10.00\">BatchNorm2d</text>\n",
       "<text text-anchor=\"start\" x=\"34\" y=\"-1471.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"82,-1480.8 82,-1502.8 122,-1502.8 122,-1480.8 82,-1480.8\"/>\n",
       "<text text-anchor=\"start\" x=\"90.33\" y=\"-1488.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"122,-1480.8 122,-1502.8 196,-1502.8 196,-1480.8 122,-1480.8\"/>\n",
       "<text text-anchor=\"start\" x=\"126.92\" y=\"-1488.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 128, 14, 14) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"82,-1458.8 82,-1480.8 122,-1480.8 122,-1458.8 82,-1458.8\"/>\n",
       "<text text-anchor=\"start\" x=\"86.58\" y=\"-1466.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"122,-1458.8 122,-1480.8 196,-1480.8 196,-1458.8 122,-1458.8\"/>\n",
       "<text text-anchor=\"start\" x=\"126.92\" y=\"-1466.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 128, 14, 14) </text>\n",
       "</g>\n",
       "<!-- 12&#45;&gt;13 -->\n",
       "<g id=\"edge13\" class=\"edge\">\n",
       "<title>12&#45;&gt;13</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-1538.9C106,-1531.12 106,-1522.1 106,-1513.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-1513.76 106,-1503.76 102.5,-1513.76 109.5,-1513.76\"/>\n",
       "</g>\n",
       "<!-- 14 -->\n",
       "<g id=\"node15\" class=\"node\">\n",
       "<title>14</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"183.5,-1422.8 28.5,-1422.8 28.5,-1378.8 183.5,-1378.8 183.5,-1422.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"28.5,-1378.8 28.5,-1422.8 69.5,-1422.8 69.5,-1378.8 28.5,-1378.8\"/>\n",
       "<text text-anchor=\"start\" x=\"33.17\" y=\"-1403.8\" font-family=\"Linux libertine\" font-size=\"10.00\">Conv2d</text>\n",
       "<text text-anchor=\"start\" x=\"34\" y=\"-1391.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"69.5,-1400.8 69.5,-1422.8 109.5,-1422.8 109.5,-1400.8 69.5,-1400.8\"/>\n",
       "<text text-anchor=\"start\" x=\"77.83\" y=\"-1408.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"109.5,-1400.8 109.5,-1422.8 183.5,-1422.8 183.5,-1400.8 109.5,-1400.8\"/>\n",
       "<text text-anchor=\"start\" x=\"114.42\" y=\"-1408.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 128, 14, 14) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"69.5,-1378.8 69.5,-1400.8 109.5,-1400.8 109.5,-1378.8 69.5,-1378.8\"/>\n",
       "<text text-anchor=\"start\" x=\"74.08\" y=\"-1386.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"109.5,-1378.8 109.5,-1400.8 183.5,-1400.8 183.5,-1378.8 109.5,-1378.8\"/>\n",
       "<text text-anchor=\"start\" x=\"114.42\" y=\"-1386.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 128, 14, 14) </text>\n",
       "</g>\n",
       "<!-- 13&#45;&gt;14 -->\n",
       "<g id=\"edge14\" class=\"edge\">\n",
       "<title>13&#45;&gt;14</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-1458.9C106,-1451.12 106,-1442.1 106,-1433.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-1433.76 106,-1423.76 102.5,-1433.76 109.5,-1433.76\"/>\n",
       "</g>\n",
       "<!-- 15 -->\n",
       "<g id=\"node16\" class=\"node\">\n",
       "<title>15</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"191,-1342.8 21,-1342.8 21,-1298.8 191,-1298.8 191,-1342.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"21,-1298.8 21,-1342.8 77,-1342.8 77,-1298.8 21,-1298.8\"/>\n",
       "<text text-anchor=\"start\" x=\"25.67\" y=\"-1323.8\" font-family=\"Linux libertine\" font-size=\"10.00\">MaxPool2d</text>\n",
       "<text text-anchor=\"start\" x=\"34\" y=\"-1311.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"77,-1320.8 77,-1342.8 117,-1342.8 117,-1320.8 77,-1320.8\"/>\n",
       "<text text-anchor=\"start\" x=\"85.33\" y=\"-1328.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"117,-1320.8 117,-1342.8 191,-1342.8 191,-1320.8 117,-1320.8\"/>\n",
       "<text text-anchor=\"start\" x=\"121.92\" y=\"-1328.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 128, 14, 14) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"77,-1298.8 77,-1320.8 117,-1320.8 117,-1298.8 77,-1298.8\"/>\n",
       "<text text-anchor=\"start\" x=\"81.58\" y=\"-1306.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"117,-1298.8 117,-1320.8 191,-1320.8 191,-1298.8 117,-1298.8\"/>\n",
       "<text text-anchor=\"start\" x=\"126.92\" y=\"-1306.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 128, 7, 7) </text>\n",
       "</g>\n",
       "<!-- 14&#45;&gt;15 -->\n",
       "<g id=\"edge15\" class=\"edge\">\n",
       "<title>14&#45;&gt;15</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-1378.9C106,-1371.12 106,-1362.1 106,-1353.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-1353.76 106,-1343.76 102.5,-1353.76 109.5,-1353.76\"/>\n",
       "</g>\n",
       "<!-- 16 -->\n",
       "<g id=\"node17\" class=\"node\">\n",
       "<title>16</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"191,-1262.8 21,-1262.8 21,-1218.8 191,-1218.8 191,-1262.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"21,-1218.8 21,-1262.8 87,-1262.8 87,-1218.8 21,-1218.8\"/>\n",
       "<text text-anchor=\"start\" x=\"25.67\" y=\"-1243.8\" font-family=\"Linux libertine\" font-size=\"10.00\">BatchNorm2d</text>\n",
       "<text text-anchor=\"start\" x=\"39\" y=\"-1231.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"87,-1240.8 87,-1262.8 127,-1262.8 127,-1240.8 87,-1240.8\"/>\n",
       "<text text-anchor=\"start\" x=\"95.33\" y=\"-1248.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"127,-1240.8 127,-1262.8 191,-1262.8 191,-1240.8 127,-1240.8\"/>\n",
       "<text text-anchor=\"start\" x=\"131.92\" y=\"-1248.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 128, 7, 7) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"87,-1218.8 87,-1240.8 127,-1240.8 127,-1218.8 87,-1218.8\"/>\n",
       "<text text-anchor=\"start\" x=\"91.58\" y=\"-1226.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"127,-1218.8 127,-1240.8 191,-1240.8 191,-1218.8 127,-1218.8\"/>\n",
       "<text text-anchor=\"start\" x=\"131.92\" y=\"-1226.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 128, 7, 7) </text>\n",
       "</g>\n",
       "<!-- 15&#45;&gt;16 -->\n",
       "<g id=\"edge16\" class=\"edge\">\n",
       "<title>15&#45;&gt;16</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-1298.9C106,-1291.12 106,-1282.1 106,-1273.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-1273.76 106,-1263.76 102.5,-1273.76 109.5,-1273.76\"/>\n",
       "</g>\n",
       "<!-- 17 -->\n",
       "<g id=\"node18\" class=\"node\">\n",
       "<title>17</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"178.5,-1182.8 33.5,-1182.8 33.5,-1138.8 178.5,-1138.8 178.5,-1182.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"33.5,-1138.8 33.5,-1182.8 74.5,-1182.8 74.5,-1138.8 33.5,-1138.8\"/>\n",
       "<text text-anchor=\"start\" x=\"38.17\" y=\"-1163.8\" font-family=\"Linux libertine\" font-size=\"10.00\">Conv2d</text>\n",
       "<text text-anchor=\"start\" x=\"39\" y=\"-1151.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"74.5,-1160.8 74.5,-1182.8 114.5,-1182.8 114.5,-1160.8 74.5,-1160.8\"/>\n",
       "<text text-anchor=\"start\" x=\"82.83\" y=\"-1168.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"114.5,-1160.8 114.5,-1182.8 178.5,-1182.8 178.5,-1160.8 114.5,-1160.8\"/>\n",
       "<text text-anchor=\"start\" x=\"119.42\" y=\"-1168.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 128, 7, 7) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"74.5,-1138.8 74.5,-1160.8 114.5,-1160.8 114.5,-1138.8 74.5,-1138.8\"/>\n",
       "<text text-anchor=\"start\" x=\"79.08\" y=\"-1146.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"114.5,-1138.8 114.5,-1160.8 178.5,-1160.8 178.5,-1138.8 114.5,-1138.8\"/>\n",
       "<text text-anchor=\"start\" x=\"119.42\" y=\"-1146.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 256, 7, 7) </text>\n",
       "</g>\n",
       "<!-- 16&#45;&gt;17 -->\n",
       "<g id=\"edge17\" class=\"edge\">\n",
       "<title>16&#45;&gt;17</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-1218.9C106,-1211.12 106,-1202.1 106,-1193.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-1193.76 106,-1183.76 102.5,-1193.76 109.5,-1193.76\"/>\n",
       "</g>\n",
       "<!-- 18 -->\n",
       "<g id=\"node19\" class=\"node\">\n",
       "<title>18</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"179.5,-1102.8 32.5,-1102.8 32.5,-1058.8 179.5,-1058.8 179.5,-1102.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"32.5,-1058.8 32.5,-1102.8 75.5,-1102.8 75.5,-1058.8 32.5,-1058.8\"/>\n",
       "<text text-anchor=\"start\" x=\"37.33\" y=\"-1083.8\" font-family=\"Linux libertine\" font-size=\"10.00\">Softplus</text>\n",
       "<text text-anchor=\"start\" x=\"39\" y=\"-1071.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"75.5,-1080.8 75.5,-1102.8 115.5,-1102.8 115.5,-1080.8 75.5,-1080.8\"/>\n",
       "<text text-anchor=\"start\" x=\"83.83\" y=\"-1088.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"115.5,-1080.8 115.5,-1102.8 179.5,-1102.8 179.5,-1080.8 115.5,-1080.8\"/>\n",
       "<text text-anchor=\"start\" x=\"120.42\" y=\"-1088.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 256, 7, 7) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"75.5,-1058.8 75.5,-1080.8 115.5,-1080.8 115.5,-1058.8 75.5,-1058.8\"/>\n",
       "<text text-anchor=\"start\" x=\"80.08\" y=\"-1066.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"115.5,-1058.8 115.5,-1080.8 179.5,-1080.8 179.5,-1058.8 115.5,-1058.8\"/>\n",
       "<text text-anchor=\"start\" x=\"120.42\" y=\"-1066.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 256, 7, 7) </text>\n",
       "</g>\n",
       "<!-- 17&#45;&gt;18 -->\n",
       "<g id=\"edge18\" class=\"edge\">\n",
       "<title>17&#45;&gt;18</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-1138.9C106,-1131.12 106,-1122.1 106,-1113.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-1113.76 106,-1103.76 102.5,-1113.76 109.5,-1113.76\"/>\n",
       "</g>\n",
       "<!-- 19 -->\n",
       "<g id=\"node20\" class=\"node\">\n",
       "<title>19</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"191,-1022.8 21,-1022.8 21,-978.8 191,-978.8 191,-1022.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"21,-978.8 21,-1022.8 87,-1022.8 87,-978.8 21,-978.8\"/>\n",
       "<text text-anchor=\"start\" x=\"25.67\" y=\"-1003.8\" font-family=\"Linux libertine\" font-size=\"10.00\">BatchNorm2d</text>\n",
       "<text text-anchor=\"start\" x=\"39\" y=\"-991.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"87,-1000.8 87,-1022.8 127,-1022.8 127,-1000.8 87,-1000.8\"/>\n",
       "<text text-anchor=\"start\" x=\"95.33\" y=\"-1008.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"127,-1000.8 127,-1022.8 191,-1022.8 191,-1000.8 127,-1000.8\"/>\n",
       "<text text-anchor=\"start\" x=\"131.92\" y=\"-1008.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 256, 7, 7) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"87,-978.8 87,-1000.8 127,-1000.8 127,-978.8 87,-978.8\"/>\n",
       "<text text-anchor=\"start\" x=\"91.58\" y=\"-986.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"127,-978.8 127,-1000.8 191,-1000.8 191,-978.8 127,-978.8\"/>\n",
       "<text text-anchor=\"start\" x=\"131.92\" y=\"-986.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 256, 7, 7) </text>\n",
       "</g>\n",
       "<!-- 18&#45;&gt;19 -->\n",
       "<g id=\"edge19\" class=\"edge\">\n",
       "<title>18&#45;&gt;19</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-1058.9C106,-1051.12 106,-1042.1 106,-1033.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-1033.76 106,-1023.76 102.5,-1033.76 109.5,-1033.76\"/>\n",
       "</g>\n",
       "<!-- 20 -->\n",
       "<g id=\"node21\" class=\"node\">\n",
       "<title>20</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"178.5,-942.8 33.5,-942.8 33.5,-898.8 178.5,-898.8 178.5,-942.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"33.5,-898.8 33.5,-942.8 74.5,-942.8 74.5,-898.8 33.5,-898.8\"/>\n",
       "<text text-anchor=\"start\" x=\"38.17\" y=\"-923.8\" font-family=\"Linux libertine\" font-size=\"10.00\">Conv2d</text>\n",
       "<text text-anchor=\"start\" x=\"39\" y=\"-911.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"74.5,-920.8 74.5,-942.8 114.5,-942.8 114.5,-920.8 74.5,-920.8\"/>\n",
       "<text text-anchor=\"start\" x=\"82.83\" y=\"-928.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"114.5,-920.8 114.5,-942.8 178.5,-942.8 178.5,-920.8 114.5,-920.8\"/>\n",
       "<text text-anchor=\"start\" x=\"119.42\" y=\"-928.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 256, 7, 7) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"74.5,-898.8 74.5,-920.8 114.5,-920.8 114.5,-898.8 74.5,-898.8\"/>\n",
       "<text text-anchor=\"start\" x=\"79.08\" y=\"-906.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"114.5,-898.8 114.5,-920.8 178.5,-920.8 178.5,-898.8 114.5,-898.8\"/>\n",
       "<text text-anchor=\"start\" x=\"119.42\" y=\"-906.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 256, 7, 7) </text>\n",
       "</g>\n",
       "<!-- 19&#45;&gt;20 -->\n",
       "<g id=\"edge20\" class=\"edge\">\n",
       "<title>19&#45;&gt;20</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-978.9C106,-971.12 106,-962.1 106,-953.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-953.76 106,-943.76 102.5,-953.76 109.5,-953.76\"/>\n",
       "</g>\n",
       "<!-- 21 -->\n",
       "<g id=\"node22\" class=\"node\">\n",
       "<title>21</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"179.5,-862.8 32.5,-862.8 32.5,-818.8 179.5,-818.8 179.5,-862.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"32.5,-818.8 32.5,-862.8 75.5,-862.8 75.5,-818.8 32.5,-818.8\"/>\n",
       "<text text-anchor=\"start\" x=\"37.33\" y=\"-843.8\" font-family=\"Linux libertine\" font-size=\"10.00\">Softplus</text>\n",
       "<text text-anchor=\"start\" x=\"39\" y=\"-831.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"75.5,-840.8 75.5,-862.8 115.5,-862.8 115.5,-840.8 75.5,-840.8\"/>\n",
       "<text text-anchor=\"start\" x=\"83.83\" y=\"-848.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"115.5,-840.8 115.5,-862.8 179.5,-862.8 179.5,-840.8 115.5,-840.8\"/>\n",
       "<text text-anchor=\"start\" x=\"120.42\" y=\"-848.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 256, 7, 7) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"75.5,-818.8 75.5,-840.8 115.5,-840.8 115.5,-818.8 75.5,-818.8\"/>\n",
       "<text text-anchor=\"start\" x=\"80.08\" y=\"-826.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"115.5,-818.8 115.5,-840.8 179.5,-840.8 179.5,-818.8 115.5,-818.8\"/>\n",
       "<text text-anchor=\"start\" x=\"120.42\" y=\"-826.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 256, 7, 7) </text>\n",
       "</g>\n",
       "<!-- 20&#45;&gt;21 -->\n",
       "<g id=\"edge21\" class=\"edge\">\n",
       "<title>20&#45;&gt;21</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-898.9C106,-891.12 106,-882.1 106,-873.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-873.76 106,-863.76 102.5,-873.76 109.5,-873.76\"/>\n",
       "</g>\n",
       "<!-- 22 -->\n",
       "<g id=\"node23\" class=\"node\">\n",
       "<title>22</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"186,-782.8 26,-782.8 26,-738.8 186,-738.8 186,-782.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"26,-738.8 26,-782.8 82,-782.8 82,-738.8 26,-738.8\"/>\n",
       "<text text-anchor=\"start\" x=\"30.67\" y=\"-763.8\" font-family=\"Linux libertine\" font-size=\"10.00\">MaxPool2d</text>\n",
       "<text text-anchor=\"start\" x=\"39\" y=\"-751.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"82,-760.8 82,-782.8 122,-782.8 122,-760.8 82,-760.8\"/>\n",
       "<text text-anchor=\"start\" x=\"90.33\" y=\"-768.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"122,-760.8 122,-782.8 186,-782.8 186,-760.8 122,-760.8\"/>\n",
       "<text text-anchor=\"start\" x=\"126.92\" y=\"-768.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 256, 7, 7) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"82,-738.8 82,-760.8 122,-760.8 122,-738.8 82,-738.8\"/>\n",
       "<text text-anchor=\"start\" x=\"86.58\" y=\"-746.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"122,-738.8 122,-760.8 186,-760.8 186,-738.8 122,-738.8\"/>\n",
       "<text text-anchor=\"start\" x=\"126.92\" y=\"-746.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 256, 3, 3) </text>\n",
       "</g>\n",
       "<!-- 21&#45;&gt;22 -->\n",
       "<g id=\"edge22\" class=\"edge\">\n",
       "<title>21&#45;&gt;22</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-818.9C106,-811.12 106,-802.1 106,-793.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-793.76 106,-783.76 102.5,-793.76 109.5,-793.76\"/>\n",
       "</g>\n",
       "<!-- 23 -->\n",
       "<g id=\"node24\" class=\"node\">\n",
       "<title>23</title>\n",
       "<polygon fill=\"aliceblue\" stroke=\"none\" points=\"177.5,-702.8 34.5,-702.8 34.5,-658.8 177.5,-658.8 177.5,-702.8\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"34.5,-658.8 34.5,-702.8 73.5,-702.8 73.5,-658.8 34.5,-658.8\"/>\n",
       "<text text-anchor=\"start\" x=\"44.28\" y=\"-683.8\" font-family=\"Linux libertine\" font-size=\"10.00\">view</text>\n",
       "<text text-anchor=\"start\" x=\"39\" y=\"-671.8\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:1</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"73.5,-680.8 73.5,-702.8 113.5,-702.8 113.5,-680.8 73.5,-680.8\"/>\n",
       "<text text-anchor=\"start\" x=\"81.83\" y=\"-688.8\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"113.5,-680.8 113.5,-702.8 177.5,-702.8 177.5,-680.8 113.5,-680.8\"/>\n",
       "<text text-anchor=\"start\" x=\"118.42\" y=\"-688.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 256, 3, 3) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"73.5,-658.8 73.5,-680.8 113.5,-680.8 113.5,-658.8 73.5,-658.8\"/>\n",
       "<text text-anchor=\"start\" x=\"78.08\" y=\"-666.8\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"113.5,-658.8 113.5,-680.8 177.5,-680.8 177.5,-658.8 113.5,-658.8\"/>\n",
       "<text text-anchor=\"start\" x=\"125.92\" y=\"-666.8\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 2304) </text>\n",
       "</g>\n",
       "<!-- 22&#45;&gt;23 -->\n",
       "<g id=\"edge23\" class=\"edge\">\n",
       "<title>22&#45;&gt;23</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-738.9C106,-731.12 106,-722.1 106,-713.56\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-713.76 106,-703.76 102.5,-713.76 109.5,-713.76\"/>\n",
       "</g>\n",
       "<!-- 24 -->\n",
       "<g id=\"node25\" class=\"node\">\n",
       "<title>24</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"183.5,-620.4 28.5,-620.4 28.5,-576.4 183.5,-576.4 183.5,-620.4\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"28.5,-576.4 28.5,-620.4 94.5,-620.4 94.5,-576.4 28.5,-576.4\"/>\n",
       "<text text-anchor=\"start\" x=\"33.17\" y=\"-601.4\" font-family=\"Linux libertine\" font-size=\"10.00\">BatchNorm1d</text>\n",
       "<text text-anchor=\"start\" x=\"46.5\" y=\"-589.4\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"94.5,-598.4 94.5,-620.4 134.5,-620.4 134.5,-598.4 94.5,-598.4\"/>\n",
       "<text text-anchor=\"start\" x=\"102.83\" y=\"-606.4\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"134.5,-598.4 134.5,-620.4 183.5,-620.4 183.5,-598.4 134.5,-598.4\"/>\n",
       "<text text-anchor=\"start\" x=\"139.42\" y=\"-606.4\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 2304) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"94.5,-576.4 94.5,-598.4 134.5,-598.4 134.5,-576.4 94.5,-576.4\"/>\n",
       "<text text-anchor=\"start\" x=\"99.08\" y=\"-584.4\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"134.5,-576.4 134.5,-598.4 183.5,-598.4 183.5,-576.4 134.5,-576.4\"/>\n",
       "<text text-anchor=\"start\" x=\"139.42\" y=\"-584.4\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 2304) </text>\n",
       "</g>\n",
       "<!-- 23&#45;&gt;24 -->\n",
       "<g id=\"edge24\" class=\"edge\">\n",
       "<title>23&#45;&gt;24</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-659.1C106,-650.63 106,-640.64 106,-631.28\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-631.31 106,-621.31 102.5,-631.31 109.5,-631.31\"/>\n",
       "</g>\n",
       "<!-- 25 -->\n",
       "<g id=\"node26\" class=\"node\">\n",
       "<title>25</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"170,-540.4 42,-540.4 42,-496.4 170,-496.4 170,-540.4\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"42,-496.4 42,-540.4 81,-540.4 81,-496.4 42,-496.4\"/>\n",
       "<text text-anchor=\"start\" x=\"48.45\" y=\"-521.4\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"46.5\" y=\"-509.4\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"81,-518.4 81,-540.4 121,-540.4 121,-518.4 81,-518.4\"/>\n",
       "<text text-anchor=\"start\" x=\"89.33\" y=\"-526.4\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"121,-518.4 121,-540.4 170,-540.4 170,-518.4 121,-518.4\"/>\n",
       "<text text-anchor=\"start\" x=\"125.92\" y=\"-526.4\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 2304) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"81,-496.4 81,-518.4 121,-518.4 121,-496.4 81,-496.4\"/>\n",
       "<text text-anchor=\"start\" x=\"85.58\" y=\"-504.4\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"121,-496.4 121,-518.4 170,-518.4 170,-496.4 121,-496.4\"/>\n",
       "<text text-anchor=\"start\" x=\"130.92\" y=\"-504.4\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 32) </text>\n",
       "</g>\n",
       "<!-- 24&#45;&gt;25 -->\n",
       "<g id=\"edge25\" class=\"edge\">\n",
       "<title>24&#45;&gt;25</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-576.5C106,-568.72 106,-559.7 106,-551.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-551.36 106,-541.36 102.5,-551.36 109.5,-551.36\"/>\n",
       "</g>\n",
       "<!-- 26 -->\n",
       "<g id=\"node27\" class=\"node\">\n",
       "<title>26</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"178.5,-460.4 33.5,-460.4 33.5,-416.4 178.5,-416.4 178.5,-460.4\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"33.5,-416.4 33.5,-460.4 99.5,-460.4 99.5,-416.4 33.5,-416.4\"/>\n",
       "<text text-anchor=\"start\" x=\"38.17\" y=\"-441.4\" font-family=\"Linux libertine\" font-size=\"10.00\">BatchNorm1d</text>\n",
       "<text text-anchor=\"start\" x=\"51.5\" y=\"-429.4\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"99.5,-438.4 99.5,-460.4 139.5,-460.4 139.5,-438.4 99.5,-438.4\"/>\n",
       "<text text-anchor=\"start\" x=\"107.83\" y=\"-446.4\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"139.5,-438.4 139.5,-460.4 178.5,-460.4 178.5,-438.4 139.5,-438.4\"/>\n",
       "<text text-anchor=\"start\" x=\"144.42\" y=\"-446.4\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 32) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"99.5,-416.4 99.5,-438.4 139.5,-438.4 139.5,-416.4 99.5,-416.4\"/>\n",
       "<text text-anchor=\"start\" x=\"104.08\" y=\"-424.4\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"139.5,-416.4 139.5,-438.4 178.5,-438.4 178.5,-416.4 139.5,-416.4\"/>\n",
       "<text text-anchor=\"start\" x=\"144.42\" y=\"-424.4\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 32) </text>\n",
       "</g>\n",
       "<!-- 25&#45;&gt;26 -->\n",
       "<g id=\"edge26\" class=\"edge\">\n",
       "<title>25&#45;&gt;26</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-496.5C106,-488.72 106,-479.7 106,-471.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-471.36 106,-461.36 102.5,-471.36 109.5,-471.36\"/>\n",
       "</g>\n",
       "<!-- 27 -->\n",
       "<g id=\"node28\" class=\"node\">\n",
       "<title>27</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"167.5,-380.4 44.5,-380.4 44.5,-336.4 167.5,-336.4 167.5,-380.4\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"44.5,-336.4 44.5,-380.4 83.5,-380.4 83.5,-336.4 44.5,-336.4\"/>\n",
       "<text text-anchor=\"start\" x=\"50.95\" y=\"-361.4\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"49\" y=\"-349.4\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"83.5,-358.4 83.5,-380.4 123.5,-380.4 123.5,-358.4 83.5,-358.4\"/>\n",
       "<text text-anchor=\"start\" x=\"91.83\" y=\"-366.4\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"123.5,-358.4 123.5,-380.4 167.5,-380.4 167.5,-358.4 123.5,-358.4\"/>\n",
       "<text text-anchor=\"start\" x=\"130.92\" y=\"-366.4\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 32) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"83.5,-336.4 83.5,-358.4 123.5,-358.4 123.5,-336.4 83.5,-336.4\"/>\n",
       "<text text-anchor=\"start\" x=\"88.08\" y=\"-344.4\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"123.5,-336.4 123.5,-358.4 167.5,-358.4 167.5,-336.4 123.5,-336.4\"/>\n",
       "<text text-anchor=\"start\" x=\"128.42\" y=\"-344.4\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 512) </text>\n",
       "</g>\n",
       "<!-- 26&#45;&gt;27 -->\n",
       "<g id=\"edge27\" class=\"edge\">\n",
       "<title>26&#45;&gt;27</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-416.5C106,-408.72 106,-399.7 106,-391.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-391.36 106,-381.36 102.5,-391.36 109.5,-391.36\"/>\n",
       "</g>\n",
       "<!-- 28 -->\n",
       "<g id=\"node29\" class=\"node\">\n",
       "<title>28</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"169.5,-300.4 42.5,-300.4 42.5,-256.4 169.5,-256.4 169.5,-300.4\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"42.5,-256.4 42.5,-300.4 85.5,-300.4 85.5,-256.4 42.5,-256.4\"/>\n",
       "<text text-anchor=\"start\" x=\"47.33\" y=\"-281.4\" font-family=\"Linux libertine\" font-size=\"10.00\">Softplus</text>\n",
       "<text text-anchor=\"start\" x=\"49\" y=\"-269.4\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"85.5,-278.4 85.5,-300.4 125.5,-300.4 125.5,-278.4 85.5,-278.4\"/>\n",
       "<text text-anchor=\"start\" x=\"93.83\" y=\"-286.4\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"125.5,-278.4 125.5,-300.4 169.5,-300.4 169.5,-278.4 125.5,-278.4\"/>\n",
       "<text text-anchor=\"start\" x=\"130.42\" y=\"-286.4\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 512) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"85.5,-256.4 85.5,-278.4 125.5,-278.4 125.5,-256.4 85.5,-256.4\"/>\n",
       "<text text-anchor=\"start\" x=\"90.08\" y=\"-264.4\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"125.5,-256.4 125.5,-278.4 169.5,-278.4 169.5,-256.4 125.5,-256.4\"/>\n",
       "<text text-anchor=\"start\" x=\"130.42\" y=\"-264.4\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 512) </text>\n",
       "</g>\n",
       "<!-- 27&#45;&gt;28 -->\n",
       "<g id=\"edge28\" class=\"edge\">\n",
       "<title>27&#45;&gt;28</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-336.5C106,-328.72 106,-319.7 106,-311.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-311.36 106,-301.36 102.5,-311.36 109.5,-311.36\"/>\n",
       "</g>\n",
       "<!-- 29 -->\n",
       "<g id=\"node30\" class=\"node\">\n",
       "<title>29</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"181,-220.4 31,-220.4 31,-176.4 181,-176.4 181,-220.4\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"31,-176.4 31,-220.4 97,-220.4 97,-176.4 31,-176.4\"/>\n",
       "<text text-anchor=\"start\" x=\"35.67\" y=\"-201.4\" font-family=\"Linux libertine\" font-size=\"10.00\">BatchNorm1d</text>\n",
       "<text text-anchor=\"start\" x=\"49\" y=\"-189.4\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"97,-198.4 97,-220.4 137,-220.4 137,-198.4 97,-198.4\"/>\n",
       "<text text-anchor=\"start\" x=\"105.33\" y=\"-206.4\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"137,-198.4 137,-220.4 181,-220.4 181,-198.4 137,-198.4\"/>\n",
       "<text text-anchor=\"start\" x=\"141.92\" y=\"-206.4\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 512) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"97,-176.4 97,-198.4 137,-198.4 137,-176.4 97,-176.4\"/>\n",
       "<text text-anchor=\"start\" x=\"101.58\" y=\"-184.4\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"137,-176.4 137,-198.4 181,-198.4 181,-176.4 137,-176.4\"/>\n",
       "<text text-anchor=\"start\" x=\"141.92\" y=\"-184.4\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 512) </text>\n",
       "</g>\n",
       "<!-- 28&#45;&gt;29 -->\n",
       "<g id=\"edge29\" class=\"edge\">\n",
       "<title>28&#45;&gt;29</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-256.5C106,-248.72 106,-239.7 106,-231.16\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-231.36 106,-221.36 102.5,-231.36 109.5,-231.36\"/>\n",
       "</g>\n",
       "<!-- 30 -->\n",
       "<g id=\"node31\" class=\"node\">\n",
       "<title>30</title>\n",
       "<polygon fill=\"#c1ffc1\" stroke=\"none\" points=\"167.5,-130 44.5,-130 44.5,-86 167.5,-86 167.5,-130\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"44.5,-86 44.5,-130 83.5,-130 83.5,-86 44.5,-86\"/>\n",
       "<text text-anchor=\"start\" x=\"50.95\" y=\"-111\" font-family=\"Linux libertine\" font-size=\"10.00\">Linear</text>\n",
       "<text text-anchor=\"start\" x=\"49\" y=\"-99\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:2</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"83.5,-108 83.5,-130 123.5,-130 123.5,-108 83.5,-108\"/>\n",
       "<text text-anchor=\"start\" x=\"91.83\" y=\"-116\" font-family=\"Linux libertine\" font-size=\"10.00\">input:</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"123.5,-108 123.5,-130 167.5,-130 167.5,-108 123.5,-108\"/>\n",
       "<text text-anchor=\"start\" x=\"128.42\" y=\"-116\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 512) </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"83.5,-86 83.5,-108 123.5,-108 123.5,-86 83.5,-86\"/>\n",
       "<text text-anchor=\"start\" x=\"88.08\" y=\"-94\" font-family=\"Linux libertine\" font-size=\"10.00\">output: </text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"123.5,-86 123.5,-108 167.5,-108 167.5,-86 123.5,-86\"/>\n",
       "<text text-anchor=\"start\" x=\"130.92\" y=\"-94\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 10) </text>\n",
       "</g>\n",
       "<!-- 29&#45;&gt;30 -->\n",
       "<g id=\"edge30\" class=\"edge\">\n",
       "<title>29&#45;&gt;30</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-176.45C106,-165.77 106,-152.56 106,-140.67\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-140.8 106,-130.8 102.5,-140.8 109.5,-140.8\"/>\n",
       "</g>\n",
       "<!-- 31 -->\n",
       "<g id=\"node32\" class=\"node\">\n",
       "<title>31</title>\n",
       "<polygon fill=\"lightyellow\" stroke=\"none\" points=\"155.99,-50 56.01,-50 56.01,-16 155.99,-16 155.99,-50\"/>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"56.01,-16 56.01,-50 119.33,-50 119.33,-16 56.01,-16\"/>\n",
       "<text text-anchor=\"start\" x=\"61.01\" y=\"-36\" font-family=\"Linux libertine\" font-size=\"10.00\">output&#45;tensor</text>\n",
       "<text text-anchor=\"start\" x=\"72.67\" y=\"-24\" font-family=\"Linux libertine\" font-size=\"10.00\">depth:0</text>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"119.33,-16 119.33,-50 155.99,-50 155.99,-16 119.33,-16\"/>\n",
       "<text text-anchor=\"start\" x=\"124.33\" y=\"-30\" font-family=\"Linux libertine\" font-size=\"10.00\">(1, 10)</text>\n",
       "</g>\n",
       "<!-- 30&#45;&gt;31 -->\n",
       "<g id=\"edge31\" class=\"edge\">\n",
       "<title>30&#45;&gt;31</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M106,-86.28C106,-78.28 106,-69.03 106,-60.55\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"109.5,-60.67 106,-50.67 102.5,-60.67 109.5,-60.67\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x2d21ddbbe50>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchview import draw_graph\n",
    "net2 = Fashion_MNIST_CNN()\n",
    "\n",
    "#model_graph = draw_graph(net2, input_size=(1,3,64,64), expand_nested=True)\n",
    "model_graph = draw_graph(net2, input_size=(1,1,28,28), expand_nested=True)\n",
    "model_graph.visual_graph\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Pytorch MNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torchenv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
