{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGU6NwlsXFSt"
   },
   "outputs": [],
   "source": [
    "#@title Import Dependencies\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import itertools\n",
    "import pickle\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from typing import TypeVar, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "Tensor = TypeVar('torch.tensor')\n",
    "\n",
    "#matplotlib.use('Agg')\n",
    "\n",
    "import click\n",
    "from argparse import Namespace\n",
    "import ast\n",
    "import os\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from typing import TypeVar, Tuple\n",
    "import gdown\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bNfVLRUYqZA"
   },
   "outputs": [],
   "source": [
    "#@title Define Hyperparameters\n",
    "\n",
    "# class_names = [\"5_o_Clock_Shadow\", \"Arched_Eyebrows\", \"Attractive\", \"Bags_Under_Eyes\", \"Bald\", \"Bangs\",\n",
    "#                 \"Big_Lips\", \"Big_Nose\", \"Black_Hair\", \"Blond_Hair\", \"Blurry\", \"Brown_Hair\", \"Bushy_Eyebrows\",\n",
    "#                 \"Chubby\", \"Double_Chin\", \"Eyeglasses\", \"Goatee\", \"Gray_Hair\", \"Heavy_Makeup\", \"High_Cheekbones\",\n",
    "#                 \"Male\", \"Mouth_Slightly_Open\", \"Mustache\", \"Narrow_Eyes\", \"No_Beard\", \"Oval_Face\", \"Pale_Skin\",\n",
    "#                 \"Pointy_Nose\", \"Receding_Hairline\", \"Rosy_Cheeks\", \"Sideburns\", \"Smiling\", \"Straight_Hair\",\n",
    "#                 \"Wavy_Hair\", \"Wearing_Earrings\", \"Wearing_Hat\", \"Wearing_Lipstick\", \"Wearing_Necklace\",\n",
    "#                 \"Wearing_Necktie\", \"Young\"]\n",
    "# num_classes = 40\n",
    "# data_shape = [3, 64, 64]\n",
    "# n_bits = 5\n",
    "# temp = 0.7\n",
    "# data_mean = [0.485, 0.456, 0.406]\n",
    "# data_std = [0.229, 0.224, 0.225]\n",
    "\n",
    "input_size = 28*28*1 # img_size = (28,28) ---> 28*28=784 in total\n",
    "batch_size = 200 # the size of input data took for one iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCsBCXMwbpH5"
   },
   "outputs": [],
   "source": [
    "# from torchvision.transforms import v2\n",
    "# transform_RandomErasing=transforms.Compose([v2.RandomErasing(),\n",
    "#                               transforms.ToTensor()])\n",
    "#from torchvision.transforms import v2\n",
    "#transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=0,std=1.0)])\n",
    "transform=transforms.Compose([transforms.ToTensor()])\n",
    "train_data = dsets.FashionMNIST(root = './data', train=True, transform = transform, download = True)\n",
    "test_data = dsets.FashionMNIST(root = './data', train=False, transform = transform, download = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfDPBdnYgfGp"
   },
   "outputs": [],
   "source": [
    "#@title Loading the data\n",
    "\n",
    "train_gen = torch.utils.data.DataLoader(dataset = train_data,\n",
    "                                             batch_size = batch_size,\n",
    "                                             shuffle = True)\n",
    "\n",
    "test_gen = torch.utils.data.DataLoader(dataset = test_data,\n",
    "                                      batch_size = batch_size,\n",
    "                                      shuffle = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cvmx=torch.zeros([3*64*64,3*64*64],device=device)\n",
    "images_list=[]\n",
    "labels_list=[]\n",
    "no_batches=len(train_gen)\n",
    "#images_mean=torch.zeros(3,64,64,device=device)\n",
    "for i ,(images,labels) in enumerate(train_gen):\n",
    "    images = Variable(images).cuda().detach()\n",
    "    labels=Variable(labels).cuda().detach()\n",
    "    # images_mean=images_mean+images.mean(0)\n",
    "    # im=torch.reshape(images,[images.shape[0],3*64*64])\n",
    "    # cvmx+=torch.matmul(torch.transpose(im,0,1),im)\n",
    "    if(i<(len(train_gen))):\n",
    "        images_list.append(images)\n",
    "        labels_list.append(labels)\n",
    "\n",
    "\n",
    "\n",
    "test_images_list=[]\n",
    "test_labels_list=[]\n",
    "test_no_batches=len(test_gen)\n",
    "for i ,(images,labels) in enumerate(test_gen):\n",
    "    images = Variable(images).cuda().detach()\n",
    "    labels=Variable(labels).cuda().detach()\n",
    "    if(i<(len(test_gen))):\n",
    "        test_images_list.append(images)\n",
    "        test_labels_list.append(labels)\n",
    "\n",
    "train_data_len=len(train_data)\n",
    "test_data_len=len(test_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fL-YXTvghaz_"
   },
   "outputs": [],
   "source": [
    "#@title Define model class\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from typing import TypeVar, Tuple\n",
    "\n",
    "Tensor = TypeVar('torch.tensor')\n",
    "\n",
    "\n",
    "class NeuralNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    base class for all NN classifiers\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv2d):\n",
    "                torch.nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.BatchNorm2d):\n",
    "                torch.nn.init.constant_(m.weight, 1)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.normal_(m.weight, 0, 0.01)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "\n",
    "class multinomial(NeuralNet):\n",
    "    \"\"\"\n",
    "    CNN for (binary) classification for CelebA, CheXpert\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 num_classes: int = 10,\n",
    "                 flattened_size: int = 28*28):\n",
    "        \"\"\"Builder.\"\"\"\n",
    "        super(multinomial, self).__init__()\n",
    "\n",
    "        self.last_layer=nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(flattened_size, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Perform forward.\"\"\"\n",
    "\n",
    "        x=self.last_layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def classify(self, x: Tensor) -> Tuple[Tensor, Tensor, Tensor]:\n",
    "        net_out = self.forward(x)\n",
    "        acc = F.softmax(net_out, dim=1)\n",
    "        class_idx = torch.max(net_out, 1)[1]\n",
    "\n",
    "        return acc, acc[0, class_idx], class_idx\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net=Fashion_MNIST_CNN()\n",
    "# n=0\n",
    "# for par in net.parameters():\n",
    "#     n+=par.numel()\n",
    "\n",
    "# n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePLIwvAFj2zH"
   },
   "outputs": [],
   "source": [
    "#@title Define loss-function & optimizer\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def images_regulariser(net): \n",
    "    li_reg_loss = 0\n",
    "    penalized     = [p for name,p in net.named_parameters() if 'bias' not in name]\n",
    "    not_penalized = [p for name,p in net.named_parameters() if 'bias' in name]\n",
    "    for p in penalized:\n",
    "        li_reg_loss += (p**2).sum()*0.5\n",
    "    #for p in net.parameters():\n",
    "#        li_reg_loss += (p**2).sum()*0.5\n",
    "    reg=li_reg_loss/(train_data_len)*l2regconst\n",
    "    return(reg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addnet(net,net2):\n",
    "    for param1, param2 in zip(net.parameters(), net2.parameters()):\n",
    "     param1.data += param2.data\n",
    "\n",
    "def multiplynet(net,a):\n",
    "   for param1 in net.parameters():\n",
    "     param1.data *=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class hclass:\n",
    "    h: Tensor\n",
    "    eta: Tensor\n",
    "    etam1g: Tensor\n",
    "    c11: Tensor\n",
    "    c21: Tensor\n",
    "    c22: Tensor\n",
    "\n",
    "@dataclass\n",
    "class BAOABhclass:\n",
    "    h: Tensor\n",
    "    eta: Tensor\n",
    "    xc1: Tensor\n",
    "    xc2: Tensor\n",
    "    xc3: Tensor\n",
    "    vc1: Tensor\n",
    "    vc2: Tensor\n",
    "    vc3: Tensor\n",
    "\n",
    "\n",
    "def hper2const(h,gam):\n",
    "    gh=gam.double()*h.double()\n",
    "    s=torch.sqrt(4*torch.expm1(-gh/2)-torch.expm1(-gh)+gh)\n",
    "    eta=(torch.exp(-gh/2)).float()\n",
    "    etam1g=((-torch.expm1(-gh/2))/gam.double()).float()\n",
    "    c11=(s/gam).float()\n",
    "    c21=(torch.exp(-gh)*(torch.expm1(gh/2.0))**2/s).float()\n",
    "    c22=(torch.sqrt(8*torch.expm1(-gh/2)-4*torch.expm1(-gh)-gh*torch.expm1(-gh))/s).float()\n",
    "    hc=hclass(h=h,eta=eta,etam1g=etam1g,c11=c11,c21=c21,c22=c22)\n",
    "    return(hc)\n",
    "\n",
    "def BAOAB_hconst(h,gam):\n",
    "    with torch.no_grad():\n",
    "        hh=copy.deepcopy(h).detach().double()\n",
    "        gamm=copy.deepcopy(gam).detach().double()\n",
    "        gh=gamm*hh\n",
    "        eta=(torch.exp(-gh/2))\n",
    "        xc1=hh/2*(1+eta)\n",
    "        xc2=(hh*hh/4)*(1+eta)\n",
    "        xc3=hh/2*torch.sqrt(-torch.expm1(-gh))\n",
    "        vc1=eta*(hh/2)\n",
    "        vc2=(hh/2)\n",
    "        vc3=torch.sqrt(-torch.expm1(-gh))\n",
    "\n",
    "        hc=BAOABhclass(h=hh.float(),eta=eta.float(),xc1=xc1.float(),xc2=xc2.float(),xc3=xc3.float(),vc1=vc1.float(),vc2=vc2.float(),vc3=vc3.float())\n",
    "        return(hc)\n",
    "\n",
    "    # eta=to_data_type(exp(-h*gam/2));\n",
    "    # h2=to_data_type(h/2);\n",
    "    # eta2=to_data_type(exp(-h2*gam/2));\n",
    "\n",
    "    # grad=gradp;\n",
    "    # xip=R;\n",
    "\n",
    "    # #xn=x+(h/2*(1+eta))*v-((h^2/4)*(1+eta))*grad+((h/2)*realsqrt(1-eta^2))*xip;\n",
    "    # #=x+xc1*v-xc2*grad+xc3*xip\n",
    "\n",
    "    # gradpn=grad_lpost(xn);\n",
    "    # vn=eta*(v-(h/2)*grad)+(realsqrt(1-eta^2))*xip-(h/2)*gradpn;    \n",
    "    # #vn=eta*v-vc1*grad-vc2*gradpn+vc3*xip\n",
    "\n",
    "def U(x,v,hc,xi1,xi2):\n",
    "\n",
    "    xn=x+hc.etam1g*v+hc.c11*xi1\n",
    "    vn=v*hc.eta+hc.c21*xi1+hc.c22*xi2\n",
    "    return([xn, vn])\n",
    "\n",
    "def bounce(x,v,xstar,width):\n",
    "    vsign=(((x-xstar+width)/(2*width)).floor()% 2)*(-2)+1\n",
    "    vn=v*vsign\n",
    "    xn=((x-xstar-width)% (4*width)-2*width).abs()-width+xstar  \n",
    "    return([xn, vn])\n",
    "\n",
    "def bouncenet():\n",
    "    for p,p_star in zip(net.parameters(),net_star.parameters()):\n",
    "        [p.data, p.v]=bounce(p.data, p.v, p_star.data, 6/torch.sqrt(l2regconst_extra))\n",
    "\n",
    "def svrg_grad(net, batch_it):\n",
    "    outputsU = net(images_list[batch_it])\n",
    "    loss_likelihood = loss_function(outputsU, labels_list[batch_it])  \n",
    "\n",
    "\n",
    "    grads_reg=[torch.zeros_like(par) for par in net.parameters()]\n",
    "    net_pars=list(net.parameters())\n",
    "    with torch.no_grad():\n",
    "        for it in range(len_params):\n",
    "            if(list_no_bias[it]):\n",
    "                grads_reg[it]=net_pars[it].data*l2regconst\n",
    "\n",
    "    net.zero_grad()\n",
    "    loss_likelihood.backward()\n",
    "    with torch.no_grad():\n",
    "        grads_likelihood=[par.grad*batch_size for par in net.parameters()]\n",
    "\n",
    "        svrg_grads=[]\n",
    "        for p,grad,grad_reg,p_star,grad_star,star_sum_grad in zip(list(net.parameters()),grads_likelihood,grads_reg,list(net_star.parameters()),net_star_grad_list[batch_it],net_star_full_grad):              \n",
    "            svrg_grads.append(grad_reg+star_sum_grad+(grad-grad_star)*no_batches+l2regconst_extra*(p.data-p_star.data))\n",
    "    return svrg_grads,loss_likelihood.data\n",
    "\n",
    "\n",
    "def UBU_step(hper2c,batch_it):   \n",
    "    with torch.no_grad():\n",
    "        for p in list(net.parameters()):\n",
    "            xi1=torch.randn_like(p.data,device=device)\n",
    "            xi2=torch.randn_like(p.data,device=device)\n",
    "            [p.data,p.v]=U(p.data,p.v,hper2c,xi1,xi2)\n",
    "\n",
    "\n",
    "    svrg_grads,loss_likelihood_data=svrg_grad(net, batch_it)\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for p,grad in zip(net.parameters(), svrg_grads):              \n",
    "            #Using variance reduction\n",
    "            p.v-=hper2c.h*grad\n",
    "      \n",
    "        for p in list(net.parameters()):\n",
    "            xi1=torch.randn_like(p.data,device=device)\n",
    "            xi2=torch.randn_like(p.data,device=device)\n",
    "            [p.data,p.v]=U(p.data,p.v,hper2c,xi1,xi2)\n",
    "\n",
    "    return(loss_likelihood_data)\n",
    "\n",
    "\n",
    "\n",
    "def UBU_step2(net, net2, hper4c,batch_it_list):   \n",
    "    with torch.no_grad():\n",
    "        for p,q in zip(net.parameters(),net2.parameters()):\n",
    "            xi1=torch.randn_like(p.data,device=device)\n",
    "            xi2=torch.randn_like(p.data,device=device)\n",
    "            [p.data,p.v]=U(p.data,p.v,hper4c,xi1,xi2)\n",
    "            [q.data,q.v]=U(q.data,q.v,hper4c,xi1,xi2)\n",
    "\n",
    "    svrg_grads2,_=svrg_grad(net2, batch_it_list[0])\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        for q,gradq in zip(net2.parameters(), svrg_grads2):              \n",
    "            q.v-=hper4c.h*gradq\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p,q in zip(net.parameters(),net2.parameters()):\n",
    "            xi1=torch.randn_like(p.data,device=device)\n",
    "            xi2=torch.randn_like(p.data,device=device)\n",
    "            [p.data,p.v]=U(p.data,p.v,hper4c,xi1,xi2)\n",
    "            [q.data,q.v]=U(q.data,q.v,hper4c,xi1,xi2)\n",
    "\n",
    "    svrg_grads,loss_likelihood_data=svrg_grad(net, batch_it_list[2])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p,grad in zip(net.parameters(), svrg_grads):              \n",
    "            p.v-=2*hper4c.h*grad\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p,q in zip(net.parameters(),net2.parameters()):\n",
    "            xi1=torch.randn_like(p.data,device=device)\n",
    "            xi2=torch.randn_like(p.data,device=device)\n",
    "            [p.data,p.v]=U(p.data,p.v,hper4c,xi1,xi2)\n",
    "            [q.data,q.v]=U(q.data,q.v,hper4c,xi1,xi2)\n",
    "\n",
    "    svrg_grads2,_=svrg_grad(net2, batch_it_list[1])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for q,grad in zip(net2.parameters(), svrg_grads2):              \n",
    "            q.v-=hper4c.h*grad\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p,q in zip(net.parameters(),net2.parameters()):\n",
    "            xi1=torch.randn_like(p.data,device=device)\n",
    "            xi2=torch.randn_like(p.data,device=device)\n",
    "            [p.data,p.v]=U(p.data,p.v,hper4c,xi1,xi2)\n",
    "            [q.data,q.v]=U(q.data,q.v,hper4c,xi1,xi2)\n",
    "    \n",
    "    return(loss_likelihood_data)\n",
    "\n",
    "def EM_step2(net, net2, h, gam, batch_it_list):   \n",
    "    svrg_grads2,_=svrg_grad(net2, batch_it_list[0])\n",
    "    svrg_grads,loss_likelihood_data=svrg_grad(net, batch_it_list[2])   \n",
    "    sqrt2=torch.tensor(2).sqrt().detach()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p,gradp in zip(net2.parameters(), svrg_grads2):              \n",
    "            p.xi=torch.randn_like(p.data,device=device)\n",
    "            p.data+=p.v*h/2\n",
    "            p.v-=(h/2)*gradp+gam*(h/2)*p.v-torch.sqrt(2*gam*h/2)*p.xi\n",
    "\n",
    "    svrg_grads2,_=svrg_grad(net2, batch_it_list[1])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p,gradp in zip(net2.parameters(), svrg_grads2):              \n",
    "            p.xi2=torch.randn_like(p.data,device=device)\n",
    "            p.data+=p.v*h/2\n",
    "            p.v-=(h/2)*gradp+gam*(h/2)*p.v-torch.sqrt(2*gam*h/2)*p.xi2\n",
    "\n",
    "        for p,q,gradp in zip(net.parameters(),net2.parameters(), svrg_grads):              \n",
    "            p.data+=p.v*h\n",
    "            p.v-=(h)*gradp+gam*(h)*p.v-torch.sqrt(2*gam*h)*(q.xi+q.xi2)/sqrt2\n",
    "\n",
    "\n",
    "    return(loss_likelihood_data)\n",
    "\n",
    "\n",
    "def BAOAB_step(net,hc,batch_it,last_grad):   \n",
    "    \n",
    "    #xn=x+xc1*v-xc2*grad+xc3*xip\n",
    "    with torch.no_grad():\n",
    "        for p,grad in zip(net.parameters(), last_grad):\n",
    "            p.xi=torch.randn_like(p.data,device=device)\n",
    "            p.data=p.data+hc.xc1*p.v-hc.xc2*grad+hc.xc3*p.xi\n",
    "\n",
    "    svrg_grads,loss_likelihood_data=svrg_grad(net, batch_it)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p,grad,gradn in zip(net.parameters(), last_grad,svrg_grads):              \n",
    "            p.v=hc.eta*p.v-hc.vc1*grad-hc.vc2*gradn+hc.vc3*p.xi\n",
    "\n",
    "    return(loss_likelihood_data,svrg_grads)\n",
    "\n",
    "\n",
    "\n",
    "def BAOAB_step2(net, net2, hc, hper2c, batch_it_list,last_grad,last_grad2):   \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p,grad in zip(net2.parameters(), last_grad2):\n",
    "            p.xi=torch.randn_like(p.data,device=device)\n",
    "            p.data=p.data+hper2c.xc1*p.v-hper2c.xc2*grad+hper2c.xc3*p.xi\n",
    "\n",
    "    svrg_grads2,_=svrg_grad(net2, batch_it_list[0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p,grad,gradn in zip(net2.parameters(), last_grad2,svrg_grads2):              \n",
    "            p.v=hper2c.eta*p.v-hper2c.vc1*grad-hper2c.vc2*gradn+hper2c.vc3*p.xi\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p,grad in zip(net2.parameters(), svrg_grads2):\n",
    "            p.xi2=torch.randn_like(p.data,device=device)\n",
    "            p.data=p.data+hper2c.xc1*p.v-hper2c.xc2*grad+hper2c.xc3*p.xi2\n",
    "\n",
    "    svrg_grads2n,_=svrg_grad(net2, batch_it_list[1])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p,grad,gradn in zip(net2.parameters(), svrg_grads2,svrg_grads2n):              \n",
    "            p.v=hper2c.eta*p.v-hper2c.vc1*grad-hper2c.vc2*gradn+hper2c.vc3*p.xi2\n",
    "\n",
    "    sqrt2=torch.tensor(2).sqrt().detach()\n",
    "    with torch.no_grad():\n",
    "        for p,q,grad in zip(net.parameters(),net2.parameters(), last_grad):\n",
    "            p.data=p.data+hc.xc1*p.v-hc.xc2*grad+hc.xc3*(q.xi+q.xi2)/sqrt2\n",
    "\n",
    "    svrg_grads,loss_likelihood_data=svrg_grad(net, batch_it_list[2])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for p,q,grad,gradn in zip(net.parameters(),net2.parameters(), last_grad,svrg_grads):              \n",
    "            p.v=hc.eta*p.v-hc.vc1*grad-hc.vc2*gradn+hc.vc3*(q.xi+q.xi2)/sqrt2\n",
    "\n",
    "    return(loss_likelihood_data,svrg_grads,svrg_grads2n)\n",
    "\n",
    "    # eta=to_data_type(exp(-h*gam/2));\n",
    "    # h2=to_data_type(h/2);\n",
    "    # eta2=to_data_type(exp(-h2*gam/2));\n",
    "\n",
    "    # grad=gradp;\n",
    "    # xip=R;\n",
    "\n",
    "    # #xn=x+(h/2*(1+eta))*v-((h^2/4)*(1+eta))*grad+((h/2)*realsqrt(1-eta^2))*xip;\n",
    "    # #xn=x+xc1*v-xc2*grad+xc3*xip\n",
    "\n",
    "    # gradpn=grad_lpost(xn);\n",
    "    # vn=eta*(v-(h/2)*grad)+(realsqrt(1-eta^2))*xip-(h/2)*gradpn;    \n",
    "    # #vn=eta*v-vc1*grad-vc2*gradpn+vc3*xip\n",
    "\n",
    "def ind_create(batch_it):\n",
    "    modit=batch_it %(2*no_batches)\n",
    "    ind=(modit<=(no_batches-1))*modit+(modit>=no_batches)*(2*no_batches-modit-1)\n",
    "    return ind\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#net = Fashion_MNIST_CNN().cuda()\n",
    "#net2=copy.deepcopy(net)\n",
    "#addnet(net2,net)\n",
    "#multiplynet(net2,1/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath=\"output_fashion_low_rank_9n.pickle\"\n",
    "# #filepath=\"output_fashion_low_rank_long.pickle\"\n",
    "# with open(filepath,\"rb\") as file:\n",
    "#    [labels_arr,test_labels_arr,test_prob_arr]=pickle.load(file)\n",
    "# labels_arr=torch.tensor(labels_arr).detach()\n",
    "# test_labels_arr=torch.tensor(test_labels_arr).detach()\n",
    "# test_prob_arr=torch.tensor(test_prob_arr).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u75Xa5VckuTH"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#@title Output arrays\n",
    "num_classes=10\n",
    "\n",
    "training_size=no_batches*batch_size\n",
    "test_size=test_data_len\n",
    "\n",
    "\n",
    "l2regconst=torch.tensor(50).detach()\n",
    "l2regconst_extra=torch.tensor(0).detach()\n",
    "gam=torch.sqrt(l2regconst)\n",
    "#hper2c=hper2const(torch.tensor(h/2),gam)\n",
    "all_images=torch.cat(images_list,dim=0).detach()\n",
    "all_labels=torch.cat(labels_list,dim=0).detach()\n",
    "# net = multinomial().cuda()\n",
    "# net2 = multinomial().cuda()\n",
    "# net.train()\n",
    "# net2.train()\n",
    "\n",
    "def find_MAP(num_steps):\n",
    "  net = multinomial().cuda()\n",
    "\n",
    "  def lpost():\n",
    "    outputs = net(all_images)    \n",
    "    loss_likelihood = loss_function(outputs, all_labels)\n",
    "    reg=images_regulariser(net)\n",
    "    loss=loss_likelihood+reg\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    return(loss)\n",
    "\n",
    "  optimizer = torch.optim.LBFGS(net.parameters(), history_size=30, max_iter=20)\n",
    "  for epoch in range(num_steps):\n",
    "    optimizer.step(lpost)\n",
    "  \n",
    "  net_star=copy.deepcopy(net)\n",
    "  len_params=len(list(net_star.parameters()))\n",
    "  #Variance reduction - saving gradients at each batch at x_star\n",
    "  net_star_grad_list=[]\n",
    "  net_star_full_grad=[torch.zeros_like(par, device=device) for par in list(net_star.parameters())]\n",
    "  for i in range(no_batches):\n",
    "      images=images_list[i]\n",
    "      labels=labels_list[i]\n",
    "      outputs=net_star(images)\n",
    "      loss_likelihood = loss_function(outputs, labels)\n",
    "      reg=images_regulariser(net)\n",
    "      net_star.zero_grad()\n",
    "      loss_likelihood.backward()\n",
    "      grads=[par.grad*batch_size for par in list(net_star.parameters())]\n",
    "      net_star_grad_list.append(grads)\n",
    "      for g, gi in zip(net_star_full_grad,grads):\n",
    "        g+=gi          \n",
    "\n",
    "  len_params=len(list(net_star.parameters()))\n",
    "  list_no_bias=torch.zeros(len_params)\n",
    "  pit=0\n",
    "  for name, p in net_star.named_parameters():\n",
    "      if 'bias' not in name:\n",
    "          list_no_bias[pit]=1.0\n",
    "      pit+=1\n",
    "  return net_star, net_star_grad_list, net_star_full_grad, len_params, list_no_bias\n",
    "\n",
    "  \n",
    "\n",
    "def SMS_UBU2(num_epochs,h,gam):\n",
    "  net=copy.deepcopy(net_star)\n",
    "  net2=copy.deepcopy(net_star)\n",
    "\n",
    "  with torch.no_grad():\n",
    "    hper4c=hper2const(torch.tensor(h/2),gam)\n",
    "    test_labels_arr=torch.zeros(test_size).detach()\n",
    "    test_prob_arr=torch.zeros([test_size,num_classes,num_epochs]).detach()\n",
    "    test_prob_arr2=torch.zeros([test_size,num_classes,num_epochs]).detach()  \n",
    "  #Initialise velocities\n",
    "    for par,par2 in zip(net.parameters(), net2.parameters()):\n",
    "      par.v = torch.randn_like(par,device=device).detach()\n",
    "      par2.v=copy.deepcopy(par.v).detach()\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    sum_loss=0\n",
    "    if(epoch%2==0):\n",
    "      rperm=random.permutation(list(range(no_batches)))      \n",
    "    rperm2=random.permutation(list(range(no_batches)))\n",
    "    for i in range(no_batches):\n",
    "      b=i        \n",
    "      it=epoch*batch_size+b\n",
    "      ind=ind_create(2*it)\n",
    "      ind2=ind_create(2*it+1)            \n",
    "      indc=ind_create(it)          \n",
    "      batch_it_list=[rperm2[ind], rperm2[ind2], rperm[indc]]\n",
    "\n",
    "      loss_likelihood=UBU_step2(net,net2,hper4c,batch_it_list)\n",
    "      sum_loss=sum_loss+loss_likelihood\n",
    "\n",
    "    print('Epoch [%d/%d], Step [%d/%d]' %(epoch+1, num_epochs, i+1, no_batches))\n",
    "    correct = 0\n",
    "    total = 0  \n",
    "\n",
    "    with torch.no_grad():\n",
    "      for testit in range(test_no_batches):\n",
    "        imagest=test_images_list[testit]\n",
    "        labelst=test_labels_list[testit]\n",
    "        actual_batch_size=len(imagest)\n",
    "        test_labels_arr[(testit*batch_size):(testit*batch_size+actual_batch_size)]=labelst.detach().cpu()\n",
    "        outputt = net(imagest).detach()#.reshape(actual_batch_size).detach()\n",
    "        outputt2= net2(imagest).detach()\n",
    "        _, predictedt = torch.max(outputt,1)\n",
    "        #_, predictedt2 = torch.max(outputt2,1)\n",
    "        correct += (predictedt == labelst).sum()\n",
    "        total += labelst.size(0)\n",
    "\n",
    "        test_prob_arr[(testit*batch_size):(testit*batch_size+actual_batch_size),0:num_classes,epoch]=torch.softmax(outputt,dim=1)\n",
    "\n",
    "        test_prob_arr2[(testit*batch_size):(testit*batch_size+actual_batch_size),0:num_classes,epoch]=torch.softmax(outputt2,dim=1)\n",
    "    \n",
    "    #net.train()       \n",
    "    print('Test accuracy of the model: %.3f %%' %((100*correct)/(total+1)))\n",
    "    print('Epoch [%d], Average Loss: %0.4f' %(epoch+1, sum_loss/no_batches))\n",
    "  return(test_labels_arr,test_prob_arr,test_prob_arr2)\n",
    "  \n",
    "def SG_UBU2(num_epochs,h,gam):\n",
    "  net=copy.deepcopy(net_star)\n",
    "  net2=copy.deepcopy(net_star)\n",
    "  with torch.no_grad():\n",
    "    hper4c=hper2const(torch.tensor(h/2),gam)\n",
    "    test_labels_arr=torch.zeros(test_size)\n",
    "    test_prob_arr=torch.zeros([test_size,num_classes,num_epochs])\n",
    "    test_prob_arr2=torch.zeros([test_size,num_classes,num_epochs])  \n",
    "  #Initialise velocities\n",
    "    for par,par2 in zip(net.parameters(), net2.parameters()):\n",
    "      par.v = torch.randn_like(par,device=device).detach()\n",
    "      par2.v=copy.deepcopy(par.v).detach()\n",
    "\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    sum_loss=0\n",
    "    for i in range(no_batches):\n",
    "      ind=torch.randint(high=no_batches,size=(1,1))\n",
    "      ind2=torch.randint(high=no_batches,size=(1,1))\n",
    "      if(torch.rand(1)<0.5):\n",
    "        indc=ind\n",
    "      else:\n",
    "        indc=ind2\n",
    "      batch_it_list=[ind, ind2, indc]\n",
    "\n",
    "      loss_likelihood=UBU_step2(net,net2,hper4c,batch_it_list)\n",
    "      sum_loss=sum_loss+loss_likelihood\n",
    "\n",
    "    print('Epoch [%d/%d], Step [%d/%d]' %(epoch+1, num_epochs, i+1, no_batches))\n",
    "    correct = 0\n",
    "    total = 0  \n",
    "\n",
    "    with torch.no_grad():\n",
    "      for testit in range(test_no_batches):\n",
    "        imagest=test_images_list[testit]\n",
    "        labelst=test_labels_list[testit]\n",
    "        actual_batch_size=len(imagest)\n",
    "        test_labels_arr[(testit*batch_size):(testit*batch_size+actual_batch_size)]=labelst.detach().cpu()\n",
    "        outputt = net(imagest).detach()#.reshape(actual_batch_size).detach()\n",
    "        outputt2= net2(imagest).detach()\n",
    "        _, predictedt = torch.max(outputt,1)\n",
    "        #_, predictedt2 = torch.max(outputt2,1)\n",
    "        correct += (predictedt == labelst).sum()\n",
    "        total += labelst.size(0)\n",
    "\n",
    "        test_prob_arr[(testit*batch_size):(testit*batch_size+actual_batch_size),0:num_classes,epoch]=torch.softmax(outputt,dim=1)\n",
    "\n",
    "        test_prob_arr2[(testit*batch_size):(testit*batch_size+actual_batch_size),0:num_classes,epoch]=torch.softmax(outputt2,dim=1)\n",
    "    \n",
    "    #net.train()       \n",
    "    print('Test accuracy of the model: %.3f %%' %((100*correct)/(total+1)))\n",
    "    print('Epoch [%d], Average Loss: %0.4f' %(epoch+1, sum_loss/no_batches))\n",
    "  return(test_labels_arr,test_prob_arr,test_prob_arr2)\n",
    "\n",
    "def SG_EM2(num_epochs,h,gam):\n",
    "  net=copy.deepcopy(net_star)\n",
    "  net2=copy.deepcopy(net_star)\n",
    "  with torch.no_grad():\n",
    "    test_labels_arr=torch.zeros(test_size)\n",
    "    test_prob_arr=torch.zeros([test_size,num_classes,num_epochs])\n",
    "    test_prob_arr2=torch.zeros([test_size,num_classes,num_epochs])  \n",
    "  #Initialise velocities\n",
    "    for par,par2 in zip(net.parameters(), net2.parameters()):\n",
    "      par.v = torch.randn_like(par,device=device).detach()\n",
    "      par2.v=copy.deepcopy(par.v).detach()\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    sum_loss=0\n",
    "    for i in range(no_batches):\n",
    "      ind=torch.randint(high=no_batches,size=(1,1))\n",
    "      ind2=torch.randint(high=no_batches,size=(1,1))\n",
    "      if(torch.rand(1)<0.5):\n",
    "        indc=ind\n",
    "      else:\n",
    "        indc=ind2\n",
    "      batch_it_list=[ind, ind2, indc]\n",
    "\n",
    "      loss_likelihood=EM_step2(net,net2,h,gam,batch_it_list)\n",
    "      sum_loss=sum_loss+loss_likelihood\n",
    "\n",
    "    print('Epoch [%d/%d], Step [%d/%d]' %(epoch+1, num_epochs, i+1, no_batches))\n",
    "    correct = 0\n",
    "    total = 0  \n",
    "\n",
    "    with torch.no_grad():\n",
    "      for testit in range(test_no_batches):\n",
    "        imagest=test_images_list[testit]\n",
    "        labelst=test_labels_list[testit]\n",
    "        actual_batch_size=len(imagest)\n",
    "        test_labels_arr[(testit*batch_size):(testit*batch_size+actual_batch_size)]=labelst.detach().cpu()\n",
    "        outputt = net(imagest).detach()#.reshape(actual_batch_size).detach()\n",
    "        outputt2= net2(imagest).detach()\n",
    "        _, predictedt = torch.max(outputt,1)\n",
    "        #_, predictedt2 = torch.max(outputt2,1)\n",
    "        correct += (predictedt == labelst).sum()\n",
    "        total += labelst.size(0)\n",
    "\n",
    "        test_prob_arr[(testit*batch_size):(testit*batch_size+actual_batch_size),0:num_classes,epoch]=torch.softmax(outputt,dim=1)\n",
    "\n",
    "        test_prob_arr2[(testit*batch_size):(testit*batch_size+actual_batch_size),0:num_classes,epoch]=torch.softmax(outputt2,dim=1)\n",
    "    \n",
    "    #net.train()       \n",
    "    print('Test accuracy of the model: %.3f %%' %((100*correct)/(total+1)))\n",
    "    print('Epoch [%d], Average Loss: %0.4f' %(epoch+1, sum_loss/no_batches))\n",
    "  return(test_labels_arr,test_prob_arr,test_prob_arr2)\n",
    "\n",
    "def SG_BAOAB(num_epochs,h,gam):\n",
    "  net=copy.deepcopy(net_star)\n",
    "  \n",
    "  with torch.nograd():\n",
    "    hc=BAOAB_hconst(h,gam)\n",
    "    test_labels_arr=torch.zeros(test_size)\n",
    "    test_prob_arr=torch.zeros([test_size,num_classes,num_epochs])\n",
    "    #Initialise velocities\n",
    "    for par in list(net.parameters()):\n",
    "      par.v = torch.randn_like(par,device=device)      \n",
    "\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    sum_loss=0\n",
    "    for i in range(no_batches):\n",
    "      ind=torch.randint(high=no_batches,size=(1,1))\n",
    "      if(epoch==0 and i==0):\n",
    "        last_grad,_=svrg_grad(net,ind)\n",
    "      loss_likelihood,last_grad=BAOAB_step(net,hc,ind,last_grad)\n",
    "\n",
    "      sum_loss=sum_loss+loss_likelihood\n",
    "\n",
    "    print('Epoch [%d/%d], Step [%d/%d]' %(epoch+1, num_epochs, i+1, no_batches))\n",
    "    correct = 0\n",
    "    total = 0  \n",
    "\n",
    "    with torch.no_grad():\n",
    "      for testit in range(test_no_batches):\n",
    "        imagest=test_images_list[testit]\n",
    "        labelst=test_labels_list[testit]\n",
    "        actual_batch_size=len(imagest)\n",
    "        test_labels_arr[(testit*batch_size):(testit*batch_size+actual_batch_size)]=labelst.detach().cpu()\n",
    "        outputt = net(imagest).detach()#.reshape(actual_batch_size).detach()\n",
    "        _, predictedt = torch.max(outputt,1)\n",
    "        #_, predictedt2 = torch.max(outputt2,1)\n",
    "        correct += (predictedt == labelst).sum()\n",
    "        total += labelst.size(0)\n",
    "\n",
    "        test_prob_arr[(testit*batch_size):(testit*batch_size+actual_batch_size),0:num_classes,epoch]=torch.softmax(outputt,dim=1)\n",
    "    \n",
    "    #net.train()       \n",
    "    print('Test accuracy of the model: %.3f %%' %((100*correct)/(total+1)))\n",
    "    print('Epoch [%d], Average Loss: %0.4f' %(epoch+1, sum_loss/no_batches))\n",
    "  return(test_labels_arr,test_prob_arr)\n",
    "\n",
    "\n",
    "\n",
    "def SG_BAOAB2(num_epochs,h,gam):\n",
    "  net=copy.deepcopy(net_star)\n",
    "  net2=copy.deepcopy(net_star)  \n",
    "  with torch.no_grad():\n",
    "    hc=BAOAB_hconst(h,gam)\n",
    "    hper2c=BAOAB_hconst(h/2,gam)      \n",
    "    test_labels_arr=torch.zeros(test_size)\n",
    "    test_prob_arr=torch.zeros([test_size,num_classes,num_epochs])\n",
    "    test_prob_arr2=torch.zeros([test_size,num_classes,num_epochs])  \n",
    "  #Initialise velocities\n",
    "    for par,par2 in zip(net.parameters(), net2.parameters()):\n",
    "      par.v = torch.randn_like(par,device=device).detach()\n",
    "      par2.v=copy.deepcopy(par.v).detach()\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    sum_loss=0\n",
    "    for i in range(no_batches):\n",
    "\n",
    "      ind=torch.randint(high=no_batches,size=(1,1))\n",
    "      ind2=torch.randint(high=no_batches,size=(1,1))\n",
    "      if(torch.rand(1)<0.5):\n",
    "        indc=ind\n",
    "      else:\n",
    "        indc=ind2\n",
    "      batch_it_list=[ind, ind2, indc]\n",
    "\n",
    "      if(epoch==0 and i==0):\n",
    "        last_grad,_=svrg_grad(net,ind)\n",
    "        last_grad2,_=svrg_grad(net2,indc)\n",
    "\n",
    "      loss_likelihood,last_grad,last_grad2=BAOAB_step2(net,net2,hc,hper2c,batch_it_list,last_grad,last_grad2)\n",
    "\n",
    "      sum_loss=sum_loss+loss_likelihood\n",
    "\n",
    "    print('Epoch [%d/%d], Step [%d/%d]' %(epoch+1, num_epochs, i+1, no_batches))\n",
    "    correct = 0\n",
    "    total = 0  \n",
    "\n",
    "    with torch.no_grad():\n",
    "      for testit in range(test_no_batches):\n",
    "        imagest=test_images_list[testit]\n",
    "        labelst=test_labels_list[testit]\n",
    "        actual_batch_size=len(imagest)\n",
    "        test_labels_arr[(testit*batch_size):(testit*batch_size+actual_batch_size)]=labelst.detach().cpu()\n",
    "        outputt = net(imagest).detach()#.reshape(actual_batch_size).detach()\n",
    "        outputt2 = net2(imagest).detach()#.reshape(actual_batch_size).detach()\n",
    "        _, predictedt = torch.max(outputt,1)\n",
    "        #_, predictedt2 = torch.max(outputt2,1)\n",
    "        correct += (predictedt == labelst).sum()\n",
    "        total += labelst.size(0)\n",
    "\n",
    "        test_prob_arr[(testit*batch_size):(testit*batch_size+actual_batch_size),0:num_classes,epoch]=torch.softmax(outputt,dim=1)\n",
    "        test_prob_arr2[(testit*batch_size):(testit*batch_size+actual_batch_size),0:num_classes,epoch]=torch.softmax(outputt2,dim=1)\n",
    "    \n",
    "    #net.train()       \n",
    "    print('Test accuracy of the model: %.3f %%' %((100*correct)/(total+1)))\n",
    "    print('Epoch [%d], Average Loss: %0.4f' %(epoch+1, sum_loss/no_batches))\n",
    "  return(test_labels_arr,test_prob_arr,test_prob_arr2)\n",
    "\n",
    "\n",
    "def SMS_BAOAB2(num_epochs,h,gam):\n",
    "  net=copy.deepcopy(net_star)\n",
    "  net2=copy.deepcopy(net_star)  \n",
    "  with torch.no_grad():\n",
    "    hc=BAOAB_hconst(h,gam)\n",
    "    hper2c=BAOAB_hconst(h/2,gam)      \n",
    "    test_labels_arr=torch.zeros(test_size)\n",
    "    test_prob_arr=torch.zeros([test_size,num_classes,num_epochs])\n",
    "    test_prob_arr2=torch.zeros([test_size,num_classes,num_epochs])  \n",
    "  #Initialise velocities\n",
    "    for par,par2 in zip(net.parameters(), net2.parameters()):\n",
    "      par.v = torch.randn_like(par,device=device).detach()\n",
    "      par2.v=copy.deepcopy(par.v).detach()\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    sum_loss=0\n",
    "    if(epoch%2==0):\n",
    "      rperm=random.permutation(list(range(no_batches)))      \n",
    "    rperm2=random.permutation(list(range(no_batches)))\n",
    "    for i in range(no_batches):\n",
    "      b=i        \n",
    "      it=epoch*batch_size+b\n",
    "      ind=ind_create(2*it)\n",
    "      ind2=ind_create(2*it+1)            \n",
    "      indc=ind_create(it)          \n",
    "      batch_it_list=[rperm2[ind], rperm2[ind2], rperm[indc]]\n",
    "\n",
    "      if(epoch==0 and i==0):\n",
    "        last_grad,_=svrg_grad(net,ind)\n",
    "        last_grad2,_=svrg_grad(net2,indc)\n",
    "\n",
    "      loss_likelihood,last_grad,last_grad2=BAOAB_step2(net,net2,hc,hper2c,batch_it_list,last_grad,last_grad2)\n",
    "\n",
    "      sum_loss=sum_loss+loss_likelihood\n",
    "\n",
    "    print('Epoch [%d/%d], Step [%d/%d]' %(epoch+1, num_epochs, i+1, no_batches))\n",
    "    correct = 0\n",
    "    total = 0  \n",
    "\n",
    "    with torch.no_grad():\n",
    "      for testit in range(test_no_batches):\n",
    "        imagest=test_images_list[testit]\n",
    "        labelst=test_labels_list[testit]\n",
    "        actual_batch_size=len(imagest)\n",
    "        test_labels_arr[(testit*batch_size):(testit*batch_size+actual_batch_size)]=labelst.detach().cpu()\n",
    "        outputt = net(imagest).detach()#.reshape(actual_batch_size).detach()\n",
    "        outputt2 = net2(imagest).detach()#.reshape(actual_batch_size).detach()\n",
    "        _, predictedt = torch.max(outputt,1)\n",
    "        #_, predictedt2 = torch.max(outputt2,1)\n",
    "        correct += (predictedt == labelst).sum()\n",
    "        total += labelst.size(0)\n",
    "\n",
    "        test_prob_arr[(testit*batch_size):(testit*batch_size+actual_batch_size),0:num_classes,epoch]=torch.softmax(outputt,dim=1)\n",
    "        test_prob_arr2[(testit*batch_size):(testit*batch_size+actual_batch_size),0:num_classes,epoch]=torch.softmax(outputt2,dim=1)\n",
    "    \n",
    "    #net.train()       \n",
    "    print('Test accuracy of the model: %.3f %%' %((100*correct)/(total+1)))\n",
    "    print('Epoch [%d], Average Loss: %0.4f' %(epoch+1, sum_loss/no_batches))\n",
    "  return(test_labels_arr,test_prob_arr,test_prob_arr2)\n",
    "\n",
    "net_star, net_star_grad_list, net_star_full_grad, len_params, list_no_bias=find_MAP(40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=0.5\n",
    "num_epochs=int(400*rat)\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_BAOAB2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_BAOAB_h-1.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,int(40*rat):num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,int(40*rat):num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=1\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_BAOAB2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_BAOAB_h0.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=2\n",
    "num_epochs=400*rat\n",
    "h=torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_BAOAB2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_BAOAB_h1.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=4\n",
    "num_epochs=400*rat\n",
    "h=torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_BAOAB2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_BAOAB_h2.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=8\n",
    "num_epochs=400*rat\n",
    "h=torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_BAOAB2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_BAOAB_h3.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=16\n",
    "num_epochs=400*rat\n",
    "h=torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_BAOAB2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_BAOAB_h4.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=32\n",
    "num_epochs=400*rat\n",
    "h=torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_BAOAB2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_BAOAB_h5.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=0.5\n",
    "num_epochs=int(400*rat)\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SMS_BAOAB2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SMS_BAOAB_h-1.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,int(40*rat):num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,int(40*rat):num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=1\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SMS_BAOAB2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SMS_BAOAB_h0.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=2\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SMS_BAOAB2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SMS_BAOAB_h1.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=4\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SMS_BAOAB2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SMS_BAOAB_h2.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=8\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SMS_BAOAB2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SMS_BAOAB_h3.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=16\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SMS_BAOAB2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SMS_BAOAB_h4.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=32\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SMS_BAOAB2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SMS_BAOAB_h5.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=0.5\n",
    "num_epochs=int(400*rat)\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_UBU2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_UBU_h-1.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,int(40*rat):num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,int(40*rat):num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=1\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_UBU2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_UBU_h0.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=2\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_UBU2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_UBU_h1.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=4\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_UBU2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_UBU_h2.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=8\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_UBU2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_UBU_h3.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=16\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_UBU2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_UBU_h4.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=32\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_UBU2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_UBU_h5.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=0.5\n",
    "num_epochs=int(400*rat)\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SMS_UBU2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SMS_UBU_h-1.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,int(40*rat):num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,int(40*rat):num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=1\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SMS_UBU2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SMS_UBU_h0.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=2\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SMS_UBU2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SMS_UBU_h1.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=4\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SMS_UBU2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SMS_UBU_h2.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=8\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SMS_UBU2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SMS_UBU_h3.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=16\n",
    "num_epochs=800*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SMS_UBU2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SMS_UBU_h4l.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,10*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,10*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=16\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SMS_UBU2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SMS_UBU_h4.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=32\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SMS_UBU2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SMS_UBU_h5.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=0.5\n",
    "num_epochs=int(400*rat)\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_EM2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_EM_h-1.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,int(40*rat):num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,int(40*rat):num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=1\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_EM2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_EM_h0.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=2\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_EM2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_EM_h1.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=4\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_EM2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_EM_h2.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=8\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_EM2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_EM_h3.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=16\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_EM2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_EM_h4.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=32\n",
    "num_epochs=400*rat\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_EM2(num_epochs,h,gam)\n",
    "filepath=\"output_fashion_multinomial_SG_EM_h5.pickle\"\n",
    "with open(filepath,\"wb\") as file:\n",
    "  pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rat=64\n",
    "num_epochs=800\n",
    "h= torch.tensor(1e-3)/rat\n",
    "test_labels_arr,test_prob_arr,test_prob_arr2=SG_EM2(num_epochs,h,gam)\n",
    "# filepath=\"output_fashion_multinomial_SG_EM_h5.pickle\"\n",
    "# with open(filepath,\"wb\") as file:\n",
    "#   pickle.dump([test_labels_arr,test_prob_arr, test_prob_arr2],file)\n",
    "\n",
    "test_prob=torch.Tensor(test_prob_arr[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "test_prob2=torch.Tensor(test_prob_arr2[:,:,40*rat:num_epochs]).mean(-1).reshape(test_size,num_classes)\n",
    "\n",
    "test_prob_correct_label=torch.Tensor([test_prob[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "test_prob_correct_label2=torch.Tensor([test_prob2[it, test_labels_arr[it].int()] for it in range(test_size)])\n",
    "print(\"Average bias in probability of true category\", (test_prob_correct_label-test_prob_correct_label2).abs().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_params=len(list(net_star.parameters()))\n",
    "list_no_bias=torch.zeros(len_params)\n",
    "it=0\n",
    "for name, p in net_star.named_parameters():\n",
    "    if 'bias' not in name:\n",
    "        list_no_bias[it]=1.0\n",
    "    it+=1\n",
    "\n",
    "def hess_vec(vec):   \n",
    "    out=net_star(images)\n",
    "    loss=loss_function(out,labels)*train_data_len\n",
    "    net_star.zero_grad()\n",
    "    grad=list(torch.autograd.grad(loss,list(net_star.parameters()),create_graph=True))\n",
    "    res=torch.zeros(1).cuda()\n",
    "    for it in range(len_params):\n",
    "        res+=(grad[it]*vec[it]).sum()\n",
    "\n",
    "    net_star.zero_grad()\n",
    "    hvp=list(torch.autograd.grad(res,list(net_star.parameters()),create_graph=False))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for it in range(len_params):\n",
    "            if(list_no_bias[it]):\n",
    "                hvp[it]+=vec[it]*l2regconst\n",
    "    return(hvp)\n",
    "\n",
    "def hess_vec_full(vec):\n",
    "    hvfull=copy.deepcopy(list(vec))\n",
    "    for p in hvfull:\n",
    "        p.requires_grad=False\n",
    "        p*=0\n",
    "    len_pars=len(hvfull)\n",
    "    # for it in range(no_batches):\n",
    "    #     images=images_list[it]\n",
    "    #     labels=labels_list[it]\n",
    "    #     hv=hess_vec(vec)\n",
    "    #     for pit in range(len_pars):\n",
    "    #         hvfull[pit]+=hv[pit]\n",
    "    images=all_images\n",
    "    labels=all_labels\n",
    "    hv=hess_vec(vec)\n",
    "    for pit in range(len_pars):\n",
    "        hvfull[pit]+=hv[pit]\n",
    "\n",
    "    return(hvfull)\n",
    "\n",
    "def norm_par(vec):\n",
    "    n=torch.zeros(1).detach().cuda()\n",
    "    for p in vec:\n",
    "        n+=p.pow(2).sum()\n",
    "    return n.sqrt()\n",
    "\n",
    "def multiply_par(vec,c):\n",
    "    res_vec=copy.deepcopy(vec)\n",
    "    for it in range(len_params):\n",
    "        res_vec[it].requires_grad=False\n",
    "        res_vec[it]=res_vec[it]*c\n",
    "    return res_vec\n",
    "\n",
    "def scalar_prod_par(vec1,vec2):\n",
    "    res=torch.zeros(1).cuda()\n",
    "    for it in range(len_params):\n",
    "        res+=(vec1[it]*vec2[it]).sum()\n",
    "    return res\n",
    "\n",
    "def add_par(vec1,vec2):\n",
    "    res_vec=copy.deepcopy(vec1)\n",
    "    for it in range(len_params):\n",
    "        res_vec[it].requires_grad=False\n",
    "        res_vec[it]=vec1[it]+vec2[it]\n",
    "    return res_vec\n",
    "\n",
    "# print(norm_par(list(net_star.parameters())))\n",
    "# print(norm_par(multiply_par(list(net_star.parameters()),0.1)))\n",
    "\n",
    "vec=copy.deepcopy(list(net_star.parameters()))\n",
    "for p in vec:\n",
    "    p.requires_grad=False\n",
    "norm_iter=norm_par(vec)\n",
    "vec=multiply_par(vec,torch.Tensor(1/norm_iter))\n",
    "for iter in range(20):\n",
    "    vec=hess_vec_full(vec)\n",
    "    norm_iter=norm_par(vec)\n",
    "    vec=multiply_par(vec,1/norm_iter)    \n",
    "    print(norm_iter)\n",
    "\n",
    "max_eigen_vec=copy.deepcopy(vec)\n",
    "max_eigen=norm_iter\n",
    "\n",
    "vec=copy.deepcopy(list(net_star.parameters()))\n",
    "for p in vec:\n",
    "    p.requires_grad=False\n",
    "vec=add_par(vec,multiply_par(max_eigen_vec,-scalar_prod_par(vec,max_eigen_vec)))\n",
    "norm_iter=norm_par(vec)\n",
    "vec=multiply_par(vec,torch.Tensor(1/norm_iter))\n",
    "\n",
    "for iter in range(20):\n",
    "    vec=hess_vec_full(vec)\n",
    "    vec=add_par(vec,multiply_par(max_eigen_vec,-scalar_prod_par(vec,max_eigen_vec)))\n",
    "    norm_iter=norm_par(vec)\n",
    "    vec=multiply_par(vec,1/norm_iter)    \n",
    "    print(norm_iter)\n",
    "\n",
    "\n",
    "# hvpf=hess_vec_full(vtest)\n",
    "# nhvp=0\n",
    "# for p in hvpf:\n",
    "#     nhvp+=p.abs().sum()\n",
    "# print(nhvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filepath=\"output_fashion.pickle\"\n",
    "#with open(filepath,\"rb\") as file:\n",
    "#    [labels_arr,test_labels_arr,test_prob_arr,_,_]=pickle.load(file)\n",
    "#labels_arr=torch.tensor(labels_arr).detach()\n",
    "#test_labels_arr=torch.tensor(test_labels_arr).detach()\n",
    "#test_prob_arr=torch.tensor(test_prob_arr).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome should be a binary list of the ordinal outcome. [0, 1, 0] for exmaple.\n",
    "# Probs should be a list of probabilities. [0.79, 0.09, 0.12] for example.\n",
    "# Outcome and Probs must be provided with the same order as probabilities.\n",
    "\n",
    "def rps_single(probs, true_label):\n",
    "    outcome=torch.zeros(num_classes)\n",
    "    outcome[true_label.int()]=1.0\n",
    "    cum_probs = torch.cumsum(probs,0)\n",
    "    cum_outcomes = torch.cumsum(outcome,0)\n",
    "    \n",
    "    #print(cum_outcomes)\n",
    "    #print(cum_probs)\n",
    "    sum_rps = 0\n",
    "    for i in range(len(outcome)):         \n",
    "        sum_rps+= (cum_probs[i] - cum_outcomes[i])**2\n",
    "    \n",
    "    return sum_rps/(num_classes-1)\n",
    "\n",
    "def rps_calc(test_probs, true_labels):\n",
    "    rps_vec=torch.zeros(test_data_len)\n",
    "    for it in range(test_data_len):\n",
    "        rps_vec[it]=rps_single(test_probs[it,:].reshape(num_classes),true_labels[it])\n",
    "    return rps_vec\n",
    "\n",
    "def nll_calc(test_probs, true_labels):\n",
    "    res=0\n",
    "    for it in range(test_data_len):\n",
    "        res-=torch.log(test_probs[it,true_labels[it].int()])\n",
    "    return res/test_data_len\n",
    "\n",
    "def adaptive_calibration_error(test_probs,true_labels, num_bins=20):\n",
    "    max_probs, predicted_labels = torch.max(test_probs,1)\n",
    "    ind=torch.argsort(max_probs,stable=True)\n",
    "    sorted_max_probs=max_probs[ind]\n",
    "    sorted_predicted_labels=predicted_labels[ind]\n",
    "    sorted_true_labels=true_labels[ind]\n",
    "\n",
    "    correct = (sorted_predicted_labels == sorted_true_labels).clone().detach().float()\n",
    "    bins=(torch.tensor(range(test_data_len))/torch.tensor(test_data_len/num_bins)).floor()\n",
    "\n",
    "    o=torch.tensor(0.0)\n",
    "    for b in range(num_bins):\n",
    "        mask = (bins == b)\n",
    "        if torch.any(mask):\n",
    "            #print(b, sorted_max_probs[mask].mean(), (correct[mask] - sorted_max_probs[mask]).mean())\n",
    "            o += (correct[mask] - sorted_max_probs[mask]).mean().abs()\n",
    "\n",
    "    return o / num_bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def GRdiagnostics(res):\n",
    "#   J=res.shape[0] #Number of chains\n",
    "#   L=res.shape[1] #Number of samples after burnin\n",
    "#   res_means=res.mean(dim=1)\n",
    "#   res_mean=res_means.mean()\n",
    "#   B=(res_means-res_mean).pow(2).sum()*L/(J-1)\n",
    "#   W=(res_means.reshape([J,1])@torch.ones([1,L])-res).pow(2).sum()/(J*(L-1))\n",
    "#   R=(W*(L-1)/L+B/L)/W\n",
    "#   return R\n",
    "\n",
    "\n",
    "# par_chains=4\n",
    "# no_GR_epochs=40\n",
    "# test_prob_GR_arr=torch.zeros([test_size,num_classes])\n",
    "# nll_GR_arr=torch.zeros([par_chains,no_GR_epochs])\n",
    "# for chain in range(par_chains):\n",
    "#     net=copy.deepcopy(net_star)\n",
    "#     net.eval()\n",
    "#     for par in list(net.parameters()):\n",
    "#       par.v = torch.randn_like(par,device=device)          \n",
    "#     for epoch in range(no_GR_epochs):\n",
    "#       print(\"chain: \",chain, \"/epoch:\",epoch)\n",
    "#       if(epoch % 2 == 1):\n",
    "#         irange=range(no_batches-1,-1,-1)\n",
    "#       else:\n",
    "#         irange=range(no_batches)\n",
    "#       for b in irange:\n",
    "#         images=images_list[b]\n",
    "#         labels=labels_list[b]\n",
    "#         UBU_step(hper2c,images,labels,b)\n",
    "\n",
    "#       for testit in range(test_no_batches):\n",
    "#         imagest=test_images_list[testit]\n",
    "#         labelst=test_labels_list[testit]\n",
    "#         actual_batch_size=len(imagest)\n",
    "#         outputt = net(imagest).detach()\n",
    "#         test_prob_GR_arr[(testit*batch_size):(testit*batch_size+actual_batch_size),:]=torch.softmax(outputt,dim=1)\n",
    "      \n",
    "#       nll_GR_arr[chain,epoch]=nll_calc(test_prob_GR_arr,test_labels_arr)\n",
    "#       print(\"NLL:\", nll_GR_arr[chain,epoch])\n",
    "\n",
    "# print(GRdiagnostics(nll_GR_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_prob=torch.Tensor(test_prob_arr[:,:,29,0]).reshape(test_size,num_classes)\n",
    "#torch.cumsum(test_prob[1,:].reshape(num_classes),0)\n",
    "#rps_single(test_prob[1,:].reshape(num_classes),test_labels_arr[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs=1\n",
    "#no bayesian\n",
    "def compute_acc_ace_rps_no_bayes(es):\n",
    "    copies=int(num_runs/es)\n",
    "    ace_arr=torch.zeros(copies)\n",
    "    rps_arr=torch.zeros(copies)\n",
    "    nll_arr=torch.zeros(copies)\n",
    "    accuracy_arr=torch.zeros(copies)\n",
    "\n",
    "    for it in range(copies):\n",
    "        test_prob=torch.Tensor(test_prob_arr[:,:,14,it*es:(it+1)*es]).mean(-1).reshape(test_size,num_classes)\n",
    "        ace_arr[it]=adaptive_calibration_error(test_prob,test_labels_arr)\n",
    "        rps_arr[it]=(rps_calc(test_prob, test_labels_arr)).mean()\n",
    "        nll_arr[it]=nll_calc(test_prob, test_labels_arr)\n",
    "        _, predictedt = torch.max(test_prob,1)\n",
    "        accuracy_arr[it]= (predictedt==test_labels_arr.reshape(1,test_size)).sum()/test_size\n",
    "    print(\"Non-Bayesian, ensemble size:\", es)\n",
    "    print(\"mean accuracy:\",accuracy_arr.mean(),\"std:\",accuracy_arr.std())\n",
    "    print(\"mean ace:\",ace_arr.mean(),\"std:\",ace_arr.std())\n",
    "    print(\"mean nll:\",nll_arr.mean(),\"std:\",nll_arr.std())\n",
    "    print(\"mean rps:\",rps_arr.mean(),\"std:\",rps_arr.std())\n",
    "    return [accuracy_arr.mean(),accuracy_arr.std(),ace_arr.mean(),ace_arr.std(),rps_arr.mean(),rps_arr.std(),nll_arr.mean(),nll_arr.std()]\n",
    "\n",
    "acc=torch.zeros(5)\n",
    "acc_std=torch.zeros(5)\n",
    "ace=torch.zeros(5)\n",
    "ace_std=torch.zeros(5)\n",
    "rps=torch.zeros(5)\n",
    "rps_std=torch.zeros(5)\n",
    "nll=torch.zeros(5)\n",
    "nll_std=torch.zeros(5)\n",
    "[acc[0],acc_std[0],ace[0],ace_std[0],rps[0],rps_std[0],nll[0],nll_std[0]]=compute_acc_ace_rps_no_bayes(1)\n",
    "[acc[1],acc_std[1],ace[1],ace_std[1],rps[1],rps_std[1],nll[1],nll_std[1]]=compute_acc_ace_rps_no_bayes(2)\n",
    "[acc[2],acc_std[2],ace[2],ace_std[2],rps[2],rps_std[2],nll[2],nll_std[2]]=compute_acc_ace_rps_no_bayes(4)\n",
    "[acc[3],acc_std[3],ace[3],ace_std[3],rps[3],rps_std[3],nll[3],nll_std[3]]=compute_acc_ace_rps_no_bayes(8)\n",
    "[acc[4],acc_std[4],ace[4],ace_std[4],rps[4],rps_std[4],nll[4],nll_std[4]]=compute_acc_ace_rps_no_bayes(16)\n",
    "\n",
    "# from scipy.io import savemat\n",
    "# filepath=\"results_fashion_no_bayes.mat\"\n",
    "# mdic={\"acc\":acc.cpu().numpy(),\"acc_std\":acc_std.cpu().numpy(),\"nll\": nll.cpu().numpy(),\"nll_std\":nll_std.cpu().numpy(),\\\n",
    "#       \"ace\": ace.cpu().numpy(),\"ace_std\":ace_std.cpu().numpy(), \"rps\":rps.cpu().numpy(),\"rps_std\":rps_std.cpu().numpy()}\n",
    "# savemat(filepath,mdic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-Bayesian, ensemble size: 1\n",
    "mean accuracy: tensor(0.9296) std: tensor(0.0020)\n",
    "mean ace: tensor(0.0555) std: tensor(0.0018)\n",
    "mean nll: tensor(0.4709) std: tensor(0.0251)\n",
    "mean rps: tensor(0.0230) std: tensor(0.0008)\n",
    "Non-Bayesian, ensemble size: 2\n",
    "mean accuracy: tensor(0.9383) std: tensor(0.0015)\n",
    "mean ace: tensor(0.0294) std: tensor(0.0016)\n",
    "mean nll: tensor(0.3109) std: tensor(0.0127)\n",
    "mean rps: tensor(0.0188) std: tensor(0.0005)\n",
    "Non-Bayesian, ensemble size: 4\n",
    "mean accuracy: tensor(0.9422) std: tensor(0.0010)\n",
    "mean ace: tensor(0.0177) std: tensor(0.0011)\n",
    "mean nll: tensor(0.2385) std: tensor(0.0066)\n",
    "mean rps: tensor(0.0168) std: tensor(0.0003)\n",
    "Non-Bayesian, ensemble size: 8\n",
    "mean accuracy: tensor(0.9443) std: tensor(0.0006)\n",
    "mean ace: tensor(0.0126) std: tensor(0.0007)\n",
    "mean nll: tensor(0.2025) std: tensor(0.0042)\n",
    "mean rps: tensor(0.0157) std: tensor(0.0002)\n",
    "Non-Bayesian, ensemble size: 16\n",
    "mean accuracy: tensor(0.9459) std: tensor(0.0009)\n",
    "mean ace: tensor(0.0098) std: tensor(0.0007)\n",
    "mean nll: tensor(0.1832) std: tensor(0.0029)\n",
    "mean rps: tensor(0.0152) std: tensor(8.2395e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#swa\n",
    "def compute_acc_ace_rps_swa(es):\n",
    "    copies=int(num_runs/es)\n",
    "    ace_arr=torch.zeros(copies)\n",
    "    rps_arr=torch.zeros(copies)\n",
    "    nll_arr=torch.zeros(copies)\n",
    "    accuracy_arr=torch.zeros(copies)\n",
    "\n",
    "    for it in range(copies):\n",
    "        test_prob=torch.Tensor(test_prob_arr[:,:,19,it*es:(it+1)*es]).mean(-1).reshape(test_size,num_classes)\n",
    "        ace_arr[it]=adaptive_calibration_error(test_prob,test_labels_arr)\n",
    "        rps_arr[it]=(rps_calc(test_prob, test_labels_arr)).mean()\n",
    "        nll_arr[it]=nll_calc(test_prob, test_labels_arr)\n",
    "        _, predictedt = torch.max(test_prob,1)\n",
    "        accuracy_arr[it]= (predictedt==test_labels_arr.reshape(1,test_size)).sum()/test_size\n",
    "    print(\"SWA, ensemble size:\", es)\n",
    "    print(\"mean accuracy:\",accuracy_arr.mean(),\"std:\",accuracy_arr.std())\n",
    "    print(\"mean ace:\",ace_arr.mean(),\"std:\",ace_arr.std())\n",
    "    print(\"mean nll:\",nll_arr.mean(),\"std:\",nll_arr.std())\n",
    "    print(\"mean rps:\",rps_arr.mean(),\"std:\",rps_arr.std())\n",
    "    return [accuracy_arr.mean(),accuracy_arr.std(),ace_arr.mean(),ace_arr.std(),rps_arr.mean(),rps_arr.std(),nll_arr.mean(),nll_arr.std()]\n",
    "\n",
    "acc=torch.zeros(5)\n",
    "acc_std=torch.zeros(5)\n",
    "ace=torch.zeros(5)\n",
    "ace_std=torch.zeros(5)\n",
    "rps=torch.zeros(5)\n",
    "rps_std=torch.zeros(5)\n",
    "nll=torch.zeros(5)\n",
    "nll_std=torch.zeros(5)\n",
    "[acc[0],acc_std[0],ace[0],ace_std[0],rps[0],rps_std[0],nll[0],nll_std[0]]=compute_acc_ace_rps_swa(1)\n",
    "[acc[1],acc_std[1],ace[1],ace_std[1],rps[1],rps_std[1],nll[1],nll_std[1]]=compute_acc_ace_rps_swa(2)\n",
    "[acc[2],acc_std[2],ace[2],ace_std[2],rps[2],rps_std[2],nll[2],nll_std[2]]=compute_acc_ace_rps_swa(4)\n",
    "[acc[3],acc_std[3],ace[3],ace_std[3],rps[3],rps_std[3],nll[3],nll_std[3]]=compute_acc_ace_rps_swa(8)\n",
    "[acc[4],acc_std[4],ace[4],ace_std[4],rps[4],rps_std[4],nll[4],nll_std[4]]=compute_acc_ace_rps_swa(16)\n",
    "\n",
    "# from scipy.io import savemat\n",
    "# filepath=\"results_fashion_swa.mat\"\n",
    "# mdic={\"acc\":acc.cpu().numpy(),\"acc_std\":acc_std.cpu().numpy(),\"nll\": nll.cpu().numpy(),\"nll_std\":nll_std.cpu().numpy(),\\\n",
    "#       \"ace\": ace.cpu().numpy(),\"ace_std\":ace_std.cpu().numpy(), \"rps\":rps.cpu().numpy(),\"rps_std\":rps_std.cpu().numpy()}\n",
    "# savemat(filepath,mdic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SWA, ensemble size: 1\n",
    "mean accuracy: tensor(0.9337) std: tensor(0.0016)\n",
    "mean ace: tensor(0.0537) std: tensor(0.0016)\n",
    "mean nll: tensor(0.4826) std: tensor(0.0191)\n",
    "mean rps: tensor(0.0220) std: tensor(0.0006)\n",
    "SWA, ensemble size: 2\n",
    "mean accuracy: tensor(0.9403) std: tensor(0.0012)\n",
    "mean ace: tensor(0.0309) std: tensor(0.0012)\n",
    "mean nll: tensor(0.3312) std: tensor(0.0111)\n",
    "mean rps: tensor(0.0184) std: tensor(0.0004)\n",
    "SWA, ensemble size: 4\n",
    "mean accuracy: tensor(0.9435) std: tensor(0.0011)\n",
    "mean ace: tensor(0.0223) std: tensor(0.0010)\n",
    "mean nll: tensor(0.2579) std: tensor(0.0078)\n",
    "mean rps: tensor(0.0166) std: tensor(0.0003)\n",
    "SWA, ensemble size: 8\n",
    "mean accuracy: tensor(0.9453) std: tensor(0.0008)\n",
    "mean ace: tensor(0.0180) std: tensor(0.0011)\n",
    "mean nll: tensor(0.2186) std: tensor(0.0036)\n",
    "mean rps: tensor(0.0157) std: tensor(0.0002)\n",
    "SWA, ensemble size: 16\n",
    "mean accuracy: tensor(0.9462) std: tensor(0.0008)\n",
    "mean ace: tensor(0.0158) std: tensor(0.0009)\n",
    "mean nll: tensor(0.1964) std: tensor(0.0034)\n",
    "mean rps: tensor(0.0153) std: tensor(5.5335e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bayesian\n",
    "def compute_acc_ace_rps_bayes(es):\n",
    "    copies=int(num_runs/es)\n",
    "    ace_arr=torch.zeros(copies)\n",
    "    rps_arr=torch.zeros(copies)\n",
    "    nll_arr=torch.zeros(copies)\n",
    "    accuracy_arr=torch.zeros(copies)\n",
    "\n",
    "    for it in range(copies):\n",
    "        test_prob=torch.Tensor(test_prob_arr[:,:,30:60,it*es:(it+1)*es]).mean(-1).mean(-1).reshape(test_size,num_classes)\n",
    "        ace_arr[it]=adaptive_calibration_error(test_prob,test_labels_arr)\n",
    "        rps_arr[it]=(rps_calc(test_prob, test_labels_arr)).mean()\n",
    "        nll_arr[it]=nll_calc(test_prob, test_labels_arr)\n",
    "        _, predictedt = torch.max(test_prob,1)\n",
    "        accuracy_arr[it]= (predictedt==test_labels_arr.reshape(1,test_size)).sum()/test_size\n",
    "    print(\"Bayesian, ensemble size:\", es)\n",
    "    print(\"mean accuracy:\",accuracy_arr.mean(),\"std:\",accuracy_arr.std())\n",
    "    print(\"mean ace:\",ace_arr.mean(),\"std:\",ace_arr.std())\n",
    "    print(\"mean nll:\",nll_arr.mean(),\"std:\",nll_arr.std())\n",
    "    print(\"mean rps:\",rps_arr.mean(),\"std:\",rps_arr.std())\n",
    "    return [accuracy_arr.mean(),accuracy_arr.std(),ace_arr.mean(),ace_arr.std(),rps_arr.mean(),rps_arr.std(),nll_arr.mean(),nll_arr.std()]\n",
    "\n",
    "\n",
    "\n",
    "acc=torch.zeros(5)\n",
    "acc_std=torch.zeros(5)\n",
    "ace=torch.zeros(5)\n",
    "ace_std=torch.zeros(5)\n",
    "rps=torch.zeros(5)\n",
    "rps_std=torch.zeros(5)\n",
    "nll=torch.zeros(5)\n",
    "nll_std=torch.zeros(5)\n",
    "[acc[0],acc_std[0],ace[0],ace_std[0],rps[0],rps_std[0],nll[0],nll_std[0]]=compute_acc_ace_rps_bayes(1)\n",
    "[acc[1],acc_std[1],ace[1],ace_std[1],rps[1],rps_std[1],nll[1],nll_std[1]]=compute_acc_ace_rps_bayes(2)\n",
    "[acc[2],acc_std[2],ace[2],ace_std[2],rps[2],rps_std[2],nll[2],nll_std[2]]=compute_acc_ace_rps_bayes(4)\n",
    "[acc[3],acc_std[3],ace[3],ace_std[3],rps[3],rps_std[3],nll[3],nll_std[3]]=compute_acc_ace_rps_bayes(8)\n",
    "[acc[4],acc_std[4],ace[4],ace_std[4],rps[4],rps_std[4],nll[4],nll_std[4]]=compute_acc_ace_rps_bayes(16)\n",
    "\n",
    "# from scipy.io import savemat\n",
    "# filepath=\"results_fashion_bayes.mat\"\n",
    "# mdic={\"acc\":acc.cpu().numpy(),\"acc_std\":acc_std.cpu().numpy(),\"nll\": nll.cpu().numpy(),\"nll_std\":nll_std.cpu().numpy(),\\\n",
    "#       \"ace\": ace.cpu().numpy(),\"ace_std\":ace_std.cpu().numpy(), \"rps\":rps.cpu().numpy(),\"rps_std\":rps_std.cpu().numpy()}\n",
    "# savemat(filepath,mdic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian, ensemble size: 1\n",
    "mean accuracy: tensor(0.9358) std: tensor(nan)\n",
    "mean ace: tensor(0.0123) std: tensor(nan)\n",
    "mean nll: tensor(0.2039) std: tensor(nan)\n",
    "mean rps: tensor(0.0177) std: tensor(nan)\n",
    "\n",
    "Bayesian, ensemble size: 1\n",
    "mean accuracy: tensor(0.9400) std: tensor(nan)\n",
    "mean ace: tensor(0.0072) std: tensor(nan)\n",
    "mean nll: tensor(0.1876) std: tensor(nan)\n",
    "mean rps: tensor(0.0169) std: tensor(nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prob=torch.Tensor(test_prob_arr[:,7,0]).reshape(test_size,1)\n",
    "# ace=adaptive_calibration_error(test_labels_arr.reshape(test_size,1).numpy(),test_prob.numpy(),20)\n",
    "# ace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_prob=torch.Tensor(test_prob_arr[:,10:16,48:64]).mean(-1).mean(-1).reshape(test_size,1)\n",
    "# ace=adaptive_calibration_error(test_labels_arr.reshape(test_size,1).numpy(),test_prob.numpy(),20)\n",
    "# ace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchview import draw_graph\n",
    "net2 = Fashion_MNIST_CNN()\n",
    "\n",
    "#model_graph = draw_graph(net2, input_size=(1,3,64,64), expand_nested=True)\n",
    "model_graph = draw_graph(net2, input_size=(1,1,28,28), expand_nested=True)\n",
    "model_graph.visual_graph\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Pytorch MNIST.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "torchenv39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
